<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>数据挖掘课程复习</title>
    <link href="/BLOG/2025/06/11/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0/"/>
    <url>/BLOG/2025/06/11/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<blockquote><p>用于HITSZ 25春数据挖掘复习</p></blockquote><h1 id="概述">概述</h1><h2 id="数据挖掘的含义">数据挖掘的含义</h2><p>数据挖掘是从海量数据集中发现有趣的（<mark>非平凡的、隐含的、未被发现但有用的</mark>）模式、模型或知识的过程。</p><h2 id="数据挖掘过程">数据挖掘过程</h2><p>知识发现的过程：<br />1. 数据预处理<br /> 1. 数据清洗<br /> 2. 数据集成<br /> 3. 数据选择<br /> 4. 数据变换<br />2. 数据挖掘<br />3. 模式/模型评估<br />4. 知识表示</p><h2 id="数据挖掘的应用">数据挖掘的应用</h2><ul><li>数据分析与决策支持<ul><li>客户画像、需求分析</li><li>交叉市场分析</li></ul></li><li>风险分析与管理<ul><li>竞争分析</li><li>风险评估</li><li>资产管理</li><li>资源规划</li></ul></li><li>欺诈检测与异常模式挖掘<ul><li>医疗、电信等</li></ul></li></ul><h2 id="数据仓库">数据仓库</h2><p><strong>定义</strong>：数据仓库是<mark>面向主题的、集成的、随时间变化的、又相对稳定的</mark>数据集合，它支持管理层的决策过程<br /><strong>数据仓库的关键特征</strong>：</p><ul><li>面向主题的：<ul><li>数据仓库的数据是以分析的主题为中心构建的</li></ul></li><li>集成的：<ul><li>数据仓库的数据来自不同的数据源，需要按照统一的结构、格式、度量、语义进行数据处理，最后集成到数据仓库的主题</li></ul></li><li>随时间变化的：<ul><li>数据仓库的历史数据，需要随时间延长而增加（周期性更新）</li><li>数据仓库的综合数据，需要随时间延长而变化</li></ul></li><li>相对稳定的：<ul><li>数据仓库的数据主要用于支持决策，不会涉及频繁的修改与变化，主要是查询与分析。</li></ul></li></ul><hr /><h1 id="数据特征分析与预处理">数据特征分析与预处理</h1><h2 id="数据类型及其特征">数据类型及其特征</h2><h2 id="数据对象">数据对象：</h2><p>数据集由数据对象构成，数据对象又称之为样本、实例。</p><figure><img src="样本.png" alt="数据样本示意图" /><figcaption aria-hidden="true">数据样本示意图</figcaption></figure><h2 id="属性类型">属性类型</h2><p><strong>定义</strong>：一个数据字段，表示一个数据对象的某个特征</p><p><strong>分类</strong>：</p><ul><li><p><strong>标称属性:</strong> 与名称相关的值、分类或枚举属性<mark>(定性的)</mark></p><ul><li><strong>二元属性:</strong>标称属性的一种特殊情况，只有2个状态的标称属性。</li></ul></li><li><p><strong>序数属性:</strong> 值有一个有意义的顺序<mark>(定性的)</mark></p></li><li><p><strong>数值属性:</strong> 可度量的量 <mark>(定量的)</mark></p><ul><li><strong>区间标度属性:</strong><ul><li>在统一度量的尺度下</li><li>值有序（e.g. 日历日期， 温度计数据）</li><li>没有固有0点</li></ul></li><li><strong>比率标度属性：</strong><ul><li>具有固有零点，可以说一个值是一个值得多少倍</li><li>举例：字数、体重、工作年限</li></ul></li></ul></li></ul><h2 id="数据的描述统计">数据的描述统计</h2><ul><li><strong>数据离散特征：</strong>均值、众数、最值、分位数、方差、离群点……</li><li><strong>排序区间：</strong><ul><li>数据离散度： 多个粒度上得分析</li><li>排序区间得盆图/分位数图分析</li></ul></li></ul><p><strong>描述统计的图形显示：</strong>箱型图、直方图、分位数图、散点图</p><h2 id="数据可视化方法">数据可视化方法</h2><p><strong>数据可视化方法得分类：</strong></p><ul><li>基于像素的可视化技术<ul><li>协方差矩阵的热力图</li></ul></li><li>基于几何投影的可视化技术<ul><li>直接可视化、散点图、透视地形、平行坐标</li></ul></li><li>基于图标的可视化技术<ul><li>脸谱图、人物画像图</li></ul></li><li>基于分层的可视化技术<ul><li>树状图、<em>锥形树</em></li></ul></li><li>可视化复杂数据与关系<ul><li>标签云、线形图、饼图、结构图/流程图、<em>怪图、系统进化图</em></li></ul></li></ul><h2 id="测量数据的相似性和相异性">测量数据的相似性和相异性</h2><ul><li><strong>相似性：</strong> 度量数据的相似程度，越大越相似，通常范围在<span class="math display">[0,1]</span></li><li><strong>相异性：</strong>(e.g. distance)，度量数据的差异，最小值通常为0 ，越大差异越大</li></ul><p><strong>相异度矩阵：</strong>对角线为0的下三角矩阵，元素为对应对象之间的距离 <imgsrc="相异度矩阵.png" alt="相异度矩阵矩阵" /></p><h3 id="标称属性的邻近度量">标称属性的邻近度量</h3><ul><li><p>得分匹配：</p><p><span class="math display">$$d(i,j) = \frac{p - m}{p}$$</span></p><p>其中 <span class="math inline"><em>m</em></span>为相同变量数量，<span class="math inline"><em>p</em></span>为变量总数。</p></li><li><p>独热编码：<br />具体而言即将名称或类别转化为独热编码，随后利用编码计算距离</p></li></ul><h3 id="二进制属性的度量">二进制属性的度量</h3><figure><img src="列联表.png" alt="列联表" /><figcaption aria-hidden="true">列联表</figcaption></figure><p><strong>对称二元变量距离：</strong> <span class="math inline">$d(i,j)= \frac{r+s}{q+r+s+t}$</span></p><p><strong>不对称二元变量距离：</strong> <spanclass="math inline">$d(i,j) = \frac{r+s}{q+r+s}$</span></p><p><strong>Jaccard系数(不对称二元变量相似性)：</strong> <spanclass="math inline">$Sim_{Jaccard}(i,j) = \frac{q}{q+r+s}$</span></p><p><strong>TIP:</strong> 解释一下这个对称的含义，关键在于0是否有意义<img src="解释.png" alt="对称的含义解释" /></p><h4id="例题二进制属性的非对称相异性度量">例题：二进制属性的非对称相异性度量</h4><figure><img src="例题1.png" alt="二进制属性的非对称相异性度量" /><figcaption aria-hidden="true">二进制属性的非对称相异性度量</figcaption></figure><h3 id="数值属性的度量">数值属性的度量</h3><p><strong>闵可夫斯基距离公式：</strong></p><p><span class="math display">$$D(X, Y) = \left( \sum_{i=1}^n |x_i -y_i|^p \right)^{\frac{1}{p}}$$</span></p><p>其中 <spanclass="math inline"><em>p</em> ≥ 1</span>，是可调节的距离参数。</p><p>主要使用上述公式度量，其中有一些特例可以参照下图： <imgsrc="距离.png" alt="闵可夫斯基距离公式特例" /></p><h3 id="区间尺度与有序变量的度量">区间尺度与有序变量的度量</h3><ul><li>区间尺度：进行归一化或标准化处理随后计算闵可夫斯基距离</li><li>有序变量：使用排序代替值，并对排序值做最值归一化，随后计算闵可夫斯基距离</li></ul><h3 id="其他度量方法">其他度量方法：</h3><h4 id="余弦相似度">余弦相似度：</h4><p><span class="math display">$$\text{cos}(\theta) = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n}A_i^2} \cdot \sqrt{\sum_{i=1}^{n} B_i^2}}$$</span></p><p>用于衡量两个向量方向的相似程度，常用于文本分类、推荐系统等。</p><h4 id="kl-散度kullback-leibler-divergence">KL 散度（Kullback-LeiblerDivergence）：</h4><p><span class="math display">$$D_{KL}(P \parallel Q) = \sum_{i=1}^{n} p_i \log \frac{p_i}{q_i}$$</span></p><p>衡量两个概率分布之间的差异，常用于模型分布与目标分布之间的对比。</p><hr /><h2 id="数据预处理">数据预处理</h2><p><strong>目的：</strong>数据存在不完全、噪音、不一致的问题，高质量的数据挖掘依赖高质量的数据</p><h3 id="数据预处理的主要任务">数据预处理的主要任务：</h3><ul><li><strong>数据清理：</strong> 异常值、缺失值、噪音<ul><li>针对缺失值：删除整个样本、均值或聚类填充</li><li>针对异常值与噪音： 通过分箱、聚类、回归去除</li></ul></li><li><strong>数据集成：</strong> 多个来源合到一起</li><li><strong>数据变换：</strong> 归一化等操作</li><li><strong>数据归约：</strong> 减小数据的冗余<ul><li>维度规约： 减少无用特征，或产生新的特征代替原来的<ul><li>决策树规约、PCA</li></ul></li><li>数值规约：减小数据量，例如用模型参数+离群点代替原始数据<ul><li>线性回归、聚类</li></ul></li><li>数据压缩</li></ul></li><li><strong>数据离散化和概念分层：</strong><ul><li>连续变量离散化</li><li>人为指定偏序关系</li></ul></li></ul><hr /><h1 id="关联规则">关联规则</h1><h2 id="关联规则挖掘概念">关联规则挖掘概念</h2><p>一种从事务数据集中发现项与项之间有趣关系的技术，形式为：</p><p><spanclass="math display"><em>X</em> ⇒ <em>Y</em>,  <em>X</em> ∩ <em>Y</em> = ∅</span></p><h3 id="支持度support">支持度（Support）：</h3><p><span class="math display">$$\text{Support}(X \Rightarrow Y) = \frac{\text{包含 } X \cup Y \text{的事务数}}{\text{总事务数}}$$</span></p><h3 id="置信度confidence">置信度（Confidence）：</h3><p><span class="math display">$$\text{Confidence}(X \Rightarrow Y) = \frac{\text{Support}(X \cupY)}{\text{Support}(X)} = P(Y|X)$$</span></p><h3 id="提升度lift">提升度（Lift）：</h3><p><span class="math display">$$\text{Lift}(X \Rightarrow Y) =\frac{\text{Confidence}(X \RightarrowY)}{\text{Support}(Y)} =  \frac{P(Y|X)}{P(Y)}$$</span></p><p>Lift &gt; 1 表示正相关，=1 表示独立，&lt;1 表示负相关。</p><h2 id="频繁项集与其分类">频繁项集与其分类</h2><h3 id="频繁项集">频繁项集：</h3><p>如果一个项集 <span class="math inline"><em>X</em></span>的支持度满足：</p><p><span class="math display">Support(<em>X</em>) ≥ min_sup</span></p><p>则 <span class="math inline"><em>X</em></span> 是频繁项集。</p><h3 id="闭频繁项集closed">闭频繁项集（Closed）：</h3><p>项集 <span class="math inline"><em>X</em></span>是闭的，当不存在一个超集 <spanclass="math inline"><em>Y</em> ⊃ <em>X</em></span>，使得：</p><p><spanclass="math display">Support(<em>Y</em>) = Support(<em>X</em>)</span></p><h3 id="极大频繁项集maximal">极大频繁项集（Maximal）：</h3><p>项集 <span class="math inline"><em>X</em></span>是极大的，当不存在一个超集 <spanclass="math inline"><em>Y</em> ⊃ <em>X</em></span> 是频繁的。</p><h3 id="三者关系">三者关系：</h3><ul><li>极大频繁项集 ⊆ 闭频繁项集 ⊆ 所有频繁项集；</li><li>闭项集压缩信息不丢失，极大项集压缩更彻底但不保留精确支持度。</li></ul><h2 id="关联规则基本模型">关联规则基本模型</h2><p>关联规则是指同时满足最小支持度和最小置信度阈值的规则，形式如下：</p><p><spanclass="math display"><em>X</em> ⇒ <em>Y</em>,  <em>X</em> ∩ <em>Y</em> = ∅</span></p><h3 id="挖掘流程">挖掘流程：</h3><ol type="1"><li>找出所有频繁项集（满足 <spanclass="math inline">Support ≥ min_sup</span>）；</li><li>从频繁项集中生成所有满足 <spanclass="math inline">Confidence ≥ min_conf</span> 的规则。</li></ol><p>只有同时满足这两个条件的规则，才被称为“强规则”。</p><h2 id="apriori算法流程">📌 Apriori算法流程</h2><p>Apriori是一种经典的频繁项集挖掘算法，用于从大量事务数据中发现频繁项集和生成关联规则。</p><h3 id="核心思想apriori原理">核心思想：Apriori原理</h3><blockquote><p><strong>如果一个项集是频繁的，那么它的所有子集也一定是频繁的。</strong></p></blockquote><p>也就是说，<strong>如果某个项集不是频繁的，则其所有超集都不可能是频繁的</strong>，因此可以在搜索中直接剪枝，提升效率。</p><h3 id="算法计算流程">算法计算流程</h3><h4 id="步骤-1扫描数据库找出频繁-1-项集l₁">步骤 1️⃣：扫描数据库，找出频繁1 项集（L₁）</h4><ul><li>统计每个单项的支持度；</li><li>过滤出支持度 ≥ <code>min_sup</code> 的项；</li><li>得到频繁 1 项集集合 L₁。</li></ul><h4 id="步骤-2由-l₁-构造候选-2-项集c₂">步骤 2️⃣：由 L₁ 构造候选 2项集（C₂）</h4><ul><li><strong>连接步骤</strong>：将 L₁ 中的项两两组合生成 2项候选项集；</li><li><strong>剪枝步骤</strong>：剔除那些包含非频繁子集的候选项集；</li><li>重新扫描数据库统计每个候选项集的支持度；</li><li>保留支持度 ≥ <code>min_sup</code> 的项集，得到频繁 2 项集 L₂。</li></ul><h4 id="步骤-3迭代构造更高阶项集-l₃l₄">步骤 3️⃣：迭代构造更高阶项集L₃、L₄…</h4><ul><li>从 L₂ 生成 C₃，再从中提取 L₃；</li><li>重复 <strong>连接-剪枝-计数</strong> 的过程；</li><li>直到某一轮没有产生新的频繁项集。</li></ul><h4 id="步骤-4由频繁项集生成关联规则">步骤4️⃣：由频繁项集生成关联规则</h4><ul><li>对每个频繁项集 <spanclass="math inline"><em>L</em></span>，枚举其所有非空子集 <spanclass="math inline"><em>S</em></span>；</li><li>构造规则 <spanclass="math inline"><em>S</em> ⇒ (<em>L</em>−<em>S</em>)</span>；</li><li>计算置信度（Confidence）：</li></ul><p><span class="math display">$$\text{Confidence}(S \Rightarrow T) = \frac{\text{Support}(S \cupT)}{\text{Support}(S)}$$</span></p><ul><li>只保留满足 <span class="math inline">Confidence ≥ min_conf</span>的规则。</li></ul><h3 id="提高-apriori-算法效率的方法">提高 Apriori 算法效率的方法</h3><p>Apriori算法在面对大规模数据时效率较低，以下是常用的几种优化策略：</p><ol type="1"><li><p>Hash-based itemset counting（基于哈希的项集计数）</p><ul><li>将候选项集映射到哈希桶中；</li><li>如果某个哈希桶中的计数低于最小支持度，则其对应的项集可直接剪枝；</li><li><strong>作用：减少无效候选项集数量。</strong></li></ul></li><li><p>Transaction reduction（事务压缩）</p><ul><li>每轮扫描后，删除不包含任何频繁项集的事务；</li><li>后续迭代无需再扫描这些事务；</li><li><strong>作用：减少数据扫描量，提高效率。</strong></li></ul></li><li><p>Partitioning（划分策略）</p><ul><li>将数据库划分为多个子集，分别挖掘局部频繁项集；</li><li>将局部频繁项集合并，再在全库中验证；</li><li><strong>只需两次完整扫描</strong>，适合大数据和并行处理。</li></ul></li><li><p>Sampling（采样）</p><ul><li>从数据库中随机抽取部分事务作为样本；</li><li>在样本上挖掘频繁项集，再用全体数据验证；</li><li><strong>优点：快速；缺点：可能漏掉边缘频繁项集。</strong></li></ul></li></ol><h2 id="关联规则的度量">关联规则的度量</h2><p>除了上述说到的支持度、置信度、提升度以外，还有一些度量方式。这里主要是四种零不变性度量。</p><p>在关联规则中，某些度量值不受<strong>零事务（nulltransactions）</strong>影响，称为<strong>零不变性度量</strong>。</p><ul><li>零事务：不包含任何待考察项集的事务；</li><li>零不变性：度量值只由 <spanclass="math inline"><em>P</em>(<em>A</em>|<em>B</em>)</span> 和 <spanclass="math inline"><em>P</em>(<em>B</em>|<em>A</em>)</span>决定，与总事务数无关；</li></ul><p>以下四种度量具有<strong>零不变性</strong>，且其值范围都在 <spanclass="math inline">[0,1]</span>，值越大表示 A 与 B 的关联越紧密。</p><ul><li><strong>全置信度</strong> <span class="math display">$$\text{all_confidence}(A, B) = \frac{\text{sup}(A \cupB)}{\max(\text{sup}(A), \text{sup}(B))} = \min(P(A|B), P(B|A))$$</span></li><li><strong>最大置信度 </strong> <spanclass="math display">max_confidence(<em>A</em>,<em>B</em>) = max (<em>P</em>(<em>A</em>|<em>B</em>),<em>P</em>(<em>B</em>|<em>A</em>))</span></li><li><strong>余弦置信度</strong><br /><span class="math display">$$\text{cosine}(A, B) = \frac{P(A \cap B)}{\sqrt{P(A) \cdot P(B)}} =\sqrt{P(B|A) \cdot P(A|B)}$$</span></li><li><strong>Kelu置信度</strong><br /><span class="math display">$$\text{Kulc}(A, B) = \frac{1}{2} \left( P(A|B) + P(B|A) \right)$$</span></li></ul><h1 id="分类">分类</h1><h2 id="模型评估方法">模型评估方法</h2><ul><li>留出法： 按照比例随机划分出训练集和测试集，多次随机抽样取均值</li><li>交叉验证： 把数据划分为k份，每次选择一份作为测试集，重复k次，取均值<ul><li>留一法： 每份仅有一个样本</li></ul></li><li>Bootstrap 自助法:又放回的抽样m次作为训练集，没有被抽到的作为测试集</li></ul><h2 id="分类模型评估指标">分类模型评估指标</h2><p><strong>混淆矩阵（Confusion Matrix）</strong></p><table><thead><tr class="header"><th>实际/预测</th><th>正类（预测）</th><th>负类（预测）</th></tr></thead><tbody><tr class="odd"><td>正类（实际）</td><td>TP（真正例）</td><td>FN（假负例）</td></tr><tr class="even"><td>负类（实际）</td><td>FP（假正例）</td><td>TN（真负例）</td></tr></tbody></table><ul><li><strong>TP</strong>：预测为正，实际为正</li><li><strong>FP</strong>：预测为正，实际为负</li><li><strong>FN</strong>：预测为负，实际为正</li><li><strong>TN</strong>：预测为负，实际为负</li></ul><p><strong>指标：</strong></p><ul><li>准确率</li></ul><p><span class="math display">$$\text{Accuracy} = \frac{TP + TN}{TP + FP + FN + TN}$$</span></p><ul><li>召回率</li></ul><p><span class="math display">$$\text{Recall} = \frac{TP}{TP + FN}$$</span></p><ul><li>精确率</li></ul><p><span class="math display">$$\text{Precision} = \frac{TP}{TP + FP}$$</span></p><ul><li>F1-Score</li></ul><p><span class="math display">$$\text{F1} = \frac{2 \cdot \text{Precision} \cdot\text{Recall}}{\text{Precision} + \text{Recall}}$$</span></p><ul><li>F-beta</li></ul><p><span class="math display">$$\text{F1} = \frac{(1+\beta^2) \cdot \text{Precision} \cdot\text{Recall}}{\beta^2 \cdot \text{Precision} + \text{Recall}}$$</span></p><ul><li><p>P-R 曲线（Precision-Recall Curve）</p><ul><li><strong>横轴：Recall（召回率）</strong></li><li><strong>纵轴：Precision（精确率）</strong></li><li>通过不断调整分类阈值，绘制出一系列 (P, R) 点，形成一条曲线</li></ul></li><li><p>ROC 曲线（Receiver Operating Characteristic）</p><ul><li>横轴：FPR（假正率） = <span class="math inline">$\frac{FP}{FP +TN}$</span></li><li>纵轴：TPR（真正率） = 召回率 = <spanclass="math inline">$\frac{TP}{TP + FN}$</span></li><li>曲线下的面积（AUC）越接近 1，模型越好</li></ul></li></ul><h2 id="决策树归纳算法的流程以-id3c4.5cart-为例">🛠️决策树归纳算法的流程（以 ID3/C4.5/CART 为例）</h2><ol type="1"><li><p><strong>输入训练集</strong> <spanclass="math inline"><em>D</em></span>，每条样本有多个属性 +类别标签。</p></li><li><p><strong>判断停止条件</strong>：</p><ul><li>样本全属于一个类别 → 生成叶节点，停止；</li><li>属性已用完，或样本不足 → 用多数类别作为叶节点标记。</li></ul></li><li><p><strong>选择最优划分属性 A</strong>：</p><ul><li>使用某种<strong>划分指标</strong>（如信息增益、增益率、基尼指数）选择属性A；</li><li>若无可用属性 → 直接创建叶节点。</li></ul></li><li><p><strong>按属性 A 的取值划分数据集</strong>：</p><ul><li>对每个取值 <spanclass="math inline"><em>a</em><sub><em>i</em></sub></span>，将数据集划分为<span class="math inline"><em>D</em><sub><em>i</em></sub></span>；</li><li>为每个子集创建分支子节点。</li></ul></li><li><p><strong>对子节点递归重复上述过程</strong>：</p><ul><li>在子集 <spanclass="math inline"><em>D</em><sub><em>i</em></sub></span>上继续构建子树。</li></ul></li><li><p><strong>形成整棵决策树</strong>：</p><ul><li>直到所有子集都满足停止条件。</li></ul></li></ol><h2 id="属性选择度量">属性选择度量</h2><p>有三种，分别为 信息增益、增益率、基尼系数</p><h3 id="信息增益information-gain">信息增益（Information Gain）</h3><p>信息增益是 ID3 算法使用的度量标准，它衡量的是：使用某个属性 A对数据进行划分后，<strong>系统信息熵（混乱度）减少了多少</strong>。</p><p>公式：</p><p><span class="math display">$$\text{Gain}(D, A) = Ent(D) - \sum_{v \in \text{Values}(A)}\frac{|D_v|}{|D|} \cdot Ent(D_v)$$</span></p><ul><li>其中 <span class="math inline">$Ent(D) = -\sum_{i=1}^{m}p_i\text{log}_2 (p_i)$</span> 是原始数据集的熵 ；</li><li><span class="math inline"><em>D</em><sub><em>v</em></sub></span>表示属性 A 取值为 <span class="math inline"><em>v</em></span>的子集；</li><li>值越大表示“划分后不确定性下降得越多”，越适合作为划分属性。</li></ul><p>缺点：偏向于选择取值多的属性（比如身份证号），容易过拟合。</p><hr /><h3 id="增益率gain-ratio">增益率（Gain Ratio）</h3><p>为了解决信息增益偏好多值属性的问题，C4.5 算法引入了增益率：</p><p><span class="math display">$$\text{GainRatio}(D, A) = \frac{\text{Gain}(D, A)}{IV(A)}$$</span></p><ul><li><span class="math inline"><em>I</em><em>V</em>(<em>A</em>)</span> 是A 的“固有值信息”或“分裂信息”，可以理解为划分的复杂度。 <spanclass="math display">$$IV(A) = - \sum_{j=1}^{v}\frac{|D_j|}{D} \text{log}_2(\frac{|D_j|}{D})$$</span></li><li>增益率在衡量信息增益的同时，<strong>惩罚属性值过多的划分</strong>，更加平衡。</li></ul><hr /><h3 id="基尼指数gini-index">基尼指数（Gini Index）</h3><p>CART算法采用基尼指数来衡量属性划分的纯净程度。基尼指数越小，表示子集越“纯”。</p><p>某个集合 D 的基尼指数定义为：</p><p><span class="math display">$$\begin{align}Gini(D) &amp;= 1 - \sum_{k=1}^{K} p_k^2 \\Gini_{split}(D) &amp;= \frac{|D_1|}{|D|}Gini(D_1) +\frac{|D_2|}{|D|}Gini(D_2)\end{align}$$</span></p><ul><li><span class="math inline"><em>p</em><sub><em>k</em></sub></span>表示属于第 k 类的概率；</li><li>当样本全属于一个类别时，Gini = 0；</li><li>每次选择能<strong>最小化划分后加权 Gini</strong>的属性作为最优划分属性。</li></ul><hr /><p>✅ 总结对比：</p><table><thead><tr class="header"><th>度量方式</th><th>使用算法</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr class="odd"><td>信息增益</td><td>ID3</td><td>简单直观，计算熵有理论基础</td><td>偏向多值属性，可能过拟合</td></tr><tr class="even"><td>增益率</td><td>C4.5</td><td>纠正信息增益偏差，权衡复杂度</td><td>偏向取值较均匀的属性</td></tr><tr class="odd"><td>基尼指数</td><td>CART</td><td>计算更快，适合二叉树构建</td><td>没有熵那样的理论解释</td></tr></tbody></table><h2 id="常见分类方法的主要思想">常见分类方法的主要思想</h2><h3 id="朴素贝叶斯分类naive-bayes-classification">朴素贝叶斯分类（NaiveBayes Classification）</h3><p>朴素贝叶斯是一种基于<strong>贝叶斯定理</strong>并假设属性之间相互条件独立的概率分类方法。</p><ul><li><p>给定训练数据集 <spanclass="math inline"><em>D</em></span>，每个样本表示为一个 <spanclass="math inline"><em>n</em></span> 维属性向量：<br /><spanclass="math display"><em>X</em> = (<em>x</em><sub>1</sub>,<em>x</em><sub>2</sub>,...,<em>x</em><sub><em>n</em></sub>)</span></p></li><li><p>假设总共有 <span class="math inline"><em>m</em></span>个类别：<spanclass="math inline"><em>C</em><sub>1</sub>, <em>C</em><sub>2</sub>, ..., <em>C</em><sub><em>m</em></sub></span><br />目标是确定输入 <span class="math inline"><em>X</em></span>属于哪个类别，即计算后验概率最大的类别： <spanclass="math display"><em>Ĉ</em> = arg max<sub><em>C</em><sub><em>i</em></sub></sub><em>P</em>(<em>C</em><sub><em>i</em></sub>∣<em>X</em>)</span></p></li><li><p>根据<strong>贝叶斯定理</strong>： <span class="math display">$$P(C_i \mid X) = \frac{P(C_i) \cdot P(X \mid C_i)}{P(X)}$$</span></p></li><li><p>因为 <span class="math inline"><em>P</em>(<em>X</em>)</span>对所有类别都相同，所以在分类时只需最大化分子部分： <spanclass="math display"><em>Ĉ</em> = arg max<sub><em>C</em><sub><em>i</em></sub></sub><em>P</em>(<em>C</em><sub><em>i</em></sub>) ⋅ <em>P</em>(<em>X</em>∣<em>C</em><sub><em>i</em></sub>)</span></p></li><li><p>若假设特征条件独立，则有： <span class="math display">$$P(X \mid C_i) = \prod_{k=1}^{n} P(x_k \mid C_i)$$</span></p></li></ul><h4 id="laplacian-correction拉普拉斯校准">LaplacianCorrection（拉普拉斯校准）</h4><p>为了解决0概率问题，引入 <strong>Laplacian 平滑（也叫 Add-OneSmoothing）</strong>，公式调整为：</p><p><span class="math display">$$P(x_k \mid C_i) = \frac{N_{ik} + 1}{N_i + d}$$</span></p><ul><li><spanclass="math inline"><em>N</em><sub><em>i</em><em>k</em></sub></span>：类别<span class="math inline"><em>C</em><sub><em>i</em></sub></span> 中属性<span class="math inline"><em>A</em><sub><em>k</em></sub></span> 取值为<span class="math inline"><em>x</em><sub><em>k</em></sub></span>的样本数<br /></li><li><spanclass="math inline"><em>N</em><sub><em>i</em></sub></span>：类别 <spanclass="math inline"><em>C</em><sub><em>i</em></sub></span>的总样本数<br /></li><li><span class="math inline"><em>d</em></span>：属性 <spanclass="math inline"><em>A</em><sub><em>k</em></sub></span>的可能取值数（即类别数）</li></ul><p>这样，即使某个值从未出现（<spanclass="math inline"><em>N</em><sub><em>i</em><em>k</em></sub> = 0</span>），加1 后也不会导致整个概率为 0。</p><h3 id="knn-分类k-nearest-neighbors">KNN 分类（K-NearestNeighbors）</h3><p>KNN（K近邻）是一种<strong>基于实例的非参数分类方法</strong>，核心思想是：<strong>“相似样本具有相似的类别。”</strong></p><p>基本流程：</p><ul><li>给定一个待分类样本，计算它与训练集中所有样本之间的距离（如欧几里得距离）；</li><li>选出距离最近的 <span class="math inline"><em>K</em></span>个“邻居”；</li><li>让这些邻居“投票”，投票最多的类别即为预测结果。</li></ul><h3 id="逻辑回归logistic-regression">逻辑回归（LogisticRegression）</h3><p>逻辑回归是一种<strong>用于二分类任务的线性模型</strong>，其本质是学习一个函数，输出属于某一类别的概率。</p><h4 id="基本思想">基本思想：</h4><ul><li><p>给定输入特征 <spanclass="math inline"><em>X</em> = (<em>x</em><sub>1</sub>,<em>x</em><sub>2</sub>,...,<em>x</em><sub><em>n</em></sub>)</span>，逻辑回归学习一个线性组合：</p><p><spanclass="math display"><em>z</em> = <em>w</em><sup><em>T</em></sup><em>x</em> + <em>b</em></span></p></li><li><p>然后通过<strong>Sigmoid 函数</strong>将其映射到 <spanclass="math inline">[0,1]</span> 范围，作为正类的概率：</p><p><span class="math display">$$P(y = 1 \mid x) = \frac{1}{1 + e^{-z}} = \frac{1}{1 + e^{-(w^T x + b)}}$$</span></p></li><li><p>预测时，通常设置阈值 <span class="math inline">0.5</span>，若<span class="math inline"><em>P</em> ≥ 0.5</span>则预测为正类，否则为负类。</p></li><li><p>没有闭合解，通常使用梯度下架配合交叉熵损失求解（在训练数据上拟合）</p></li></ul><h3 id="神经网络">神经网络</h3><p><maek>跳过</mark></p><h3 id="支持向量机svm分类">支持向量机（SVM）分类</h3><p>支持向量机是一种<strong>二分类模型</strong>，通过寻找一个最佳分割超平面，将不同类别的数据点分开。</p><h4 id="核心思想">核心思想：</h4><ul><li>寻找一个<strong>最大间隔（Margin）</strong>的超平面，使得距离超平面最近的样本点（支持向量）距离最大化；</li><li>最大间隔有助于提高模型的泛化能力。</li></ul><h4 id="线性可分情况">线性可分情况：</h4><ul><li><p>给定训练样本，SVM 找到一个线性超平面满足：</p><p><spanclass="math display"><em>w</em><sup><em>T</em></sup><em>x</em> + <em>b</em> = 0</span></p></li><li><p>并满足分类约束：</p><p><spanclass="math display"><em>y</em><sub><em>i</em></sub>(<em>w</em><sup><em>T</em></sup><em>x</em><sub><em>i</em></sub>+<em>b</em>) ≥ 1,  <em>i</em> = 1, 2, …, <em>n</em></span></p></li><li><p>目标是最大化间隔，即最小化 <spanclass="math inline">∥<em>w</em>∥<sup>2</sup></span>。</p></li></ul><h4 id="线性不可分情况">线性不可分情况：</h4><ul><li><p>使用<strong>软间隔（SoftMargin）</strong>，允许一定程度的分类错误，加入松弛变量 <spanclass="math inline"><em>ξ</em><sub><em>i</em></sub></span>；</p></li><li><p>目标函数变为：</p><p><span class="math display">$$\min_{w,b} \frac{1}{2}\|w\|^2 + C \sum_{i=1}^n \xi_i$$</span></p></li><li><p>其中 <span class="math inline"><em>C</em></span>控制误差惩罚强度。</p></li></ul><h4 id="非线性分类">非线性分类：</h4><ul><li>通过<strong>核函数（Kernel）</strong>，将数据映射到高维空间，使其线性可分；</li><li>常用核函数包括：<ul><li>线性核：<spanclass="math inline"><em>K</em>(<em>x</em>,<em>z</em>) = <em>x</em><sup><em>T</em></sup><em>z</em></span></li><li>多项式核：<spanclass="math inline"><em>K</em>(<em>x</em>,<em>z</em>) = (<em>x</em><sup><em>T</em></sup><em>z</em>+<em>c</em>)<sup><em>d</em></sup></span></li><li>径向基函数核（RBF）：<spanclass="math inline"><em>K</em>(<em>x</em>,<em>z</em>) = exp (−<em>γ</em>∥<em>x</em>−<em>z</em>∥<sup>2</sup>)</span></li></ul></li></ul><h3 id="集成方法ensemble-methods">集成方法（Ensemble Methods）</h3><p>集成方法是将多个<strong>弱分类器</strong>组合成一个强分类器的技术，旨在提高模型的准确性和稳定性。</p><h4 id="基本思想-1">基本思想：</h4><ul><li>通过结合多个模型的预测结果，减少单个模型的偏差和方差；</li><li>常见方式包括投票（分类）或平均（回归）。</li></ul><h4 id="主要类型">主要类型：</h4><ol type="1"><li><strong>Bagging（Bootstrap Aggregating）</strong><ul><li>通过自助采样（有放回抽样）构建多个训练子集，训练多个模型；<br /></li><li>预测时对多个模型结果投票或平均；<br /></li><li>代表算法：随机森林（Random Forest）。</li></ul></li><li><strong>Boosting</strong><ul><li>逐步训练一系列弱分类器，每个分类器关注前一个分类器错误分类的样本；<br /></li><li>通过加权组合提升整体性能；<br /></li><li>代表算法：AdaBoost、Gradient Boosting、XGBoost。</li></ul></li><li><strong>Stacking（堆叠）</strong><ul><li>训练多个不同类型的基模型；<br /></li><li>使用另一个模型（元学习器）学习如何组合基模型的输出。</li></ul></li></ol><h1 id="聚类">聚类</h1><p><strong>聚类分析：</strong>将数据对象分为多个类或簇，类内对象相似，类间对象相异</p><h2 id="基于划分的聚类方法">基于划分的聚类方法</h2><p>构造n个对象数据集D的划分，将其划分为k个列（所以需要给定K值的是基于划分方法的）</p><h3 id="k-平均聚类算法">K-平均聚类算法</h3><p><strong>步骤：</strong></p><ul><li>选择一个含有随机选择样本的k个簇的初始划分，计算这些簇的质心</li><li>根据欧氏距离把剩余的每个样本分配到距离它最近的簇质心的划分</li><li>计算被分配到每个簇的样本的均值向量，作为新的簇的质心</li><li>重复2,3直到k个簇的质心点不再发生变化或平方误差准则最小<ul><li>这里平方误差是说每个簇内样本到中心的平方误差的总和的总和</li></ul></li></ul><p><strong>优点:</strong> - 相对有效性，复杂度为 <spanclass="math inline"><em>O</em>(<em>k</em><em>n</em><em>t</em>)</span>，其中 n 是对象数目, k 是簇数目, t 是迭代次数</p><p><strong>缺点：</strong> - 需要预先指顶簇的数目k, -不能处理噪音数据和孤立点(outliers) -不适合用来发现具有非凸形状(non-convex shapes)的簇</p><h3 id="k-中心点聚类算法">K-中心点聚类算法</h3><p>主要用于解决K-均值在异常值上的缺点</p><p><strong>k-中心点算法流程:</strong> 1. 任意选取 k 个点作为 中心点</p><ol start="2" type="1"><li><p>按照与中心点最近的原则，将剩余点分配到当前最佳的中心点代表的类中</p></li><li><p>在每一类中，计算每个成员点对应的准则函数，选取准则函数最小时对应的点作为新的中心点</p></li><li><p>重复2-3的过程，直到所有的 中心点点不再发生变化，或已达到设定的最大迭代次数</p></li></ol><blockquote><p>其中准则函数为，一类中，某个成员点和其他成员点的距离之和</p></blockquote><p><strong>优点：</strong> 当存在噪音和孤立点时, K-medoids 比 K-means更健壮。 <strong>缺点：</strong> K-medoids 对于小数据集工作得很好,但不能很好地用于大数据集，计算质心的步骤时间复杂度是<spanclass="math inline"><em>O</em>(<em>n</em><sup>2</sup>)</span>，运行速度较慢</p><h3 id="pam算法">PAM算法</h3><p>是最早提出的k-中心点聚类算法</p><p><strong>PAM算法流程：</strong><br />1. 首先随机选择k个对象作为中心，把每个对象分配给离它最近的中心。<br />2. 对每个Medoid和非Medoid点(<spanclass="math inline"><em>O</em>(<em>k</em>(<em>n</em>−<em>k</em>))</span>)，尝试交换它们的位置，计算新的聚类代价(<spanclass="math inline"><em>O</em>(<em>n</em>−<em>k</em>)</span>)。<br />3.如果总的损失减少，则交换中心对象和非中心对象；如果总的损失增加，则不进行交换</p><p><strong>优点：</strong> 当存在噪音和孤立点时, K-medoids 比 K-means更健壮；<br /><strong>缺点：</strong> 同K-medoids， 每轮复杂度<spanclass="math inline"><em>O</em>(<em>k</em>(<em>n</em>−<em>k</em>)<sup>2</sup>)</span></p><h3 id="clara算法">CLARA算法</h3><p>不考虑整个数据集, 而是选择数据的一小部分作为样本</p><p><strong>CLARA算法流程：</strong><br />1. 从数据集中抽取一个样本集, 并对样本集使用PAM，划分为k个簇<br />2. 将其他未被抽到的样本划分到上述确定的簇中<br />3. 重复上述步骤，选择最好的聚类作为输出</p><p><strong>优点:</strong> 可以处理的数据集比 PAM大</p><p><strong>缺点:</strong> - 有效性依赖于样本集的大小 -基于样本的好的聚类并不一定是 整个数据集的好的聚类, 样本可能发生倾斜</p><h3 id="clarans算法">CLARANS算法</h3><h4 id="输入参数">输入参数：</h4><ul><li>数据集 ( D )，样本个数 ( n )</li><li>聚类数 ( k )</li><li>每轮最大尝试次数 <code>max_neighbor</code></li><li>总共执行的局部搜索次数 <code>num_local</code></li></ul><h4 id="多次随机局部搜索共执行-num_local-次">多次随机局部搜索（共执行<code>num_local</code> 次）：</h4><p>对于每次局部搜索：</p><ol type="1"><li>随机选择初始的 ( k ) 个 Medoids<br /></li><li>当前 Medoids 作为初始解，循环尝试邻居（即交换一个 Medoid 和一个非Medoid）：<ul><li>随机选择一个非 Medoid 点，与当前某个 Medoid交换，得到一个“邻居解”</li><li>计算这个新解的聚类代价</li><li>如果代价降低，则更新当前解为这个新解</li></ul></li><li>重复上述步骤，直到连续 <code>max_neighbor</code>次尝试都没有找到更优解</li><li>记录该局部搜索中最好的解</li></ol><h4 id="最终输出">最终输出：</h4><p>在 <code>num_local</code>次局部搜索中，选择代价最低的那个作为最终聚类结果</p><h2 id="基于层次的聚类方法">基于层次的聚类方法</h2><p>层次的聚类方法将数据对象组成一棵聚类的树,可以进一步分为: -凝聚的(agglomerative)层次聚类 (自底向上形成) - 分裂的(divisive)层次聚类(自顶向下形成)</p><h3 id="agnes-agglomerative-nesting算法">AGNES (AgglomerativeNesting)算法</h3><p><strong>算法流程：</strong></p><ul><li>初始化：计算包含每对样本间距离（如欧氏距离）的相似矩阵，把每个样本作为一个簇；<br /></li><li>选择：使用相似矩阵查找最相似的两个簇；<ul><li>两个簇间的相似度由这两个不同簇中距离最近的数据点对的相似度来确定－－单链接方法(Single-link)<br /></li></ul></li><li>更新：<ul><li>将两个簇合并为一个簇，簇的个数通过合并被更新；<br /></li><li>同时更新相似矩阵，将两个簇的两行（两列）距离用1行（1列）距离替换反映合并操作<br /></li></ul></li><li>重复：执行n-1次选择与更新；<br /></li><li>结束：当所有样本都合并成一个簇或满足某个终止条件时，整个过程结束</li></ul><h3 id="diana-divisive-analysis算法">DIANA (Divisive Analysis)算法</h3><p><strong>主要思想：</strong>采用自顶向下策略</p><ul><li>首先将所有样本置于一个簇中；</li><li>然后逐渐细分为越来越小的簇，来增加簇的数目；</li><li>直到每个样本自成一个簇，或者达到某个终结条件。</li><li>例如达到了某个希望的簇的数目或两个最近的簇之间的距离超过了某个阈值。</li></ul><h2 id="基于密度的聚类方法">基于密度的聚类方法</h2><h2 id="基于模型的聚类方法">基于模型的聚类方法</h2>]]></content>
    
    
    <categories>
      
      <category>计算机</category>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机</tag>
      
      <tag>数据挖掘</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>线性代数：矩阵与线性方程组</title>
    <link href="/BLOG/2025/05/25/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%EF%BC%9A%E7%9F%A9%E9%98%B5%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/"/>
    <url>/BLOG/2025/05/25/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%EF%BC%9A%E7%9F%A9%E9%98%B5%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<blockquote><p>从这个一个博客开始，我们将进行线性代数的复习。由于是用于夏令营以及与推免考核所用，我们一些简单的定义和定理将直接带过</p></blockquote><h1 id="线性代数矩阵与线性方程组">线性代数：矩阵与线性方程组</h1><p>我们从矩阵开始复习，同时带到线性相关、子空间等概念。</p><p>我们先介绍最简单的概念，线性相关与线性无关</p><h2 id="线性相关与线性无关">线性相关与线性无关</h2><p>我们定义列向量 <spanclass="math inline">{<em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, <em>a</em><sub>3</sub>, …，<em>a</em><sub><em>n</em></sub>}</span></p><ul><li><p><strong>线性相关(Linear Dependent)</strong>：若是存在不全为<strong>0</strong> 的向量 <spanclass="math inline"><em>X</em> = {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, <em>x</em><sub>3</sub>, …, <em>x</em><sub><em>n</em></sub>}</span>满足 <spanclass="math inline"><em>x</em><sub>1</sub><em>a</em><sub>1</sub> + <em>x</em><sub>2</sub><em>a</em><sub>2</sub> + <em>x</em><sub>3</sub><em>a</em><sub>3</sub> + … + <em>x</em><sub><em>n</em></sub><em>a</em><sub><em>n</em></sub> = 0</span>，则称列向量<spanclass="math inline">{<em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, <em>a</em><sub>3</sub>, …，<em>a</em><sub><em>n</em></sub>}</span>线性相关</p></li><li><p><strong>线性无关(Linear Independent)</strong>：与上述相反，如果不存在不全为 <strong>0</strong> 的向量 <spanclass="math inline"><em>X</em> = {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, <em>x</em><sub>3</sub>, …, <em>x</em><sub><em>n</em></sub>}</span>满足条件，则称之为线性无关</p></li></ul><p>随后我们简单回顾矩阵，矩阵的定义从向量导入，如果矩阵有m行和n列，我们就说矩阵的大小为<spanclass="math inline"><em>m</em> * <em>n</em></span>，如果<spanclass="math inline"><em>m</em> = <em>n</em></span>，我们称为方阵（<strong>squarematrix</strong>）</p><h2 id="矩阵的计算">矩阵的计算</h2><p>这里我们简单介绍矩阵的而一些计算与属性</p><ul><li><p>矩阵和标量相乘，即每个元素和标量相乘</p></li><li><p>矩阵（向量）和矩阵（向量）相乘， 即按照顺序依次相乘</p><ul><li>若<spanclass="math inline"><em>A</em><em>B</em> = <em>B</em><em>A</em></span>，则称A与B乘法可交换。n阶单位矩阵E与任何n阶矩阵乘法可交换</li></ul></li><li><p>矩阵的分块乘法</p></li><li><p>矩阵<strong>转置</strong></p></li><li><p>矩阵<strong>共轭</strong>:即矩阵内每个元素取其共轭的结果</p></li><li><p>矩阵的<strong>秩（Rank)</strong>:矩阵列向量中最大的线性无关的数目（The maximum number of LinearIndependent columns)</p></li><li><p>矩阵的<strong>零化度（Nullity）：</strong>矩阵的列数减去矩阵的秩</p></li></ul><h3 id="矩阵的逆">矩阵的逆</h3><p>如果两个方阵A和B的乘积是单位矩阵，AB=I，那么A和B就是互为逆矩阵</p><p>矩阵的逆有如下定义：</p><ul><li><p><spanclass="math inline">(<em>A</em><sup>−1</sup>)<sup>−1</sup> = <em>A</em></span></p></li><li><p><spanclass="math inline">(<em>k</em><em>A</em>)<sup>−1</sup> = <em>k</em><sup>−1</sup><em>A</em><sup>−1</sup></span></p></li><li><p><spanclass="math inline">(<em>A</em><sup><em>T</em></sup>)<sup>−1</sup> = (<em>A</em><sup>−1</sup>)<sup><em>T</em></sup></span></p></li><li><p><spanclass="math inline">(<em>A</em><em>B</em>)<sup>−1</sup> = <em>B</em><sup>−1</sup><em>A</em><sup>−1</sup></span></p></li></ul><p>矩阵的逆还有如下的性质（需要牢记）：</p><figure><img src="Rank.png" alt="矩阵逆的性质" /><figcaption aria-hidden="true">矩阵逆的性质</figcaption></figure><p>对于一个方阵，其可逆判别可用如下任意一条：</p><figure><img src="invert.jpg" alt="矩阵可逆的判别" /><figcaption aria-hidden="true">矩阵可逆的判别</figcaption></figure><h3 id="矩阵的行列式">矩阵的行列式</h3><p>具体计算我们带过，我们讨论一下行列式的性质</p><ul><li><p>交换任意两行，行列式取负号</p></li><li><p>行列式的标量乘法： <spanclass="math inline"><em>d</em><em>e</em><em>t</em>(2<em>A</em>) = 2<sup><em>n</em></sup><em>d</em><em>e</em><em>t</em>(<em>A</em>)</span></p></li><li><p>矩阵乘法的行列式：</p></li></ul><figure><img src="det.jpg" alt="矩阵乘法的行列式" /><figcaption aria-hidden="true">矩阵乘法的行列式</figcaption></figure><ul><li><p>如果一个方阵的行列式不为0，那么它是可逆的，反之，如果一个方阵可逆，那么它的行列式不为0</p></li><li><p><span class="math inline">$\bulletA^{-1}=\frac{1}{det(A)}C^T$</span> 其中<spanclass="math inline"><em>C</em></span>为伴随矩阵，具体如下</p></li></ul><p><span class="math display">$$C=\begin{bmatrix}c_{11} &amp; \cdots &amp; c_{1n} \\\vdots &amp; \ddots &amp; \vdots \\c_{n1} &amp; \cdots &amp; c_{nn}\end{bmatrix}$$</span></p><p>其中 <spanclass="math inline"><em>c</em><sub><em>i</em><em>j</em></sub> = (−1)<sup><em>i</em> + <em>j</em></sup><em>d</em><em>e</em><em>t</em>(<em>A</em><sub><em>i</em><em>j</em></sub>)</span>为代数余子式</p><h3 id="初等矩阵">初等矩阵</h3><p><strong>初等矩阵</strong>： 对单位矩阵进行一次变换后的矩阵</p><p>相关定理（<strong>左行右列</strong>）：</p><ul><li><p>对A施行初等<strong>行变换</strong>，其结果等于在A左边乘以相应的初等矩阵；</p></li><li><p>对A施行初等<strong>列变换</strong>，其结果等于在A右边乘以相应的初等矩阵</p></li><li><p>n阶方阵可逆的<strong>充要条件</strong>是它能表示成一些初等矩阵的乘积</p></li></ul><h2 id="线性方程组">线性方程组</h2><p>我们借助矩阵带一下线性方程组的相关概念与性质</p><p><strong>我们可以用矩阵表达一个线性方程组，即： <spanclass="math inline"><em>A</em><em>x</em> = <em>b</em></span>,其中A为矩阵，<spanclass="math inline"><em>x</em>, <em>b</em></span>分别为对应维度的列向量</strong></p><h3 id="解的情况与相容性">解的情况与相容性</h3><p>线性方程组解 <span class="math inline"><em>x</em></span>的情况有三种：<strong>无解、有唯一解、有无穷解</strong></p><p>若方程组有解，则称其为 <strong>相容（Consistent）</strong>；若是无解，则称之为 <strong>不相容（Inconsistent）</strong></p><h3 id="线性方程组有解的解释">线性方程组有解的解释</h3><p>首先介绍一个概念</p><ul><li><strong>张成的空间</strong>：对于一个向量集 <spanclass="math inline"><em>S</em></span> , 其向量的所有线性组合组成的向量集<span class="math inline"><em>V</em></span> ，称之为 <spanclass="math inline"><em>S</em></span> 张成的空间，记作 <spanclass="math inline"><em>S</em><em>p</em><em>a</em><em>n</em>(<em>S</em>)</span></li></ul><p>因此若 <spanclass="math inline"><em>A</em><em>x</em> = <em>b</em></span>有解，可以解释为：</p><ul><li><p><span class="math inline"><em>b</em></span> 是可以表示为矩阵<span class="math inline"><em>A</em></span> 的列向量的线性组合</p></li><li><p><span class="math inline"><em>b</em></span> 再矩阵 <spanclass="math inline"><em>A</em></span> 的列向量的所张成的空间里</p></li></ul><h3 id="解的数量与矩阵a的关系">解的数量与矩阵A的关系</h3><ul><li><p>有唯一解：矩阵A的列向量线性无关，即 <spanclass="math inline"><em>R</em><em>a</em><em>n</em><em>k</em>(<em>A</em>) = <em>n</em>(<em>N</em><em>u</em><em>l</em><em>l</em><em>i</em><em>t</em><em>y</em>(<em>A</em>)=0)</span></p></li><li><p>有无穷解：矩阵A的列向量线性相关，即 <spanclass="math inline"><em>R</em><em>a</em><em>n</em><em>k</em>(<em>A</em>) &lt; <em>n</em>(<em>N</em><em>u</em><em>l</em><em>l</em><em>i</em><em>t</em><em>y</em>(<em>A</em>)&gt;0)</span></p></li></ul><h3id="线性方程组求解高斯消元法gaussian-elimination">线性方程组求解：高斯消元法（GaussianElimination）</h3><p>首先有线性方程组等价定理：两个线性方程组等价 <spanclass="math inline">⇔</span> 两个线性方程组有相同的解</p><p><strong>高斯消元法</strong>：将增广矩阵化简为简约型阶梯形式，进而求解的方法</p><ul><li><p><strong>增广矩阵</strong>：将矩阵A与列向量b横向拼接起来，即 <spanclass="math inline">[<em>A</em>|<em>b</em>]</span></p></li><li><p><strong>简约型阶梯形式</strong>：即呈阶梯状，并且每个阶梯第一位为1，具体如下图：</p></li></ul><figure><img src="Echelon.jpg" alt="简约型阶梯形式示例" /><figcaption aria-hidden="true">简约型阶梯形式示例</figcaption></figure><p>若是最后化简得结果存在只有最后一列不为0得情况，则线性方程组无解;反之则有解，若是有一行全部为0，则存在无数解。</p><figure><img src="result.jpg" alt="解的分类" /><figcaption aria-hidden="true">解的分类</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>数学</category>
      
      <category>线性代数</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
      <tag>线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>博客写入教程</title>
    <link href="/BLOG/2025/05/24/%E5%8D%9A%E5%AE%A2%E5%86%99%E5%85%A5%E6%95%99%E7%A8%8B/"/>
    <url>/BLOG/2025/05/24/%E5%8D%9A%E5%AE%A2%E5%86%99%E5%85%A5%E6%95%99%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="markdown-写作模板含-front-matter-与语法指南">📘 Markdown写作模板（含 Front Matter 与语法指南）</h1><hr /><h2 id="第一部分-front-matter-配置说明">第一部分：📄 Front Matter配置说明</h2><blockquote><p>Front Matter 是 Markdown文件顶部的元信息，用于配置文章标题、标签、分类、日期等内容。通常写在<code>---</code> 包裹的 YAML 格式中。</p></blockquote><h3 id="示例模板">🧾 示例模板</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">post</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">我的第一篇博客</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2025-05-24 10:00:00</span><br><span class="hljs-attr">updated:</span> <span class="hljs-number">2025-05-24 12:00:00</span><br><span class="hljs-attr">comments:</span> <span class="hljs-literal">true</span><br><span class="hljs-attr">tags:</span> [<span class="hljs-string">Markdown</span>, <span class="hljs-string">教程</span>]<br><span class="hljs-attr">categories:</span> [<span class="hljs-string">技术笔记</span>]<br><span class="hljs-attr">permalink:</span> <span class="hljs-string">/markdown-guide.html</span><br><span class="hljs-attr">excerpt:</span> <span class="hljs-string">本文介绍了</span> <span class="hljs-string">Markdown</span> <span class="hljs-string">的语法与博客头部配置。</span><br><span class="hljs-attr">disableNunjucks:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">lang:</span> <span class="hljs-string">zh-CN</span><br><span class="hljs-attr">published:</span> <span class="hljs-literal">true</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><h3 id="字段说明表">📋 字段说明表</h3><table><thead><tr class="header"><th>字段名</th><th>说明</th><th>默认值</th></tr></thead><tbody><tr class="odd"><td><code>layout</code></td><td>页面布局模板（如 <code>post</code>、<code>page</code>）</td><td><code>config.default_layout</code></td></tr><tr class="even"><td><code>title</code></td><td>页面标题</td><td>文件名</td></tr><tr class="odd"><td><code>date</code></td><td>建立日期</td><td>文件创建时间</td></tr><tr class="even"><td><code>updated</code></td><td>最近更新日期</td><td>文件更新时间</td></tr><tr class="odd"><td><code>comments</code></td><td>是否启用评论</td><td><code>true</code></td></tr><tr class="even"><td><code>tags</code></td><td>标签数组，用于分类内容</td><td>空</td></tr><tr class="odd"><td><code>categories</code></td><td>分类数组（适用于归档）</td><td>空</td></tr><tr class="even"><td><code>permalink</code></td><td>自定义永久链接（应以 <code>/</code> 或 <code>.html</code>结尾）</td><td><code>null</code></td></tr><tr class="odd"><td><code>excerpt</code></td><td>页面摘要，建议结合插件处理</td><td>空</td></tr><tr class="even"><td><code>disableNunjucks</code></td><td>禁用 Nunjucks模板语法（<code>&#123;&#123; &#125;&#125;</code>、<code>&#123;% %&#125;</code>）</td><td><code>false</code></td></tr><tr class="odd"><td><code>lang</code></td><td>页面语言，覆盖默认配置</td><td>继承自 <code>_config.yml</code></td></tr><tr class="even"><td><code>published</code></td><td>是否发布该文章</td><td><code>_posts</code> 下为 <code>true</code>，草稿为<code>false</code></td></tr></tbody></table><hr /><h2 id="第二部分-markdown-语法速查手册">第二部分：📝 Markdown语法速查手册</h2><h3 id="标题">1️⃣ 标题</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-section"># 一级标题</span><br><span class="hljs-section">## 二级标题</span><br><span class="hljs-section">### 三级标题</span><br></code></pre></td></tr></table></figure><hr /><h3 id="段落与换行">2️⃣ 段落与换行</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs markdown">这是第一段。<br><br>这是第二段，  <br>这里使用两个空格加换行。<br></code></pre></td></tr></table></figure><hr /><h3 id="文本格式">3️⃣ 文本格式</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-strong">**加粗文本**</span><br><span class="hljs-emphasis">*斜体文本*</span><br>~~删除线文本~~<br><span class="hljs-code">`行内代码`</span><br></code></pre></td></tr></table></figure><hr /><h3 id="引用块">4️⃣ 引用块</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-quote">&gt; 一级引用</span><br>&gt;&gt; 二级引用<br></code></pre></td></tr></table></figure><hr /><h3 id="列表">5️⃣ 列表</h3><ul><li><strong>无序列表</strong></li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">-</span> 项目一<br><span class="hljs-bullet">-</span> 项目二<br></code></pre></td></tr></table></figure><ul><li><strong>有序列表</strong></li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">1.</span> 步骤一<br><span class="hljs-bullet">2.</span> 步骤二<br></code></pre></td></tr></table></figure><hr /><h3 id="代码块">6️⃣ 代码块</h3><ul><li><strong>行内代码</strong></li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">这是 <span class="hljs-code">`inline code`</span><br></code></pre></td></tr></table></figure><ul><li><strong>多行代码块</strong></li></ul><details><summary>Python 示例</summary><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">greet</span>():<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Hello Markdown!&quot;</span>)<br></code></pre></td></tr></table></figure></details><hr /><h3 id="表格">7️⃣ 表格</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs markdown">| 名称    | 类型   | 描述       |<br>|---------|--------|------------|<br>| title   | 字符串 | 文章标题   |<br>| date    | 日期   | 创建时间   |<br></code></pre></td></tr></table></figure><hr /><h3 id="链接与图片">8️⃣ 链接与图片</h3><ul><li><strong>链接</strong></li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">[<span class="hljs-string">OpenAI 官网</span>](<span class="hljs-link">https://www.openai.com</span>)<br></code></pre></td></tr></table></figure><ul><li><strong>图片</strong></li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">![<span class="hljs-string">替代文字</span>](<span class="hljs-link">https://via.placeholder.com/150</span>)<br></code></pre></td></tr></table></figure><hr /><h3 id="分隔线">9️⃣ 分隔线</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">---<br></code></pre></td></tr></table></figure><hr /><h3 id="任务列表github-支持">🔟 任务列表（GitHub 支持）</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">-</span> [x] 学习 Markdown<br><span class="hljs-bullet">-</span> [ ] 写一篇博客<br></code></pre></td></tr></table></figure><hr /><h3 id="转义字符防止符号被识别">🧩 转义字符（防止符号被识别）</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown">\<span class="hljs-emphasis">*星号\*</span> 不会加粗  <br>\<span class="hljs-code">`反引号\`</span> 不会变成代码  <br></code></pre></td></tr></table></figure><h3 id="公式说明">公式说明</h3><blockquote><p>$$ E = mc^2 $$ 可以实现公式的书写</p><p><spanclass="math display"><em>E</em> = <em>m</em><em>c</em><sup>2</sup></span></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>技术笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Markdown</tag>
      
      <tag>教程</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
