<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>数据挖掘课程复习</title>
    <link href="/BLOG/2025/06/11/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0/"/>
    <url>/BLOG/2025/06/11/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<blockquote><p>用于HITSZ 25春数据挖掘复习</p></blockquote><h1 id="概述">概述</h1><h2 id="数据挖掘的含义">数据挖掘的含义</h2><p>数据挖掘是从海量数据集中发现有趣的（<mark>非平凡的、隐含的、未被发现但有用的</mark>）模式、模型或知识的过程。</p><h2 id="数据挖掘过程">数据挖掘过程</h2><p>知识发现的过程：<br />1. 数据预处理<br /> 1. 数据清洗<br /> 2. 数据集成<br /> 3. 数据选择<br /> 4. 数据变换<br />2. 数据挖掘<br />3. 模式/模型评估<br />4. 知识表示</p><h2 id="数据挖掘的应用">数据挖掘的应用</h2><ul><li>数据分析与决策支持<ul><li>客户画像、需求分析</li><li>交叉市场分析</li></ul></li><li>风险分析与管理<ul><li>竞争分析</li><li>风险评估</li><li>资产管理</li><li>资源规划</li></ul></li><li>欺诈检测与异常模式挖掘<ul><li>医疗、电信等</li></ul></li></ul><h2 id="数据仓库">数据仓库</h2><p><strong>定义</strong>：数据仓库是<mark>面向主题的、集成的、随时间变化的、又相对稳定的</mark>数据集合，它支持管理层的决策过程<br /><strong>数据仓库的关键特征</strong>：</p><ul><li>面向主题的：<ul><li>数据仓库的数据是以分析的主题为中心构建的</li></ul></li><li>集成的：<ul><li>数据仓库的数据来自不同的数据源，需要按照统一的结构、格式、度量、语义进行数据处理，最后集成到数据仓库的主题</li></ul></li><li>随时间变化的：<ul><li>数据仓库的历史数据，需要随时间延长而增加（周期性更新）</li><li>数据仓库的综合数据，需要随时间延长而变化</li></ul></li><li>相对稳定的：<ul><li>数据仓库的数据主要用于支持决策，不会涉及频繁的修改与变化，主要是查询与分析。</li></ul></li></ul><hr /><h1 id="数据特征分析与预处理">数据特征分析与预处理</h1><h2 id="数据类型及其特征">数据类型及其特征</h2><h2 id="数据对象">数据对象：</h2><p>数据集由数据对象构成，数据对象又称之为样本、实例。</p><figure><img src="样本.png" alt="数据样本示意图" /><figcaption aria-hidden="true">数据样本示意图</figcaption></figure><h2 id="属性类型">属性类型</h2><p><strong>定义</strong>：一个数据字段，表示一个数据对象的某个特征</p><p><strong>分类</strong>：</p><ul><li><p><strong>标称属性:</strong> 与名称相关的值、分类或枚举属性<mark>(定性的)</mark></p><ul><li><strong>二元属性:</strong>标称属性的一种特殊情况，只有2个状态的标称属性。</li></ul></li><li><p><strong>序数属性:</strong> 值有一个有意义的顺序<mark>(定性的)</mark></p></li><li><p><strong>数值属性:</strong> 可度量的量 <mark>(定量的)</mark></p><ul><li><strong>区间标度属性:</strong><ul><li>在统一度量的尺度下</li><li>值有序（e.g. 日历日期， 温度计数据）</li><li>没有固有0点</li></ul></li><li><strong>比率标度属性：</strong><ul><li>具有固有零点，可以说一个值是一个值得多少倍</li><li>举例：字数、体重、工作年限</li></ul></li></ul></li></ul><h2 id="数据的描述统计">数据的描述统计</h2><ul><li><strong>数据离散特征：</strong>均值、众数、最值、分位数、方差、离群点……</li><li><strong>排序区间：</strong><ul><li>数据离散度： 多个粒度上得分析</li><li>排序区间得盆图/分位数图分析</li></ul></li></ul><p><strong>描述统计的图形显示：</strong>箱型图、直方图、分位数图、散点图</p><h2 id="数据可视化方法">数据可视化方法</h2><p><strong>数据可视化方法得分类：</strong></p><ul><li>基于像素的可视化技术<ul><li>协方差矩阵的热力图</li></ul></li><li>基于几何投影的可视化技术<ul><li>直接可视化、散点图、透视地形、平行坐标</li></ul></li><li>基于图标的可视化技术<ul><li>脸谱图、人物画像图</li></ul></li><li>基于分层的可视化技术<ul><li>树状图、<em>锥形树</em></li></ul></li><li>可视化复杂数据与关系<ul><li>标签云、线形图、饼图、结构图/流程图、<em>怪图、系统进化图</em></li></ul></li></ul><h2 id="测量数据的相似性和相异性">测量数据的相似性和相异性</h2><ul><li><strong>相似性：</strong> 度量数据的相似程度，越大越相似，通常范围在<span class="math display">[0,1]</span></li><li><strong>相异性：</strong>(e.g. distance)，度量数据的差异，最小值通常为0 ，越大差异越大</li></ul><p><strong>相异度矩阵：</strong>对角线为0的下三角矩阵，元素为对应对象之间的距离 <imgsrc="相异度矩阵.png" alt="相异度矩阵矩阵" /></p><h3 id="标称属性的邻近度量">标称属性的邻近度量</h3><ul><li><p>得分匹配：</p><p><span class="math display">$$d(i,j) = \frac{p - m}{p}$$</span></p><p>其中 <span class="math inline"><em>m</em></span>为相同变量数量，<span class="math inline"><em>p</em></span>为变量总数。</p></li><li><p>独热编码：<br />具体而言即将名称或类别转化为独热编码，随后利用编码计算距离</p></li></ul><h3 id="二进制属性的度量">二进制属性的度量</h3><figure><img src="列联表.png" alt="列联表" /><figcaption aria-hidden="true">列联表</figcaption></figure><p><strong>对称二元变量距离：</strong> <span class="math inline">$d(i,j)= \frac{r+s}{q+r+s+t}$</span></p><p><strong>不对称二元变量距离：</strong> <spanclass="math inline">$d(i,j) = \frac{r+s}{q+r+s}$</span></p><p><strong>Jaccard系数(不对称二元变量相似性)：</strong> <spanclass="math inline">$Sim_{Jaccard}(i,j) = \frac{q}{q+r+s}$</span></p><p><strong>TIP:</strong> 解释一下这个对称的含义，关键在于0是否有意义<img src="解释.png" alt="对称的含义解释" /></p><h4id="例题二进制属性的非对称相异性度量">例题：二进制属性的非对称相异性度量</h4><figure><img src="例题1.png" alt="二进制属性的非对称相异性度量" /><figcaption aria-hidden="true">二进制属性的非对称相异性度量</figcaption></figure><h3 id="数值属性的度量">数值属性的度量</h3><p><strong>闵可夫斯基距离公式：</strong></p><p><span class="math display">$$D(X, Y) = \left( \sum_{i=1}^n |x_i -y_i|^p \right)^{\frac{1}{p}}$$</span></p><p>其中 <spanclass="math inline"><em>p</em> ≥ 1</span>，是可调节的距离参数。</p><p>主要使用上述公式度量，其中有一些特例可以参照下图： <imgsrc="距离.png" alt="闵可夫斯基距离公式特例" /></p><h3 id="区间尺度与有序变量的度量">区间尺度与有序变量的度量</h3><ul><li>区间尺度：进行归一化或标准化处理随后计算闵可夫斯基距离</li><li>有序变量：使用排序代替值，并对排序值做最值归一化，随后计算闵可夫斯基距离</li></ul><h3 id="其他度量方法">其他度量方法：</h3><h4 id="余弦相似度">余弦相似度：</h4><p><span class="math display">$$\text{cos}(\theta) = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n}A_i^2} \cdot \sqrt{\sum_{i=1}^{n} B_i^2}}$$</span></p><p>用于衡量两个向量方向的相似程度，常用于文本分类、推荐系统等。</p><h4 id="kl-散度kullback-leibler-divergence">KL 散度（Kullback-LeiblerDivergence）：</h4><p><span class="math display">$$D_{KL}(P \parallel Q) = \sum_{i=1}^{n} p_i \log \frac{p_i}{q_i}$$</span></p><p>衡量两个概率分布之间的差异，常用于模型分布与目标分布之间的对比。</p><hr /><h2 id="数据预处理">数据预处理</h2><p><strong>目的：</strong>数据存在不完全、噪音、不一致的问题，高质量的数据挖掘依赖高质量的数据</p><h3 id="数据预处理的主要任务">数据预处理的主要任务：</h3><ul><li><strong>数据清理：</strong> 异常值、缺失值、噪音<ul><li>针对缺失值：删除整个样本、均值或聚类填充</li><li>针对异常值与噪音： 通过分箱、聚类、回归去除</li></ul></li><li><strong>数据集成：</strong> 多个来源合到一起</li><li><strong>数据变换：</strong> 归一化等操作</li><li><strong>数据归约：</strong> 减小数据的冗余<ul><li>维度规约： 减少无用特征，或产生新的特征代替原来的<ul><li>决策树规约、PCA</li></ul></li><li>数值规约：减小数据量，例如用模型参数+离群点代替原始数据<ul><li>线性回归、聚类</li></ul></li><li>数据压缩</li></ul></li><li><strong>数据离散化和概念分层：</strong><ul><li>连续变量离散化</li><li>人为指定偏序关系</li></ul></li></ul><hr /><h1 id="关联规则">关联规则</h1><h2 id="关联规则挖掘概念">关联规则挖掘概念</h2><p>一种从事务数据集中发现项与项之间有趣关系的技术，形式为：</p><p><spanclass="math display"><em>X</em> ⇒ <em>Y</em>,  <em>X</em> ∩ <em>Y</em> = ∅</span></p><h3 id="支持度support">支持度（Support）：</h3><p><span class="math display">$$\text{Support}(X \Rightarrow Y) = \frac{\text{包含 } X \cup Y \text{的事务数}}{\text{总事务数}}$$</span></p><h3 id="置信度confidence">置信度（Confidence）：</h3><p><span class="math display">$$\text{Confidence}(X \Rightarrow Y) = \frac{\text{Support}(X \cupY)}{\text{Support}(X)} = P(Y|X)$$</span></p><h3 id="提升度lift">提升度（Lift）：</h3><p><span class="math display">$$\text{Lift}(X \Rightarrow Y) =\frac{\text{Confidence}(X \RightarrowY)}{\text{Support}(Y)} =  \frac{P(Y|X)}{P(Y)}$$</span></p><p>Lift &gt; 1 表示正相关，=1 表示独立，&lt;1 表示负相关。</p><h2 id="频繁项集与其分类">频繁项集与其分类</h2><h3 id="频繁项集">频繁项集：</h3><p>如果一个项集 <span class="math inline"><em>X</em></span>的支持度满足：</p><p><span class="math display">Support(<em>X</em>) ≥ min_sup</span></p><p>则 <span class="math inline"><em>X</em></span> 是频繁项集。</p><h3 id="闭频繁项集closed">闭频繁项集（Closed）：</h3><p>项集 <span class="math inline"><em>X</em></span>是闭的，当不存在一个超集 <spanclass="math inline"><em>Y</em> ⊃ <em>X</em></span>，使得：</p><p><spanclass="math display">Support(<em>Y</em>) = Support(<em>X</em>)</span></p><h3 id="极大频繁项集maximal">极大频繁项集（Maximal）：</h3><p>项集 <span class="math inline"><em>X</em></span>是极大的，当不存在一个超集 <spanclass="math inline"><em>Y</em> ⊃ <em>X</em></span> 是频繁的。</p><h3 id="三者关系">三者关系：</h3><ul><li>极大频繁项集 ⊆ 闭频繁项集 ⊆ 所有频繁项集；</li><li>闭项集压缩信息不丢失，极大项集压缩更彻底但不保留精确支持度。</li></ul><h2 id="关联规则基本模型">关联规则基本模型</h2><p>关联规则是指同时满足最小支持度和最小置信度阈值的规则，形式如下：</p><p><spanclass="math display"><em>X</em> ⇒ <em>Y</em>,  <em>X</em> ∩ <em>Y</em> = ∅</span></p><h3 id="挖掘流程">挖掘流程：</h3><ol type="1"><li>找出所有频繁项集（满足 <spanclass="math inline">Support ≥ min_sup</span>）；</li><li>从频繁项集中生成所有满足 <spanclass="math inline">Confidence ≥ min_conf</span> 的规则。</li></ol><p>只有同时满足这两个条件的规则，才被称为“强规则”。</p><h2 id="apriori算法流程">📌 Apriori算法流程</h2><p>Apriori是一种经典的频繁项集挖掘算法，用于从大量事务数据中发现频繁项集和生成关联规则。</p><h3 id="核心思想apriori原理">核心思想：Apriori原理</h3><blockquote><p><strong>如果一个项集是频繁的，那么它的所有子集也一定是频繁的。</strong></p></blockquote><p>也就是说，<strong>如果某个项集不是频繁的，则其所有超集都不可能是频繁的</strong>，因此可以在搜索中直接剪枝，提升效率。</p><h3 id="算法计算流程">算法计算流程</h3><h4 id="步骤-1扫描数据库找出频繁-1-项集l₁">步骤 1️⃣：扫描数据库，找出频繁1 项集（L₁）</h4><ul><li>统计每个单项的支持度；</li><li>过滤出支持度 ≥ <code>min_sup</code> 的项；</li><li>得到频繁 1 项集集合 L₁。</li></ul><h4 id="步骤-2由-l₁-构造候选-2-项集c₂">步骤 2️⃣：由 L₁ 构造候选 2项集（C₂）</h4><ul><li><strong>连接步骤</strong>：将 L₁ 中的项两两组合生成 2项候选项集；</li><li><strong>剪枝步骤</strong>：剔除那些包含非频繁子集的候选项集；</li><li>重新扫描数据库统计每个候选项集的支持度；</li><li>保留支持度 ≥ <code>min_sup</code> 的项集，得到频繁 2 项集 L₂。</li></ul><h4 id="步骤-3迭代构造更高阶项集-l₃l₄">步骤 3️⃣：迭代构造更高阶项集L₃、L₄…</h4><ul><li>从 L₂ 生成 C₃，再从中提取 L₃；</li><li>重复 <strong>连接-剪枝-计数</strong> 的过程；</li><li>直到某一轮没有产生新的频繁项集。</li></ul><h4 id="步骤-4由频繁项集生成关联规则">步骤4️⃣：由频繁项集生成关联规则</h4><ul><li>对每个频繁项集 <spanclass="math inline"><em>L</em></span>，枚举其所有非空子集 <spanclass="math inline"><em>S</em></span>；</li><li>构造规则 <spanclass="math inline"><em>S</em> ⇒ (<em>L</em>−<em>S</em>)</span>；</li><li>计算置信度（Confidence）：</li></ul><p><span class="math display">$$\text{Confidence}(S \Rightarrow T) = \frac{\text{Support}(S \cupT)}{\text{Support}(S)}$$</span></p><ul><li>只保留满足 <span class="math inline">Confidence ≥ min_conf</span>的规则。</li></ul><h3 id="提高-apriori-算法效率的方法">提高 Apriori 算法效率的方法</h3><p>Apriori算法在面对大规模数据时效率较低，以下是常用的几种优化策略：</p><ol type="1"><li><p>Hash-based itemset counting（基于哈希的项集计数）</p><ul><li>将候选项集映射到哈希桶中；</li><li>如果某个哈希桶中的计数低于最小支持度，则其对应的项集可直接剪枝；</li><li><strong>作用：减少无效候选项集数量。</strong></li></ul></li><li><p>Transaction reduction（事务压缩）</p><ul><li>每轮扫描后，删除不包含任何频繁项集的事务；</li><li>后续迭代无需再扫描这些事务；</li><li><strong>作用：减少数据扫描量，提高效率。</strong></li></ul></li><li><p>Partitioning（划分策略）</p><ul><li>将数据库划分为多个子集，分别挖掘局部频繁项集；</li><li>将局部频繁项集合并，再在全库中验证；</li><li><strong>只需两次完整扫描</strong>，适合大数据和并行处理。</li></ul></li><li><p>Sampling（采样）</p><ul><li>从数据库中随机抽取部分事务作为样本；</li><li>在样本上挖掘频繁项集，再用全体数据验证；</li><li><strong>优点：快速；缺点：可能漏掉边缘频繁项集。</strong></li></ul></li></ol><h2 id="关联规则的度量">关联规则的度量</h2><p>除了上述说到的支持度、置信度、提升度以外，还有一些度量方式。这里主要是四种零不变性度量。</p><p>在关联规则中，某些度量值不受<strong>零事务（nulltransactions）</strong>影响，称为<strong>零不变性度量</strong>。</p><ul><li>零事务：不包含任何待考察项集的事务；</li><li>零不变性：度量值只由 <spanclass="math inline"><em>P</em>(<em>A</em>|<em>B</em>)</span> 和 <spanclass="math inline"><em>P</em>(<em>B</em>|<em>A</em>)</span>决定，与总事务数无关；</li></ul><p>以下四种度量具有<strong>零不变性</strong>，且其值范围都在 <spanclass="math inline">[0,1]</span>，值越大表示 A 与 B 的关联越紧密。</p><ul><li><strong>全置信度</strong> <span class="math display">$$\text{all_confidence}(A, B) = \frac{\text{sup}(A \cupB)}{\max(\text{sup}(A), \text{sup}(B))} = \min(P(A|B), P(B|A))$$</span></li><li><strong>最大置信度 </strong> <spanclass="math display">max_confidence(<em>A</em>,<em>B</em>) = max (<em>P</em>(<em>A</em>|<em>B</em>),<em>P</em>(<em>B</em>|<em>A</em>))</span></li><li><strong>余弦置信度</strong><br /><span class="math display">$$\text{cosine}(A, B) = \frac{P(A \cap B)}{\sqrt{P(A) \cdot P(B)}} =\sqrt{P(B|A) \cdot P(A|B)}$$</span></li><li><strong>Kelu置信度</strong><br /><span class="math display">$$\text{Kulc}(A, B) = \frac{1}{2} \left( P(A|B) + P(B|A) \right)$$</span></li></ul><h1 id="分类">分类</h1><h2 id="模型评估方法">模型评估方法</h2><ul><li>留出法： 按照比例随机划分出训练集和测试集，多次随机抽样取均值</li><li>交叉验证： 把数据划分为k份，每次选择一份作为测试集，重复k次，取均值<ul><li>留一法： 每份仅有一个样本</li></ul></li><li>Bootstrap 自助法:又放回的抽样m次作为训练集，没有被抽到的作为测试集</li></ul><h2 id="分类模型评估指标">分类模型评估指标</h2><p><strong>混淆矩阵（Confusion Matrix）</strong></p><table><thead><tr class="header"><th>实际/预测</th><th>正类（预测）</th><th>负类（预测）</th></tr></thead><tbody><tr class="odd"><td>正类（实际）</td><td>TP（真正例）</td><td>FN（假负例）</td></tr><tr class="even"><td>负类（实际）</td><td>FP（假正例）</td><td>TN（真负例）</td></tr></tbody></table><ul><li><strong>TP</strong>：预测为正，实际为正</li><li><strong>FP</strong>：预测为正，实际为负</li><li><strong>FN</strong>：预测为负，实际为正</li><li><strong>TN</strong>：预测为负，实际为负</li></ul><p><strong>指标：</strong></p><ul><li>准确率</li></ul><p><span class="math display">$$\text{Accuracy} = \frac{TP + TN}{TP + FP + FN + TN}$$</span></p><ul><li>召回率</li></ul><p><span class="math display">$$\text{Recall} = \frac{TP}{TP + FN}$$</span></p><ul><li>精确率</li></ul><p><span class="math display">$$\text{Precision} = \frac{TP}{TP + FP}$$</span></p><ul><li>F1-Score</li></ul><p><span class="math display">$$\text{F1} = \frac{2 \cdot \text{Precision} \cdot\text{Recall}}{\text{Precision} + \text{Recall}}$$</span></p><ul><li>F-beta</li></ul><p><span class="math display">$$\text{F1} = \frac{(1+\beta^2) \cdot \text{Precision} \cdot\text{Recall}}{\beta^2 \cdot \text{Precision} + \text{Recall}}$$</span></p><ul><li><p>P-R 曲线（Precision-Recall Curve）</p><ul><li><strong>横轴：Recall（召回率）</strong></li><li><strong>纵轴：Precision（精确率）</strong></li><li>通过不断调整分类阈值，绘制出一系列 (P, R) 点，形成一条曲线</li></ul></li><li><p>ROC 曲线（Receiver Operating Characteristic）</p><ul><li>横轴：FPR（假正率） = <span class="math inline">$\frac{FP}{FP +TN}$</span></li><li>纵轴：TPR（真正率） = 召回率 = <spanclass="math inline">$\frac{TP}{TP + FN}$</span></li><li>曲线下的面积（AUC）越接近 1，模型越好</li></ul></li></ul><h2 id="决策树归纳算法的流程以-id3c4.5cart-为例">🛠️决策树归纳算法的流程（以 ID3/C4.5/CART 为例）</h2><ol type="1"><li><p><strong>输入训练集</strong> <spanclass="math inline"><em>D</em></span>，每条样本有多个属性 +类别标签。</p></li><li><p><strong>判断停止条件</strong>：</p><ul><li>样本全属于一个类别 → 生成叶节点，停止；</li><li>属性已用完，或样本不足 → 用多数类别作为叶节点标记。</li></ul></li><li><p><strong>选择最优划分属性 A</strong>：</p><ul><li>使用某种<strong>划分指标</strong>（如信息增益、增益率、基尼指数）选择属性A；</li><li>若无可用属性 → 直接创建叶节点。</li></ul></li><li><p><strong>按属性 A 的取值划分数据集</strong>：</p><ul><li>对每个取值 <spanclass="math inline"><em>a</em><sub><em>i</em></sub></span>，将数据集划分为<span class="math inline"><em>D</em><sub><em>i</em></sub></span>；</li><li>为每个子集创建分支子节点。</li></ul></li><li><p><strong>对子节点递归重复上述过程</strong>：</p><ul><li>在子集 <spanclass="math inline"><em>D</em><sub><em>i</em></sub></span>上继续构建子树。</li></ul></li><li><p><strong>形成整棵决策树</strong>：</p><ul><li>直到所有子集都满足停止条件。</li></ul></li></ol><h2 id="属性选择度量">属性选择度量</h2><p>有三种，分别为 信息增益、增益率、基尼系数</p><h3 id="信息增益information-gain">信息增益（Information Gain）</h3><p>信息增益是 ID3 算法使用的度量标准，它衡量的是：使用某个属性 A对数据进行划分后，<strong>系统信息熵（混乱度）减少了多少</strong>。</p><p>公式：</p><p><span class="math display">$$\text{Gain}(D, A) = Ent(D) - \sum_{v \in \text{Values}(A)}\frac{|D_v|}{|D|} \cdot Ent(D_v)$$</span></p><ul><li>其中 <span class="math inline">$Ent(D) = -\sum_{i=1}^{m}p_i\text{log}_2 (p_i)$</span> 是原始数据集的熵 ；</li><li><span class="math inline"><em>D</em><sub><em>v</em></sub></span>表示属性 A 取值为 <span class="math inline"><em>v</em></span>的子集；</li><li>值越大表示“划分后不确定性下降得越多”，越适合作为划分属性。</li></ul><p>缺点：偏向于选择取值多的属性（比如身份证号），容易过拟合。</p><hr /><h3 id="增益率gain-ratio">增益率（Gain Ratio）</h3><p>为了解决信息增益偏好多值属性的问题，C4.5 算法引入了增益率：</p><p><span class="math display">$$\text{GainRatio}(D, A) = \frac{\text{Gain}(D, A)}{IV(A)}$$</span></p><ul><li><span class="math inline"><em>I</em><em>V</em>(<em>A</em>)</span> 是A 的“固有值信息”或“分裂信息”，可以理解为划分的复杂度。 <spanclass="math display">$$IV(A) = - \sum_{j=1}^{v}\frac{|D_j|}{D} \text{log}_2(\frac{|D_j|}{D})$$</span></li><li>增益率在衡量信息增益的同时，<strong>惩罚属性值过多的划分</strong>，更加平衡。</li></ul><hr /><h3 id="基尼指数gini-index">基尼指数（Gini Index）</h3><p>CART算法采用基尼指数来衡量属性划分的纯净程度。基尼指数越小，表示子集越“纯”。</p><p>某个集合 D 的基尼指数定义为：</p><p><span class="math display">$$\begin{align}Gini(D) &amp;= 1 - \sum_{k=1}^{K} p_k^2 \\Gini_{split}(D) &amp;= \frac{|D_1|}{|D|}Gini(D_1) +\frac{|D_2|}{|D|}Gini(D_2)\end{align}$$</span></p><ul><li><span class="math inline"><em>p</em><sub><em>k</em></sub></span>表示属于第 k 类的概率；</li><li>当样本全属于一个类别时，Gini = 0；</li><li>每次选择能<strong>最小化划分后加权 Gini</strong>的属性作为最优划分属性。</li></ul><hr /><p>✅ 总结对比：</p><table><thead><tr class="header"><th>度量方式</th><th>使用算法</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr class="odd"><td>信息增益</td><td>ID3</td><td>简单直观，计算熵有理论基础</td><td>偏向多值属性，可能过拟合</td></tr><tr class="even"><td>增益率</td><td>C4.5</td><td>纠正信息增益偏差，权衡复杂度</td><td>偏向取值较均匀的属性</td></tr><tr class="odd"><td>基尼指数</td><td>CART</td><td>计算更快，适合二叉树构建</td><td>没有熵那样的理论解释</td></tr></tbody></table><h2 id="常见分类方法的主要思想">常见分类方法的主要思想</h2><h3 id="朴素贝叶斯分类naive-bayes-classification">朴素贝叶斯分类（NaiveBayes Classification）</h3><p>朴素贝叶斯是一种基于<strong>贝叶斯定理</strong>并假设属性之间相互条件独立的概率分类方法。</p><ul><li><p>给定训练数据集 <spanclass="math inline"><em>D</em></span>，每个样本表示为一个 <spanclass="math inline"><em>n</em></span> 维属性向量：<br /><spanclass="math display"><em>X</em> = (<em>x</em><sub>1</sub>,<em>x</em><sub>2</sub>,...,<em>x</em><sub><em>n</em></sub>)</span></p></li><li><p>假设总共有 <span class="math inline"><em>m</em></span>个类别：<spanclass="math inline"><em>C</em><sub>1</sub>, <em>C</em><sub>2</sub>, ..., <em>C</em><sub><em>m</em></sub></span><br />目标是确定输入 <span class="math inline"><em>X</em></span>属于哪个类别，即计算后验概率最大的类别： <spanclass="math display"><em>Ĉ</em> = arg max<sub><em>C</em><sub><em>i</em></sub></sub><em>P</em>(<em>C</em><sub><em>i</em></sub>∣<em>X</em>)</span></p></li><li><p>根据<strong>贝叶斯定理</strong>： <span class="math display">$$P(C_i \mid X) = \frac{P(C_i) \cdot P(X \mid C_i)}{P(X)}$$</span></p></li><li><p>因为 <span class="math inline"><em>P</em>(<em>X</em>)</span>对所有类别都相同，所以在分类时只需最大化分子部分： <spanclass="math display"><em>Ĉ</em> = arg max<sub><em>C</em><sub><em>i</em></sub></sub><em>P</em>(<em>C</em><sub><em>i</em></sub>) ⋅ <em>P</em>(<em>X</em>∣<em>C</em><sub><em>i</em></sub>)</span></p></li><li><p>若假设特征条件独立，则有： <span class="math display">$$P(X \mid C_i) = \prod_{k=1}^{n} P(x_k \mid C_i)$$</span></p></li></ul><h4 id="laplacian-correction拉普拉斯校准">LaplacianCorrection（拉普拉斯校准）</h4><p>为了解决0概率问题，引入 <strong>Laplacian 平滑（也叫 Add-OneSmoothing）</strong>，公式调整为：</p><p><span class="math display">$$P(x_k \mid C_i) = \frac{N_{ik} + 1}{N_i + d}$$</span></p><ul><li><spanclass="math inline"><em>N</em><sub><em>i</em><em>k</em></sub></span>：类别<span class="math inline"><em>C</em><sub><em>i</em></sub></span> 中属性<span class="math inline"><em>A</em><sub><em>k</em></sub></span> 取值为<span class="math inline"><em>x</em><sub><em>k</em></sub></span>的样本数<br /></li><li><spanclass="math inline"><em>N</em><sub><em>i</em></sub></span>：类别 <spanclass="math inline"><em>C</em><sub><em>i</em></sub></span>的总样本数<br /></li><li><span class="math inline"><em>d</em></span>：属性 <spanclass="math inline"><em>A</em><sub><em>k</em></sub></span>的可能取值数（即类别数）</li></ul><p>这样，即使某个值从未出现（<spanclass="math inline"><em>N</em><sub><em>i</em><em>k</em></sub> = 0</span>），加1 后也不会导致整个概率为 0。</p><h3 id="knn-分类k-nearest-neighbors">KNN 分类（K-NearestNeighbors）</h3><p>KNN（K近邻）是一种<strong>基于实例的非参数分类方法</strong>，核心思想是：<strong>“相似样本具有相似的类别。”</strong></p><p>基本流程：</p><ul><li>给定一个待分类样本，计算它与训练集中所有样本之间的距离（如欧几里得距离）；</li><li>选出距离最近的 <span class="math inline"><em>K</em></span>个“邻居”；</li><li>让这些邻居“投票”，投票最多的类别即为预测结果。</li></ul><h3 id="逻辑回归logistic-regression">逻辑回归（LogisticRegression）</h3><p>逻辑回归是一种<strong>用于二分类任务的线性模型</strong>，其本质是学习一个函数，输出属于某一类别的概率。</p><h4 id="基本思想">基本思想：</h4><ul><li><p>给定输入特征 <spanclass="math inline"><em>X</em> = (<em>x</em><sub>1</sub>,<em>x</em><sub>2</sub>,...,<em>x</em><sub><em>n</em></sub>)</span>，逻辑回归学习一个线性组合：</p><p><spanclass="math display"><em>z</em> = <em>w</em><sup><em>T</em></sup><em>x</em> + <em>b</em></span></p></li><li><p>然后通过<strong>Sigmoid 函数</strong>将其映射到 <spanclass="math inline">[0,1]</span> 范围，作为正类的概率：</p><p><span class="math display">$$P(y = 1 \mid x) = \frac{1}{1 + e^{-z}} = \frac{1}{1 + e^{-(w^T x + b)}}$$</span></p></li><li><p>预测时，通常设置阈值 <span class="math inline">0.5</span>，若<span class="math inline"><em>P</em> ≥ 0.5</span>则预测为正类，否则为负类。</p></li><li><p>没有闭合解，通常使用梯度下架配合交叉熵损失求解（在训练数据上拟合）</p></li></ul><h3 id="神经网络">神经网络</h3><p><maek>跳过</mark></p><h3 id="支持向量机svm分类">支持向量机（SVM）分类</h3><p>支持向量机是一种<strong>二分类模型</strong>，通过寻找一个最佳分割超平面，将不同类别的数据点分开。</p><h4 id="核心思想">核心思想：</h4><ul><li>寻找一个<strong>最大间隔（Margin）</strong>的超平面，使得距离超平面最近的样本点（支持向量）距离最大化；</li><li>最大间隔有助于提高模型的泛化能力。</li></ul><h4 id="线性可分情况">线性可分情况：</h4><ul><li><p>给定训练样本，SVM 找到一个线性超平面满足：</p><p><spanclass="math display"><em>w</em><sup><em>T</em></sup><em>x</em> + <em>b</em> = 0</span></p></li><li><p>并满足分类约束：</p><p><spanclass="math display"><em>y</em><sub><em>i</em></sub>(<em>w</em><sup><em>T</em></sup><em>x</em><sub><em>i</em></sub>+<em>b</em>) ≥ 1,  <em>i</em> = 1, 2, …, <em>n</em></span></p></li><li><p>目标是最大化间隔，即最小化 <spanclass="math inline">∥<em>w</em>∥<sup>2</sup></span>。</p></li></ul><h4 id="线性不可分情况">线性不可分情况：</h4><ul><li><p>使用<strong>软间隔（SoftMargin）</strong>，允许一定程度的分类错误，加入松弛变量 <spanclass="math inline"><em>ξ</em><sub><em>i</em></sub></span>；</p></li><li><p>目标函数变为：</p><p><span class="math display">$$\min_{w,b} \frac{1}{2}\|w\|^2 + C \sum_{i=1}^n \xi_i$$</span></p></li><li><p>其中 <span class="math inline"><em>C</em></span>控制误差惩罚强度。</p></li></ul><h4 id="非线性分类">非线性分类：</h4><ul><li>通过<strong>核函数（Kernel）</strong>，将数据映射到高维空间，使其线性可分；</li><li>常用核函数包括：<ul><li>线性核：<spanclass="math inline"><em>K</em>(<em>x</em>,<em>z</em>) = <em>x</em><sup><em>T</em></sup><em>z</em></span></li><li>多项式核：<spanclass="math inline"><em>K</em>(<em>x</em>,<em>z</em>) = (<em>x</em><sup><em>T</em></sup><em>z</em>+<em>c</em>)<sup><em>d</em></sup></span></li><li>径向基函数核（RBF）：<spanclass="math inline"><em>K</em>(<em>x</em>,<em>z</em>) = exp (−<em>γ</em>∥<em>x</em>−<em>z</em>∥<sup>2</sup>)</span></li></ul></li></ul><h3 id="集成方法ensemble-methods">集成方法（Ensemble Methods）</h3><p>集成方法是将多个<strong>弱分类器</strong>组合成一个强分类器的技术，旨在提高模型的准确性和稳定性。</p><h4 id="基本思想-1">基本思想：</h4><ul><li>通过结合多个模型的预测结果，减少单个模型的偏差和方差；</li><li>常见方式包括投票（分类）或平均（回归）。</li></ul><h4 id="主要类型">主要类型：</h4><ol type="1"><li><strong>Bagging（Bootstrap Aggregating）</strong><ul><li>通过自助采样（有放回抽样）构建多个训练子集，训练多个模型；<br /></li><li>预测时对多个模型结果投票或平均；<br /></li><li>代表算法：随机森林（Random Forest）。</li></ul></li><li><strong>Boosting</strong><ul><li>逐步训练一系列弱分类器，每个分类器关注前一个分类器错误分类的样本；<br /></li><li>通过加权组合提升整体性能；<br /></li><li>代表算法：AdaBoost、Gradient Boosting、XGBoost。</li></ul></li><li><strong>Stacking（堆叠）</strong><ul><li>训练多个不同类型的基模型；<br /></li><li>使用另一个模型（元学习器）学习如何组合基模型的输出。</li></ul></li></ol><h1 id="聚类">聚类</h1><p><strong>聚类分析：</strong>将数据对象分为多个类或簇，类内对象相似，类间对象相异</p><h2 id="基于划分的聚类方法">基于划分的聚类方法</h2><p>构造n个对象数据集D的划分，将其划分为k个列（所以需要给定K值的是基于划分方法的）</p><h3 id="k-平均聚类算法">K-平均聚类算法</h3><p><strong>步骤：</strong></p><ul><li>选择一个含有随机选择样本的k个簇的初始划分，计算这些簇的质心</li><li>根据欧氏距离把剩余的每个样本分配到距离它最近的簇质心的划分</li><li>计算被分配到每个簇的样本的均值向量，作为新的簇的质心</li><li>重复2,3直到k个簇的质心点不再发生变化或平方误差准则最小<ul><li>这里平方误差是说每个簇内样本到中心的平方误差的总和的总和</li></ul></li></ul><p><strong>优点:</strong> - 相对有效性，复杂度为 <spanclass="math inline"><em>O</em>(<em>k</em><em>n</em><em>t</em>)</span>，其中 n 是对象数目, k 是簇数目, t 是迭代次数</p><p><strong>缺点：</strong> - 需要预先指顶簇的数目k, -不能处理噪音数据和孤立点(outliers) -不适合用来发现具有非凸形状(non-convex shapes)的簇</p><h3 id="k-中心点聚类算法">K-中心点聚类算法</h3><p>主要用于解决K-均值在异常值上的缺点</p><p><strong>k-中心点算法流程:</strong> 1. 任意选取 k 个点作为 中心点</p><ol start="2" type="1"><li><p>按照与中心点最近的原则，将剩余点分配到当前最佳的中心点代表的类中</p></li><li><p>在每一类中，计算每个成员点对应的准则函数，选取准则函数最小时对应的点作为新的中心点</p></li><li><p>重复2-3的过程，直到所有的 中心点点不再发生变化，或已达到设定的最大迭代次数</p></li></ol><blockquote><p>其中准则函数为，一类中，某个成员点和其他成员点的距离之和</p></blockquote><p><strong>优点：</strong> 当存在噪音和孤立点时, K-medoids 比 K-means更健壮。 <strong>缺点：</strong> K-medoids 对于小数据集工作得很好,但不能很好地用于大数据集，计算质心的步骤时间复杂度是<spanclass="math inline"><em>O</em>(<em>n</em><sup>2</sup>)</span>，运行速度较慢</p><h3 id="pam算法">PAM算法</h3><p>是最早提出的k-中心点聚类算法</p><p><strong>PAM算法流程：</strong><br />1. 首先随机选择k个对象作为中心，把每个对象分配给离它最近的中心。<br />2. 对每个Medoid和非Medoid点(<spanclass="math inline"><em>O</em>(<em>k</em>(<em>n</em>−<em>k</em>))</span>)，尝试交换它们的位置，计算新的聚类代价(<spanclass="math inline"><em>O</em>(<em>n</em>−<em>k</em>)</span>)。<br />3.如果总的损失减少，则交换中心对象和非中心对象；如果总的损失增加，则不进行交换</p><p><strong>优点：</strong> 当存在噪音和孤立点时, K-medoids 比 K-means更健壮；<br /><strong>缺点：</strong> 同K-medoids， 每轮复杂度<spanclass="math inline"><em>O</em>(<em>k</em>(<em>n</em>−<em>k</em>)<sup>2</sup>)</span></p><h3 id="clara算法">CLARA算法</h3><p>不考虑整个数据集, 而是选择数据的一小部分作为样本</p><p><strong>CLARA算法流程：</strong><br />1. 从数据集中抽取一个样本集, 并对样本集使用PAM，划分为k个簇<br />2. 将其他未被抽到的样本划分到上述确定的簇中<br />3. 重复上述步骤，选择最好的聚类作为输出</p><p><strong>优点:</strong> 可以处理的数据集比 PAM大</p><p><strong>缺点:</strong> - 有效性依赖于样本集的大小 -基于样本的好的聚类并不一定是 整个数据集的好的聚类, 样本可能发生倾斜</p><h3 id="clarans算法">CLARANS算法</h3><h4 id="输入参数">输入参数：</h4><ul><li>数据集 <span class="math inline"><em>D</em></span>，样本个数 <spanclass="math inline"><em>n</em></span></li><li>聚类数 <span class="math inline"><em>k</em></span></li><li>每轮最大尝试次数 <code>max_neighbor</code></li><li>总共执行的局部搜索次数 <code>num_local</code></li></ul><h4 id="多次随机局部搜索共执行-num_local-次">多次随机局部搜索（共执行<code>num_local</code> 次）：</h4><p>对于每次局部搜索：</p><ol type="1"><li>随机选择初始的 <span class="math inline"><em>k</em></span> 个Medoids<br /></li><li>当前 Medoids 作为初始解，循环尝试邻居（即交换一个 Medoid 和一个非Medoid）：<ul><li>随机选择一个非 Medoid 点，与当前某个 Medoid交换，得到一个“邻居解”</li><li>计算这个新解的聚类代价</li><li>如果代价降低，则更新当前解为这个新解</li></ul></li><li>重复上述步骤，直到连续 <code>max_neighbor</code>次尝试都没有找到更优解</li><li>记录该局部搜索中最好的解</li></ol><h4 id="最终输出">最终输出：</h4><p>在 <code>num_local</code>次局部搜索中，选择代价最低的那个作为最终聚类结果</p><h2 id="基于层次的聚类方法">基于层次的聚类方法</h2><p>层次的聚类方法将数据对象组成一棵聚类的树,可以进一步分为: -凝聚的(agglomerative)层次聚类 (自底向上形成) - 分裂的(divisive)层次聚类(自顶向下形成)</p><h3 id="agnes-agglomerative-nesting算法">AGNES (AgglomerativeNesting)算法</h3><p><strong>算法流程：</strong></p><ul><li>初始化：计算包含每对样本间距离（如欧氏距离）的相似矩阵，把每个样本作为一个簇；<br /></li><li>选择：使用相似矩阵查找最相似的两个簇；<ul><li>两个簇间的相似度由这两个不同簇中距离最近的数据点对的相似度来确定－－单链接方法(Single-link)<br /></li></ul></li><li>更新：<ul><li>将两个簇合并为一个簇，簇的个数通过合并被更新；<br /></li><li>同时更新相似矩阵，将两个簇的两行（两列）距离用1行（1列）距离替换反映合并操作<br /></li></ul></li><li>重复：执行n-1次选择与更新；<br /></li><li>结束：当所有样本都合并成一个簇或满足某个终止条件时，整个过程结束</li></ul><h3 id="diana-divisive-analysis算法">DIANA (Divisive Analysis)算法</h3><p><strong>主要思想：</strong>采用自顶向下策略</p><ul><li>首先将所有样本置于一个簇中；</li><li>然后逐渐细分为越来越小的簇，来增加簇的数目；</li><li>直到每个样本自成一个簇，或者达到某个终结条件。</li><li>例如达到了某个希望的簇的数目或两个最近的簇之间的距离超过了某个阈值。</li></ul><h3 id="birch算法流程">BIRCH算法流程</h3><p>BIRCH 是一种基于层次结构的聚类算法，适合大规模数据，使用 CF-Tree来压缩数据并高效聚类。</p><h4 id="关键数据结构cfclustering-feature">关键数据结构：CF（ClusteringFeature）</h4><p>每个聚类用三元组表示：</p><p><spanclass="math display">CF = (<em>N</em>,<strong>L</strong><strong>S</strong>,<strong>S</strong><strong>S</strong>)</span></p><ul><li><span class="math inline"><em>N</em></span>：簇中点数<br /></li><li><span class="math inline">$\mathbf{LS} = \sum_{i=1}^N\mathbf{x}_i$</span>（点坐标线性和）<br /></li><li><span class="math inline">$\mathbf{SS} = \sum_{i=1}^N\mathbf{x}_i^2$</span>（点坐标平方和）</li></ul><p>利用 CF 可快速计算簇的质心、半径和直径，无需访问原始点。</p><h4 id="cf-tree-结构">CF-Tree 结构</h4><ul><li>一种平衡树，类似 B+ 树<br /></li><li><strong>非叶节点</strong>存储子节点的 CF 信息<br /></li><li><strong>叶子节点</strong>存储实际的 CF 条目，每个 CF表示一个子簇<br /></li><li>叶子节点通过链表相连，支持顺序访问<br /></li><li>主要参数：<ul><li><spanclass="math inline"><em>B</em></span>：非叶节点最大子节点数<br /></li><li><span class="math inline"><em>L</em></span>：叶子节点最大 CF条目数<br /></li><li>阈值 <span class="math inline"><em>T</em></span>：限制每个 CF最大半径</li></ul></li></ul><h4 id="算法流程">算法流程</h4><ol type="1"><li><strong>初始化</strong>：设定阈值 <spanclass="math inline"><em>T</em></span>、<spanclass="math inline"><em>B</em></span>、<spanclass="math inline"><em>L</em></span>，初始化空 CF-Tree。<br /></li><li><strong>逐点插入</strong>：<ul><li>从根节点开始递归，找到距离点最近的叶子节点 CF。<br /></li><li>尝试将点合并入该 CF，若合并后半径 ≤ <spanclass="math inline"><em>T</em></span>，更新 CF；否则新建 CF 条目。<br /></li></ul></li><li><strong>节点分裂</strong>：<ul><li>叶子节点 CF 条目数超过 <span class="math inline"><em>L</em></span>时分裂：<ul><li>选择两个最远的 CF 条目作为种子，重新分配其余 CF 条目。<br /></li><li>生成两个叶子节点，更新父节点。<br /></li></ul></li><li>父节点子节点数超过 <span class="math inline"><em>B</em></span>时递归分裂，直到根节点。<br /></li></ul></li><li><strong>完成构建</strong>。<br /></li><li><strong>全局聚类（可选）</strong>：对叶子节点所有 CF的质心进行聚类（如 K-Means）(在每个叶子节点的簇的层面再聚类)。</li></ol><h4 id="算法优缺点">算法优缺点</h4><ul><li><strong>优点</strong>：<ul><li>只需一次扫描，速度快<br /></li><li>数据压缩存储，节省内存<br /></li><li>支持增量更新和大规模数据<br /></li></ul></li><li><strong>缺点</strong>：<ul><li>对异常点敏感<br /></li><li>结果依赖阈值 <span class="math inline"><em>T</em></span> 选取</li></ul></li></ul><h3 id="cure算法简介含抽样处理">CURE算法简介（含抽样处理）</h3><h4 id="核心思想-1">核心思想</h4><ul><li>用多个代表点描述簇的形状，避免用质心造成的误差<br /></li><li>代表点收缩至簇中心以减少异常点影响<br /></li><li>支持复杂簇结构，抗噪声能力强</li></ul><h4 id="算法流程-1">算法流程</h4><ol type="1"><li><strong>抽样（Sampling）</strong><ul><li>从大数据中随机抽取一个代表样本子集，减小计算量</li></ul></li><li><strong>初始聚类</strong><ul><li>对抽样数据，每个点看作一个簇，或者先用快速算法做粗聚类，得到初始簇集合</li></ul></li><li><strong>选择代表点</strong><ul><li>每个簇选固定数量的代表点（如距离簇质心较远的点）</li></ul></li><li><strong>收缩代表点</strong><ul><li>将代表点沿向簇中心的方向收缩一定比例（如20%-30%）</li></ul></li><li><strong>合并簇</strong><ul><li>计算簇间代表点的最小距离，合并距离最近的两个簇</li></ul></li><li><strong>重复步骤 3-5</strong><ul><li>直到满足聚类数或者其他停止条件</li></ul></li><li><strong>增量聚类（可选）</strong><ul><li>对未抽样的数据，将每个点分配到最近的簇中，实现全量数据聚类</li></ul></li></ol><h4 id="特点">特点</h4><ul><li>抽样减少计算量<br /></li><li>代表点和收缩减少异常点影响<br /></li><li>能处理非球形簇，结构复杂</li></ul><h3 id="rock-算法简介">ROCK 算法简介</h3><p>ROCK（Robust Clustering usinglinKs）是一种基于邻居“链接”关系的层次聚类算法，特别适合处理类别数据和非球形簇。</p><h4 id="关键概念">关键概念</h4><ul><li><p><strong>邻居（Neighbors）</strong><br />如果两个数据点之间的相似度（例如 Jaccard 相似度）超过某个阈值 <spanclass="math display"><em>θ</em></span>，则认为它们是邻居。</p></li><li><p><strong>链接数（Link）</strong><br />两个点的链接数是它们共同邻居的数量。链接数越大，说明两点越有可能属于同一个簇。</p></li><li><p><strong>Jaccard 相似度</strong><br />衡量两个集合相似度的指标，定义为它们交集的大小除以并集的大小，用来计算点之间的相似性。</p></li><li><p><strong>好处度（Goodness Measure）</strong><br />用于评估合并两个簇的合理性。<br />计算公式为：</p><p><span class="math display">$$Goodness(C_i, C_j) = \frac{links(C_i, C_j)}{(size(C_i) + size(C_j))^{1 +2f(\theta)} - size(C_i)^{1 + 2f(\theta)} - size(C_j)^{1 + 2f(\theta)}}$$</span></p><p>其中：</p><ul><li><spanclass="math display"><em>l</em><em>i</em><em>n</em><em>k</em><em>s</em>(<em>C</em><sub><em>i</em></sub>,<em>C</em><sub><em>j</em></sub>)</span>是两个簇间所有点对的链接数之和<br /></li><li><spanclass="math display"><em>s</em><em>i</em><em>z</em><em>e</em>(<em>C</em>)</span>是簇中点的数量<br /></li><li><span class="math display"><em>f</em>(<em>θ</em>)</span>是阈值相关的函数，用于调节合并力度</li></ul></li></ul><h4 id="算法流程-2">算法流程</h4><ol type="1"><li><strong>计算邻居关系</strong><ul><li>对数据集中每对数据点计算相似度（如 Jaccard 相似度）。<br /></li><li>如果相似度大于阈值 <spanclass="math display"><em>θ</em></span>，则认为它们是邻居。</li></ul></li><li><strong>计算链接数</strong><ul><li>对每对数据点，统计它们共同邻居的数量，这个数量称为“链接数”。</li></ul></li><li><strong>初始化簇</strong><ul><li>将每个数据点作为一个单独的簇。</li></ul></li><li><strong>计算簇间好处度</strong><ul><li>根据簇间所有点对的链接数以及簇大小，计算两个簇合并的好处度（GoodnessMeasure）。</li></ul></li><li><strong>合并簇</strong><ul><li>选择好处度最高的两个簇进行合并。</li></ul></li><li><strong>重复合并</strong><ul><li>不断重复计算好处度和合并操作，直到达到预定的簇数或满足其他终止条件。</li></ul></li></ol><h4 id="特点-1">特点</h4><ul><li>适合类别数据和非球形簇<br /></li><li>利用邻居链接捕捉复杂的结构关系<br /></li><li>计算量较大，适合中小规模数据</li></ul><h3 id="chameleon算法">CHAMELEON算法</h3><p>CHAMELEON是一种基于图的层次聚类算法，能够自动发现不同形状和密度的簇，适用于复杂数据结构。它结合了动态模型和多阶段聚类，利用簇的内部连接性和相似性来决定合并策略。</p><h4 id="核心思想-2">核心思想</h4><ul><li>利用<strong>k-近邻图（k-NN Graph）</strong>表示数据点间的关系<br /></li><li>通过度量簇间的<strong>相对连通性（RelativeConnectivity）</strong>和<strong>相对相似性（RelativeCloseness）</strong>，决定簇是否合并<br /></li><li>动态调整合并过程，适应簇的形状和密度差异</li></ul><h4 id="主要概念">主要概念</h4><ul><li><p><strong>k-近邻图</strong><br />构建一个图，点为数据样本，边连接每个点的k个最近邻，边权反映点之间的相似度</p></li><li><p><strong>相对连通性（Relative Connectivity, RC）</strong><br />衡量两个簇间连接强度相对于它们内部连接的强度</p></li><li><p><strong>相对相似性（Relative Closeness, RCl）</strong><br />衡量两个簇之间的距离相对于它们各自的内部距离</p></li></ul><h4 id="算法流程-3">算法流程</h4><ol type="1"><li><strong>构建k-近邻图</strong><ul><li>计算数据点间的相似度，构造加权k-近邻图</li></ul></li><li><strong>初始聚类</strong><ul><li>使用图划分方法（如基于最小割的聚类）将数据划分成多个细粒度的小簇</li></ul></li><li><strong>计算簇间相似度</strong><ul><li>对每对簇计算相对连通性（RC）和相对相似性（RCl）</li></ul></li><li><strong>动态合并簇</strong><ul><li>根据RC和RCl的综合评价，选择合适的簇对进行合并</li></ul></li><li><strong>重复合并</strong><ul><li>直到满足预设的簇数或没有合适的簇对可以合并</li></ul></li></ol><h4 id="特点-2">特点</h4><ul><li>适应不同簇形状和密度<br /></li><li>结合了局部结构和全局信息<br /></li><li>适合处理复杂数据集，但计算复杂度较高</li></ul><h2 id="基于密度的聚类方法">基于密度的聚类方法</h2><h3 id="密度的概念">密度的概念</h3><h4 id="ε-邻域epsilon-neighborhood">1. ε-邻域（EpsilonNeighborhood）</h4><ul><li>给定一个点 <span class="math display"><em>p</em></span>，其<strong>ε-邻域</strong> 是所有与点 <spanclass="math display"><em>p</em></span> 距离不超过 <spanclass="math display"><em>ε</em></span> 的点的集合。</li><li>通俗地说，就是以点 <span class="math display"><em>p</em></span>为圆心，半径为 <span class="math display"><em>ε</em></span>的“圆”里有谁。</li></ul><h4 id="核心对象core-object">2. 核心对象（Core Object）</h4><ul><li>如果一个点的 ε-邻域内至少包含<strong>MinPts</strong>（最小点数，包括它自己）个点，则称它为<strong>核心对象</strong>。</li><li>它说明该区域“密度足够大”。</li></ul><h4 id="直接密度可达directly-density-reachable-ddr">3.直接密度可达（Directly Density-Reachable, DDR）</h4><ul><li>如果点 <span class="math display"><em>p</em></span> 在点 <spanclass="math display"><em>q</em></span> 的 ε-邻域内，<strong>且 <spanclass="math display"><em>q</em></span>是核心对象</strong>，那么我们说：<br /><strong>点 <span class="math display"><em>p</em></span> 是从点 <spanclass="math display"><em>q</em></span> 直接密度可达的</strong>。</li><li>注意：只有从核心对象出发，才能有“直接密度可达”。</li></ul><h4 id="密度可达density-reachable">4. 密度可达（Density-Reachable）</h4><ul><li>如果存在一条由多个点组成的链 <spanclass="math display"><em>p</em><sub>1</sub>, <em>p</em><sub>2</sub>, ..., <em>p</em><sub><em>n</em></sub></span>，使得：<ul><li><spanclass="math display"><em>p</em><sub>1</sub> = <em>q</em></span>，<spanclass="math display"><em>p</em><sub><em>n</em></sub> = <em>p</em></span>；</li><li>对于每对相邻点 <spanclass="math display"><em>p</em><sub><em>i</em></sub></span> 和 <spanclass="math display"><em>p</em><sub><em>i</em> + 1</sub></span>，都有<span class="math display"><em>p</em><sub><em>i</em> + 1</sub></span>是从 <span class="math display"><em>p</em><sub><em>i</em></sub></span><strong>直接密度可达</strong>的；</li></ul></li><li>那么我们说：<strong>点 <span class="math display"><em>p</em></span>是从点 <span class="math display"><em>q</em></span>密度可达的</strong>。</li></ul><blockquote><p>这个关系是<strong>单向的</strong>：即 <spanclass="math display"><em>p</em></span> 从 <spanclass="math display"><em>q</em></span> 可达，不代表 <spanclass="math display"><em>q</em></span> 从 <spanclass="math display"><em>p</em></span> 可达。</p></blockquote><h4 id="密度相连density-connected">5. 密度相连（Density-Connected）</h4><ul><li>如果存在某个点 <span class="math display"><em>o</em></span>，使得：<ul><li>点 <span class="math display"><em>p</em></span> 和点 <spanclass="math display"><em>q</em></span> 都是从 <spanclass="math display"><em>o</em></span> 密度可达的；</li></ul></li><li>那么我们说：<strong>点 <span class="math display"><em>p</em></span>和点 <span class="math display"><em>q</em></span>是密度相连的</strong>。</li></ul><blockquote><p>这个关系是<strong>对称的</strong>，用于判断是否属于同一个簇。</p></blockquote><h4 id="示例">示例</h4><figure><img src="p.png" alt="密度相关概念的例子" /><figcaption aria-hidden="true">密度相关概念的例子</figcaption></figure><h3 id="dbscan算法">DBSCAN算法</h3><h4 id="核心参数">核心参数</h4><ul><li><strong><spanclass="math display"><em>ε</em></span>（eps）</strong>：定义点的“邻域”范围<br /></li><li><strong>MinPts</strong>：最小邻居点数，决定一个点是否为“核心点”</li></ul><h4 id="算法流程-4">算法流程</h4><ol type="1"><li><p><strong>初始化状态</strong><br />所有数据点标记为“未访问”。</p></li><li><p><strong>从一个未访问点 <spanclass="math inline"><em>p</em></span> 开始：</strong></p><ul><li>计算 <span class="math inline"><em>p</em></span> 的 <spanclass="math inline"><em>ε</em></span>-邻域（记为 <spanclass="math inline"><em>N</em><sub><em>ε</em></sub>(<em>p</em>)</span>）；</li><li>如果邻域内的点数 &lt; MinPts：将 <spanclass="math inline"><em>p</em></span> 标记为噪声，跳过；</li><li>否则，将 <span class="math inline"><em>p</em></span>作为<strong>核心点</strong>，创建一个新簇，将 <spanclass="math inline"><em>N</em><sub><em>ε</em></sub>(<em>p</em>)</span>中的所有点加入该簇。</li></ul></li><li><p><strong>扩展当前簇：</strong><br />对刚加入簇的每个点 <spanclass="math inline"><em>q</em></span>（若未访问）执行：</p><ul><li>标记 <span class="math inline"><em>q</em></span> 为已访问；</li><li>若 <span class="math inline"><em>q</em></span>是核心点（其邻域内点数 ≥ MinPts）：<ul><li>将 <spanclass="math inline"><em>N</em><sub><em>ε</em></sub>(<em>q</em>)</span>中的所有点加入当前簇（即密度可达）；</li></ul></li><li>否则（<span class="math inline"><em>q</em></span>是边界点），保留在当前簇中，不继续扩展。</li></ul></li><li><p><strong>重复步骤 2~3</strong></p><ul><li>直到所有点都被访问。</li></ul></li></ol><h4 id="dbscan-的输出">DBSCAN 的输出</h4><ul><li>若干个簇（每个由核心点及其密度可达的点组成）<br /></li><li>一些噪声点（不属于任何簇）</li></ul><h4 id="优点">优点</h4><ul><li>不需要预先指定簇数<br /></li><li>可发现任意形状的簇<br /></li><li>可识别噪声点<br /></li><li>对离群点不敏感</li></ul><h4 id="缺点">缺点</h4><ul><li>需要合适的 <span class="math display"><em>ε</em></span> 和MinPts<br /></li><li>当数据密度差异较大时效果较差<br /></li><li>高维数据中，距离不再可靠（维数灾难）</li></ul><h3 id="optics算法">OPTICS算法</h3><p>OPTICS（Ordering Points To Identify the ClusteringStructure）是一种基于密度的聚类算法，解决了 DBSCAN对参数敏感的问题。它不仅能发现不同密度的簇，还能输出数据的聚类结构排序，方便后续分析。</p><h4 id="相关概念">相关概念</h4><ul><li><p><strong>核心距离（Core Distance）</strong><br />对点 <span class="math display"><em>p</em></span>，如果它的 <spanclass="math display"><em>ε</em></span>-邻域内至少有 MinPts个点，则核心距离是到第 MinPts 个最近邻的距离；否则未定义。</p></li><li><p><strong>可达距离（Reachability Distance）</strong><br />对点 <span class="math display"><em>p</em></span> 和它的邻居 <spanclass="math display"><em>o</em></span>，可达距离定义为：</p><p><spanclass="math display"><em>r</em><em>e</em><em>a</em><em>c</em><em>h</em><em>a</em><em>b</em><em>i</em><em>l</em><em>i</em><em>t</em><em>y</em>_<em>d</em><em>i</em><em>s</em><em>t</em>(<em>o</em>,<em>p</em>) = max (<em>c</em><em>o</em><em>r</em><em>e</em>_<em>d</em><em>i</em><em>s</em><em>t</em>(<em>p</em>),<em>d</em><em>i</em><em>s</em><em>t</em>(<em>p</em>,<em>o</em>))</span></p><p>如果 <span class="math display"><em>p</em></span> 是核心点，且 <spanclass="math display"><em>o</em></span> 在 <spanclass="math display"><em>p</em></span> 的邻域内，否则未定义。</p></li></ul><h4 id="算法流程-5">算法流程</h4><ol type="1"><li><strong>初始化</strong><ul><li>给数据集中每个点设置状态为“未访问”。<br /></li><li>给每个点设置可达距离 <spanclass="math display"><em>r</em><em>e</em><em>a</em><em>c</em><em>h</em><em>a</em><em>b</em><em>i</em><em>l</em><em>i</em><em>t</em><em>y</em>_<em>d</em><em>i</em><em>s</em><em>t</em> = ∞</span>（表示未知）。<br /></li><li>准备一个空的“输出顺序列表”（ordering），用于记录访问点的顺序。</li></ul></li><li><strong>遍历所有点</strong><br />对数据集中的每个点 <span class="math display"><em>p</em></span>：<ul><li>如果 <span class="math display"><em>p</em></span>已经被访问过，跳过；<br /></li><li>否则，开始处理 <span class="math display"><em>p</em></span>：</li></ul></li><li><strong>访问点 <span class="math display"><em>p</em></span></strong><ul><li>将 <span class="math display"><em>p</em></span>标记为“已访问”。<br /></li><li>计算 <span class="math display"><em>p</em></span> 的核心距离 <spanclass="math display"><em>c</em><em>o</em><em>r</em><em>e</em>_<em>d</em><em>i</em><em>s</em><em>t</em>(<em>p</em>)</span>：<ul><li>找出 <span class="math display"><em>p</em></span> 的 <spanclass="math display"><em>ε</em></span>-邻域中的所有点，排序后取第 MinPts个最近邻点的距离作为核心距离。<br /></li><li>如果邻域内点数不足 MinPts，则 <spanclass="math display"><em>p</em></span>不是核心点，核心距离未定义。<br /></li></ul></li><li>把 <span class="math display"><em>p</em></span>加入输出顺序列表（ordering），此时 <spanclass="math display"><em>p</em></span>的可达距离保持为当前值（初始可能为无穷大）。</li></ul></li><li><strong>如果 <span class="math display"><em>p</em></span>是核心点</strong><ul><li>取 <span class="math display"><em>p</em></span> 的 <spanclass="math display"><em>ε</em></span>-邻域内所有<strong>未访问的点</strong>，放入一个优先队列（或类似结构），根据可达距离排序（距离越小优先）。<br /></li><li>对这些邻居点 <span class="math display"><em>o</em></span>：<ul><li>计算从 <span class="math display"><em>p</em></span> 到 <spanclass="math display"><em>o</em></span> 的可达距离：<br /><spanclass="math display"><em>r</em><em>e</em><em>a</em><em>c</em><em>h</em><em>a</em><em>b</em><em>i</em><em>l</em><em>i</em><em>t</em><em>y</em>_<em>d</em><em>i</em><em>s</em><em>t</em>(<em>o</em>) = max (<em>c</em><em>o</em><em>r</em><em>e</em>_<em>d</em><em>i</em><em>s</em><em>t</em>(<em>p</em>),<em>d</em><em>i</em><em>s</em><em>t</em>(<em>p</em>,<em>o</em>))</span><br /></li><li>如果 <span class="math display"><em>o</em></span>当前的可达距离大于这个新值，则更新 <spanclass="math display"><em>o</em></span> 的可达距离为新值，并将 <spanclass="math display"><em>o</em></span> 加入优先队列。<br /></li></ul></li><li>依次从优先队列中取出可达距离最小的点 <spanclass="math display"><em>q</em></span>，重复步骤 3 和4，直到优先队列空。</li></ul></li><li><strong>继续处理剩余点</strong><ul><li>返回步骤 2，处理下一个未访问的点，直到所有点都被访问。</li></ul></li></ol><h4 id="结果输出">结果输出</h4><ul><li>一条有序的点列表，反映数据的密度聚类结构。<br /></li><li>通过绘制 <strong>可达距离图（ReachabilityPlot）</strong>，可以直观观察不同密度簇的分布和边界。</li></ul><figure><img src="可达距离.jpg" alt="可达距离图示意" /><figcaption aria-hidden="true">可达距离图示意</figcaption></figure><h4 id="优点-1">优点</h4><ul><li>不需要像 DBSCAN 那样对 <span class="math display"><em>ε</em></span>选取非常敏感。<br /></li><li>能发现不同密度的簇。<br /></li><li>输出的可达距离序列方便后续聚类分析。</li></ul><h2 id="基于网格的聚类方法">基于网格的聚类方法</h2><h3 id="sting算法">STING算法</h3><p>STING是一种基于网格（Grid-based）的聚类方法，通过将空间划分为不同层次的网格结构，并基于每个网格单元中的统计信息来进行聚类判断。</p><h4 id="核心思想-3">核心思想</h4><ul><li>将数据空间预先划分成多个<strong>固定大小的单元格（网格）</strong>。</li><li>构建一个多层次的网格索引结构（层次树），每一层网格的粒度逐渐增大（上层粒度粗，下层粒度细）。</li><li>每个网格单元保存统计信息，如：<ul><li>对象数（count）</li><li>平均值（mean）</li><li>方差（variance）</li><li>最小值、最大值等</li></ul></li></ul><h4 id="算法流程-6">算法流程</h4><ol type="1"><li><strong>构建网格结构</strong><ul><li>划分空间：将数据空间划分为若干固定大小的单元格。</li><li>构建多层结构，每层是上一层的粗粒度抽象。</li></ul></li><li><strong>统计每个单元格的属性</strong><ul><li>在数据预处理阶段，统计每个单元格内的数据统计量，并自底向上传递到上层节点（汇总统计）。</li></ul></li><li><strong>选择合适的层次作为分析起点</strong><ul><li>通常选择中间层开始，粒度适中，既不太粗也不太细。</li></ul></li><li><strong>过滤和合并网格单元</strong><ul><li>对目标查询区域内的单元格：<ul><li>如果统计指标满足某些条件（如密度高），则进一步下钻到下一层；</li><li>否则停止，或直接排除该区域；</li></ul></li><li>递归进行，直到达到最低层或不能再分。</li></ul></li><li><strong>输出聚类区域</strong><ul><li>最终，在最底层（最细的网格）里，把满足“高密度”的小单元格合并在一起，作为一个簇。</li><li>类似地，另一个位置的网格也可能形成另一个簇。</li></ul></li></ol><h4 id="通俗例子">通俗例子</h4><p>假设我们有一个二维空间，数据分布如下：</p><ul><li>在左上角有一堆密集点<br /></li><li>在右下角又有一堆密集点<br /></li><li>其它区域是稀疏的</li></ul><p><strong>STING 做法：</strong></p><ul><li>初步划分整个区域为 4×4 网格，每个网格记录点的数量<br /></li><li>找出有密集数据的几个网格,对这些网格再细分（下钻）</li></ul><p><strong>最终：</strong></p><ul><li>左上角细分后合并成“簇 A”<br /></li><li>右下角细分后合并成“簇 B”<br /></li><li>其它区域被排除，不属于任何簇</li></ul><h4 id="优点-2">优点</h4><ul><li><strong>高效</strong>：由于只操作统计摘要，而非原始数据，效率高。</li><li><strong>可扩展性好</strong>：支持大规模数据集。</li><li><strong>支持多层次分析</strong>：可以灵活选择不同粒度的层级进行聚类。</li></ul><h4 id="缺点-1">缺点</h4><ul><li>聚类结果依赖于网格划分的方式，不够灵活。</li><li>网格边界可能造成聚类割裂（称为边界效应）。</li></ul><h3 id="clique-算法clustering-in-quest">CLIQUE 算法（Clustering InQUEst）</h3><p>CLIQUE 是一种 <strong>基于网格 +子空间探索的聚类算法</strong>，专门用于发现<strong>高维数据中的密度簇</strong>。</p><h4 id="核心思想-4">核心思想</h4><ul><li>将每个维度划分为固定数量的等宽区间（网格单元格）。</li><li>构建所有可能的低维子空间，并在这些子空间中寻找密度足够高的网格单元。</li><li>利用 <strong>Apriori原则</strong>：高维的密度区域必定由低维的密度区域组合而成。</li><li>通过递归方式找到所有“密度簇”所在的维度子空间，并形成聚类。</li></ul><h4 id="算法流程-7">算法流程</h4><ol type="1"><li><strong>空间划分</strong><ul><li>对每个维度划分为 <span class="math display"><em>ξ</em></span>个等宽区间（即网格单元）。</li><li>在 <span class="math display"><em>d</em></span> 维空间中，初始共形成<span class="math display"><em>ξ</em><sup><em>d</em></sup></span>个单位网格。</li></ul></li><li><strong>密度单元检测（初始在 1 维）</strong><ul><li>统计每个 1 维网格单元中的点数，若超过阈值 <spanclass="math display"><em>τ</em></span>，称为“密度单元”。</li></ul></li><li><strong>子空间扩展（类似 Apriori）</strong><ul><li>逐步组合多个维度（如 2 维、3 维…）形成更高维的子空间。</li><li>只对在低维中都是“密度单元”的组合进行尝试，减少搜索空间。</li></ul></li><li><strong>聚类形成</strong><ul><li>将在所有子空间中找到的密度单元格相邻的部分连接起来，构成聚类。</li><li>可以用连通分量的方式将多个密度单元“连通”成一个簇。</li></ul></li><li><strong>输出聚类结果</strong><ul><li>返回所有簇的点集和对应的子空间（说明在哪些维度中形成簇）。</li><li>CLIQUE 支持自动检测出<strong>不同维度的簇结构</strong>，适合高维数据分析。</li></ul></li></ol><h4 id="优点-3">优点</h4><ul><li>能发现<strong>任意维度子空间中的簇</strong>，适合处理高维稀疏数据；</li><li>网格结构使得它对噪声具有鲁棒性；</li><li>自动确定簇的数量和维度。</li></ul><h4 id="缺点-2">缺点</h4><ul><li>网格分辨率（ξ）选得不合适会影响聚类效果；</li><li>枚举所有子空间仍可能面临维度灾难，适合中等维度使用。</li></ul><h2 id="基于模型的聚类方法">基于模型的聚类方法</h2><h3 id="em-算法expectation-maximization">EM算法（Expectation-Maximization）</h3><p>EM是一种<strong>迭代优化算法</strong>，用于含有“隐藏变量”的概率模型，常用于估计最大似然参数。聚类中常用于<strong>高斯混合模型（GMM）</strong>。</p><h4 id="核心思想-5">核心思想</h4><ul><li>数据可能来自多个概率分布（比如多个高斯分布），但我们不知道每个样本来自哪个分布。</li><li>EM 用“猜测 → 估计 → 更新”的方式迭代：<ul><li><strong>E步</strong>（期望步）：根据当前参数，估计每个样本来自各个分布的概率（软聚类）。</li><li><strong>M步</strong>（最大化步）：根据上一步的估计，重新计算模型参数，使其更符合当前数据。</li></ul></li></ul><h4 id="应用于聚类的流程以-gmm-为例">应用于聚类的流程（以 GMM为例）</h4><p>设数据集为 <spanclass="math display"><em>X</em> = {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>n</em></sub>}</span>，假设它由<span class="math display"><em>k</em></span> 个高斯分布混合而成。</p><ol type="1"><li><strong>初始化参数</strong><ul><li>初始化每个分布的参数：均值 <spanclass="math display"><em>μ</em><sub><em>k</em></sub></span>、协方差<span class="math display"><em>Σ</em><sub><em>k</em></sub></span>和混合系数 <spanclass="math display"><em>π</em><sub><em>k</em></sub></span>（即每个簇的概率）。</li></ul></li><li><strong>E 步（Expectation）</strong><ul><li>计算每个样本 <spanclass="math display"><em>x</em><sub><em>i</em></sub></span> 属于每个簇<span class="math display"><em>k</em></span>的<strong>后验概率</strong>（即“软分类”）： <spanclass="math display">$$\gamma_{ik} = P(z_i = k \mid x_i) = \frac{\pi_k \cdot \mathcal{N}(x_i\mid \mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_j \cdot \mathcal{N}(x_i \mid\mu_j, \Sigma_j)}$$</span></li></ul></li><li><strong>M 步（Maximization）</strong><ul><li>用 E 步得到的“软分类”更新参数：<ul><li>更新均值： <span class="math display">$$\mu_k = \frac{\sum_{i=1}^n \gamma_{ik} x_i}{\sum_{i=1}^n \gamma_{ik}}$$</span></li><li>更新协方差： <span class="math display">$$\Sigma_k = \frac{\sum_{i=1}^n \gamma_{ik} (x_i - \mu_k)(x_i -\mu_k)^T}{\sum_{i=1}^n \gamma_{ik}}$$</span></li><li>更新混合系数： <span class="math display">$$\pi_k = \frac{1}{n} \sum_{i=1}^n \gamma_{ik}$$</span></li></ul></li></ul></li><li><strong>判断是否收敛</strong><ul><li>如果对数似然函数变化很小，就停止迭代；否则回到 E 步。</li></ul></li></ol><h4 id="简单例子二维聚类">简单例子（二维聚类）</h4><p>假设你有一组二维数据（比如年龄 vs收入），你猜它由两个高斯分布组成：</p><ul><li>初始：<ul><li>猜两个中心点和协方差</li></ul></li><li>E 步：<ul><li>对每个点，算它属于两个簇的概率（可能是 60% 属于 A，40% 属于 B）</li></ul></li><li>M 步：<ul><li>用这些概率更新两个高斯分布的参数（比如中心位置向有更多点的方向移动）</li></ul></li><li>重复，直到稳定</li></ul><h4 id="特点-3">特点</h4><table><thead><tr class="header"><th>优点</th><th>缺点</th></tr></thead><tbody><tr class="odd"><td>能做“软聚类”</td><td>对初始值敏感，易陷入局部最优</td></tr><tr class="even"><td>能适用于复杂概率模型</td><td>假设数据满足特定分布（如高斯）</td></tr><tr class="odd"><td>理论成熟，适用于密度估计、图像分割等</td><td>不适合非凸或非高斯的聚类结构</td></tr></tbody></table><h2 id="聚类质量评估方法">聚类质量评估方法</h2>]]></content>
    
    
    <categories>
      
      <category>计算机</category>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机</tag>
      
      <tag>数据挖掘</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>线性代数：矩阵与线性方程组</title>
    <link href="/BLOG/2025/05/25/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%EF%BC%9A%E7%9F%A9%E9%98%B5%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/"/>
    <url>/BLOG/2025/05/25/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%EF%BC%9A%E7%9F%A9%E9%98%B5%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<blockquote><p>从这个一个博客开始，我们将进行线性代数的复习。由于是用于夏令营以及与推免考核所用，我们一些简单的定义和定理将直接带过</p></blockquote><h1 id="线性代数矩阵与线性方程组">线性代数：矩阵与线性方程组</h1><p>我们从矩阵开始复习，同时带到线性相关、子空间等概念。</p><p>我们先介绍最简单的概念，线性相关与线性无关</p><h2 id="线性相关与线性无关">线性相关与线性无关</h2><p>我们定义列向量 <spanclass="math inline">{<em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, <em>a</em><sub>3</sub>, …，<em>a</em><sub><em>n</em></sub>}</span></p><ul><li><p><strong>线性相关(Linear Dependent)</strong>：若是存在不全为<strong>0</strong> 的向量 <spanclass="math inline"><em>X</em> = {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, <em>x</em><sub>3</sub>, …, <em>x</em><sub><em>n</em></sub>}</span>满足 <spanclass="math inline"><em>x</em><sub>1</sub><em>a</em><sub>1</sub> + <em>x</em><sub>2</sub><em>a</em><sub>2</sub> + <em>x</em><sub>3</sub><em>a</em><sub>3</sub> + … + <em>x</em><sub><em>n</em></sub><em>a</em><sub><em>n</em></sub> = 0</span>，则称列向量<spanclass="math inline">{<em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, <em>a</em><sub>3</sub>, …，<em>a</em><sub><em>n</em></sub>}</span>线性相关</p></li><li><p><strong>线性无关(Linear Independent)</strong>：与上述相反，如果不存在不全为 <strong>0</strong> 的向量 <spanclass="math inline"><em>X</em> = {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, <em>x</em><sub>3</sub>, …, <em>x</em><sub><em>n</em></sub>}</span>满足条件，则称之为线性无关</p></li></ul><p>随后我们简单回顾矩阵，矩阵的定义从向量导入，如果矩阵有m行和n列，我们就说矩阵的大小为<spanclass="math inline"><em>m</em> * <em>n</em></span>，如果<spanclass="math inline"><em>m</em> = <em>n</em></span>，我们称为方阵（<strong>squarematrix</strong>）</p><h2 id="矩阵的计算">矩阵的计算</h2><p>这里我们简单介绍矩阵的而一些计算与属性</p><ul><li><p>矩阵和标量相乘，即每个元素和标量相乘</p></li><li><p>矩阵（向量）和矩阵（向量）相乘， 即按照顺序依次相乘</p><ul><li>若<spanclass="math inline"><em>A</em><em>B</em> = <em>B</em><em>A</em></span>，则称A与B乘法可交换。n阶单位矩阵E与任何n阶矩阵乘法可交换</li></ul></li><li><p>矩阵的分块乘法</p></li><li><p>矩阵<strong>转置</strong></p></li><li><p>矩阵<strong>共轭</strong>:即矩阵内每个元素取其共轭的结果</p></li><li><p>矩阵的<strong>秩（Rank)</strong>:矩阵列向量中最大的线性无关的数目（The maximum number of LinearIndependent columns)</p></li><li><p>矩阵的<strong>零化度（Nullity）：</strong>矩阵的列数减去矩阵的秩</p></li></ul><h3 id="矩阵的逆">矩阵的逆</h3><p>如果两个方阵A和B的乘积是单位矩阵，AB=I，那么A和B就是互为逆矩阵</p><p>矩阵的逆有如下定义：</p><ul><li><p><spanclass="math inline">(<em>A</em><sup>−1</sup>)<sup>−1</sup> = <em>A</em></span></p></li><li><p><spanclass="math inline">(<em>k</em><em>A</em>)<sup>−1</sup> = <em>k</em><sup>−1</sup><em>A</em><sup>−1</sup></span></p></li><li><p><spanclass="math inline">(<em>A</em><sup><em>T</em></sup>)<sup>−1</sup> = (<em>A</em><sup>−1</sup>)<sup><em>T</em></sup></span></p></li><li><p><spanclass="math inline">(<em>A</em><em>B</em>)<sup>−1</sup> = <em>B</em><sup>−1</sup><em>A</em><sup>−1</sup></span></p></li></ul><p>矩阵的逆还有如下的性质（需要牢记）：</p><figure><img src="Rank.png" alt="矩阵逆的性质" /><figcaption aria-hidden="true">矩阵逆的性质</figcaption></figure><p>对于一个方阵，其可逆判别可用如下任意一条：</p><figure><img src="invert.jpg" alt="矩阵可逆的判别" /><figcaption aria-hidden="true">矩阵可逆的判别</figcaption></figure><h3 id="矩阵的行列式">矩阵的行列式</h3><p>具体计算我们带过，我们讨论一下行列式的性质</p><ul><li><p>交换任意两行，行列式取负号</p></li><li><p>行列式的标量乘法： <spanclass="math inline"><em>d</em><em>e</em><em>t</em>(2<em>A</em>) = 2<sup><em>n</em></sup><em>d</em><em>e</em><em>t</em>(<em>A</em>)</span></p></li><li><p>矩阵乘法的行列式：</p></li></ul><figure><img src="det.jpg" alt="矩阵乘法的行列式" /><figcaption aria-hidden="true">矩阵乘法的行列式</figcaption></figure><ul><li><p>如果一个方阵的行列式不为0，那么它是可逆的，反之，如果一个方阵可逆，那么它的行列式不为0</p></li><li><p><span class="math inline">$\bulletA^{-1}=\frac{1}{det(A)}C^T$</span> 其中<spanclass="math inline"><em>C</em></span>为伴随矩阵，具体如下</p></li></ul><p><span class="math display">$$C=\begin{bmatrix}c_{11} &amp; \cdots &amp; c_{1n} \\\vdots &amp; \ddots &amp; \vdots \\c_{n1} &amp; \cdots &amp; c_{nn}\end{bmatrix}$$</span></p><p>其中 <spanclass="math inline"><em>c</em><sub><em>i</em><em>j</em></sub> = (−1)<sup><em>i</em> + <em>j</em></sup><em>d</em><em>e</em><em>t</em>(<em>A</em><sub><em>i</em><em>j</em></sub>)</span>为代数余子式</p><h3 id="初等矩阵">初等矩阵</h3><p><strong>初等矩阵</strong>： 对单位矩阵进行一次变换后的矩阵</p><p>相关定理（<strong>左行右列</strong>）：</p><ul><li><p>对A施行初等<strong>行变换</strong>，其结果等于在A左边乘以相应的初等矩阵；</p></li><li><p>对A施行初等<strong>列变换</strong>，其结果等于在A右边乘以相应的初等矩阵</p></li><li><p>n阶方阵可逆的<strong>充要条件</strong>是它能表示成一些初等矩阵的乘积</p></li></ul><h2 id="线性方程组">线性方程组</h2><p>我们借助矩阵带一下线性方程组的相关概念与性质</p><p><strong>我们可以用矩阵表达一个线性方程组，即： <spanclass="math inline"><em>A</em><em>x</em> = <em>b</em></span>,其中A为矩阵，<spanclass="math inline"><em>x</em>, <em>b</em></span>分别为对应维度的列向量</strong></p><h3 id="解的情况与相容性">解的情况与相容性</h3><p>线性方程组解 <span class="math inline"><em>x</em></span>的情况有三种：<strong>无解、有唯一解、有无穷解</strong></p><p>若方程组有解，则称其为 <strong>相容（Consistent）</strong>；若是无解，则称之为 <strong>不相容（Inconsistent）</strong></p><h3 id="线性方程组有解的解释">线性方程组有解的解释</h3><p>首先介绍一个概念</p><ul><li><strong>张成的空间</strong>：对于一个向量集 <spanclass="math inline"><em>S</em></span> , 其向量的所有线性组合组成的向量集<span class="math inline"><em>V</em></span> ，称之为 <spanclass="math inline"><em>S</em></span> 张成的空间，记作 <spanclass="math inline"><em>S</em><em>p</em><em>a</em><em>n</em>(<em>S</em>)</span></li></ul><p>因此若 <spanclass="math inline"><em>A</em><em>x</em> = <em>b</em></span>有解，可以解释为：</p><ul><li><p><span class="math inline"><em>b</em></span> 是可以表示为矩阵<span class="math inline"><em>A</em></span> 的列向量的线性组合</p></li><li><p><span class="math inline"><em>b</em></span> 再矩阵 <spanclass="math inline"><em>A</em></span> 的列向量的所张成的空间里</p></li></ul><h3 id="解的数量与矩阵a的关系">解的数量与矩阵A的关系</h3><ul><li><p>有唯一解：矩阵A的列向量线性无关，即 <spanclass="math inline"><em>R</em><em>a</em><em>n</em><em>k</em>(<em>A</em>) = <em>n</em>(<em>N</em><em>u</em><em>l</em><em>l</em><em>i</em><em>t</em><em>y</em>(<em>A</em>)=0)</span></p></li><li><p>有无穷解：矩阵A的列向量线性相关，即 <spanclass="math inline"><em>R</em><em>a</em><em>n</em><em>k</em>(<em>A</em>) &lt; <em>n</em>(<em>N</em><em>u</em><em>l</em><em>l</em><em>i</em><em>t</em><em>y</em>(<em>A</em>)&gt;0)</span></p></li></ul><h3id="线性方程组求解高斯消元法gaussian-elimination">线性方程组求解：高斯消元法（GaussianElimination）</h3><p>首先有线性方程组等价定理：两个线性方程组等价 <spanclass="math inline">⇔</span> 两个线性方程组有相同的解</p><p><strong>高斯消元法</strong>：将增广矩阵化简为简约型阶梯形式，进而求解的方法</p><ul><li><p><strong>增广矩阵</strong>：将矩阵A与列向量b横向拼接起来，即 <spanclass="math inline">[<em>A</em>|<em>b</em>]</span></p></li><li><p><strong>简约型阶梯形式</strong>：即呈阶梯状，并且每个阶梯第一位为1，具体如下图：</p></li></ul><figure><img src="Echelon.jpg" alt="简约型阶梯形式示例" /><figcaption aria-hidden="true">简约型阶梯形式示例</figcaption></figure><p>若是最后化简得结果存在只有最后一列不为0得情况，则线性方程组无解;反之则有解，若是有一行全部为0，则存在无数解。</p><figure><img src="result.jpg" alt="解的分类" /><figcaption aria-hidden="true">解的分类</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>数学</category>
      
      <category>线性代数</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
      <tag>线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>博客写入教程</title>
    <link href="/BLOG/2025/05/24/%E5%8D%9A%E5%AE%A2%E5%86%99%E5%85%A5%E6%95%99%E7%A8%8B/"/>
    <url>/BLOG/2025/05/24/%E5%8D%9A%E5%AE%A2%E5%86%99%E5%85%A5%E6%95%99%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="markdown-写作模板含-front-matter-与语法指南">📘 Markdown写作模板（含 Front Matter 与语法指南）</h1><hr /><h2 id="第一部分-front-matter-配置说明">第一部分：📄 Front Matter配置说明</h2><blockquote><p>Front Matter 是 Markdown文件顶部的元信息，用于配置文章标题、标签、分类、日期等内容。通常写在<code>---</code> 包裹的 YAML 格式中。</p></blockquote><h3 id="示例模板">🧾 示例模板</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">post</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">我的第一篇博客</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2025-05-24 10:00:00</span><br><span class="hljs-attr">updated:</span> <span class="hljs-number">2025-05-24 12:00:00</span><br><span class="hljs-attr">comments:</span> <span class="hljs-literal">true</span><br><span class="hljs-attr">tags:</span> [<span class="hljs-string">Markdown</span>, <span class="hljs-string">教程</span>]<br><span class="hljs-attr">categories:</span> [<span class="hljs-string">技术笔记</span>]<br><span class="hljs-attr">permalink:</span> <span class="hljs-string">/markdown-guide.html</span><br><span class="hljs-attr">excerpt:</span> <span class="hljs-string">本文介绍了</span> <span class="hljs-string">Markdown</span> <span class="hljs-string">的语法与博客头部配置。</span><br><span class="hljs-attr">disableNunjucks:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">lang:</span> <span class="hljs-string">zh-CN</span><br><span class="hljs-attr">published:</span> <span class="hljs-literal">true</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><h3 id="字段说明表">📋 字段说明表</h3><table><thead><tr class="header"><th>字段名</th><th>说明</th><th>默认值</th></tr></thead><tbody><tr class="odd"><td><code>layout</code></td><td>页面布局模板（如 <code>post</code>、<code>page</code>）</td><td><code>config.default_layout</code></td></tr><tr class="even"><td><code>title</code></td><td>页面标题</td><td>文件名</td></tr><tr class="odd"><td><code>date</code></td><td>建立日期</td><td>文件创建时间</td></tr><tr class="even"><td><code>updated</code></td><td>最近更新日期</td><td>文件更新时间</td></tr><tr class="odd"><td><code>comments</code></td><td>是否启用评论</td><td><code>true</code></td></tr><tr class="even"><td><code>tags</code></td><td>标签数组，用于分类内容</td><td>空</td></tr><tr class="odd"><td><code>categories</code></td><td>分类数组（适用于归档）</td><td>空</td></tr><tr class="even"><td><code>permalink</code></td><td>自定义永久链接（应以 <code>/</code> 或 <code>.html</code>结尾）</td><td><code>null</code></td></tr><tr class="odd"><td><code>excerpt</code></td><td>页面摘要，建议结合插件处理</td><td>空</td></tr><tr class="even"><td><code>disableNunjucks</code></td><td>禁用 Nunjucks模板语法（<code>&#123;&#123; &#125;&#125;</code>、<code>&#123;% %&#125;</code>）</td><td><code>false</code></td></tr><tr class="odd"><td><code>lang</code></td><td>页面语言，覆盖默认配置</td><td>继承自 <code>_config.yml</code></td></tr><tr class="even"><td><code>published</code></td><td>是否发布该文章</td><td><code>_posts</code> 下为 <code>true</code>，草稿为<code>false</code></td></tr></tbody></table><hr /><h2 id="第二部分-markdown-语法速查手册">第二部分：📝 Markdown语法速查手册</h2><h3 id="标题">1️⃣ 标题</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-section"># 一级标题</span><br><span class="hljs-section">## 二级标题</span><br><span class="hljs-section">### 三级标题</span><br></code></pre></td></tr></table></figure><hr /><h3 id="段落与换行">2️⃣ 段落与换行</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs markdown">这是第一段。<br><br>这是第二段，  <br>这里使用两个空格加换行。<br></code></pre></td></tr></table></figure><hr /><h3 id="文本格式">3️⃣ 文本格式</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-strong">**加粗文本**</span><br><span class="hljs-emphasis">*斜体文本*</span><br>~~删除线文本~~<br><span class="hljs-code">`行内代码`</span><br></code></pre></td></tr></table></figure><hr /><h3 id="引用块">4️⃣ 引用块</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-quote">&gt; 一级引用</span><br>&gt;&gt; 二级引用<br></code></pre></td></tr></table></figure><hr /><h3 id="列表">5️⃣ 列表</h3><ul><li><strong>无序列表</strong></li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">-</span> 项目一<br><span class="hljs-bullet">-</span> 项目二<br></code></pre></td></tr></table></figure><ul><li><strong>有序列表</strong></li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">1.</span> 步骤一<br><span class="hljs-bullet">2.</span> 步骤二<br></code></pre></td></tr></table></figure><hr /><h3 id="代码块">6️⃣ 代码块</h3><ul><li><strong>行内代码</strong></li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">这是 <span class="hljs-code">`inline code`</span><br></code></pre></td></tr></table></figure><ul><li><strong>多行代码块</strong></li></ul><details><summary>Python 示例</summary><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">greet</span>():<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Hello Markdown!&quot;</span>)<br></code></pre></td></tr></table></figure></details><hr /><h3 id="表格">7️⃣ 表格</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs markdown">| 名称    | 类型   | 描述       |<br>|---------|--------|------------|<br>| title   | 字符串 | 文章标题   |<br>| date    | 日期   | 创建时间   |<br></code></pre></td></tr></table></figure><hr /><h3 id="链接与图片">8️⃣ 链接与图片</h3><ul><li><strong>链接</strong></li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">[<span class="hljs-string">OpenAI 官网</span>](<span class="hljs-link">https://www.openai.com</span>)<br></code></pre></td></tr></table></figure><ul><li><strong>图片</strong></li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">![<span class="hljs-string">替代文字</span>](<span class="hljs-link">https://via.placeholder.com/150</span>)<br></code></pre></td></tr></table></figure><hr /><h3 id="分隔线">9️⃣ 分隔线</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">---<br></code></pre></td></tr></table></figure><hr /><h3 id="任务列表github-支持">🔟 任务列表（GitHub 支持）</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">-</span> [x] 学习 Markdown<br><span class="hljs-bullet">-</span> [ ] 写一篇博客<br></code></pre></td></tr></table></figure><hr /><h3 id="转义字符防止符号被识别">🧩 转义字符（防止符号被识别）</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown">\<span class="hljs-emphasis">*星号\*</span> 不会加粗  <br>\<span class="hljs-code">`反引号\`</span> 不会变成代码  <br></code></pre></td></tr></table></figure><h3 id="公式说明">公式说明</h3><blockquote><p>$$ E = mc^2 $$ 可以实现公式的书写</p><p><spanclass="math display"><em>E</em> = <em>m</em><em>c</em><sup>2</sup></span></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>技术笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Markdown</tag>
      
      <tag>教程</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
