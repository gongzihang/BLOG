<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Softmax的进化之旅</title>
    <link href="/BLOG/2025/06/19/Softmax%E7%9A%84%E8%BF%9B%E5%8C%96%E4%B9%8B%E6%97%85/"/>
    <url>/BLOG/2025/06/19/Softmax%E7%9A%84%E8%BF%9B%E5%8C%96%E4%B9%8B%E6%97%85/</url>
    
    <content type="html"><![CDATA[<blockquote><p>由于主包最近准备了一场 Coding面试，初步领略了下FlashAttention的神奇，其中的关键操作便是对于Softmax的优化。面试结束之后主包也是决定记录一下这个操作，于是有了这篇博客。</p></blockquote><blockquote><p>由于是准备的Coding面，这里也会附上代码的😁 (所以也可以叫<em>手撕online softmax</em>?)</p></blockquote><h1 id="为什么需要online-softmax">为什么需要Online Softmax</h1><p>这个首先要从FlashAttention说起，这里简单介绍，原始Transformer中，会存在一个Score矩阵，其维度为<spanclass="math display"><em>N</em> ⋅ <em>N</em></span>，非常巨大。这导致其在GPU上没法分块计算。由此我们想要分块计算Q,K,V矩阵，FlashAttention便完成了这个任务，有效减小了GPU的IO操作数量，从而实现了加速。</p><p>想法很简单，但是实现起来存在一些问题,其中的关键便是Softmax，我们从头开始说起</p><h1 id="softmax的进化">Softmax的“进化”</h1><h2 id="标准的softmax">标准的Softmax</h2><p>我们都知道Softmax的公式如下：</p><p><span class="math display">$$Softmax(x_i) = \frac{e^{x_i}}{\sum_{j}^{N} e^{x_j}}$$</span></p><p>这个的手撕代码很简单，我们对矩阵做行Softmax：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br>x = torch.randn(<span class="hljs-number">2</span>,<span class="hljs-number">5</span>)<br>row_sum = torch.exp(x).<span class="hljs-built_in">sum</span>(dim=-<span class="hljs-number">1</span>,keepdim=<span class="hljs-literal">True</span>)<br>ours_out = torch.exp(x) / row_sum<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;======= ours softmax =======&quot;</span>)<br><span class="hljs-built_in">print</span>(ours_out)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;======= Standard softmax =======&quot;</span>)<br><span class="hljs-built_in">print</span>(F.softmax(x, dim=-<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><p>输出如下： <figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs lua">======= ours softmax =======<br>tensor(<span class="hljs-string">[[0.4979, 0.0880, 0.0198, 0.0267, 0.3675],</span><br><span class="hljs-string">        [0.1271, 0.1828, 0.3216, 0.1422, 0.2263]]</span>)<br>======= Standard softmax =======<br>tensor(<span class="hljs-string">[[0.4979, 0.0880, 0.0198, 0.0267, 0.3675],</span><br><span class="hljs-string">        [0.1271, 0.1828, 0.3216, 0.1422, 0.2263]]</span>)<br></code></pre></td></tr></table></figure></p><hr /><p>看上去非常不错，但是当我们希望使用fp16精度或者输入的数据大一些，由于指数的存在，会很容易的溢出，比如我们仅仅将x的输入扩大100倍（<code>x = torch.randn(2,5)*100</code>）,输出就会变为</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs lua">======= ours softmax =======<br>tensor(<span class="hljs-string">[[nan, 0., 0., 0., 0.],</span><br><span class="hljs-string">        [0., 0., nan, 0., 0.]]</span>)<br></code></pre></td></tr></table></figure><p>结果变得不稳定，同时出现数据溢出。由此，便提出了<code>Safe_softmax</code></p><h2 id="safe_softmax">Safe_softmax</h2><p>为了解决上述问题，我们可以很简单的对输入做一个平移。具体而言我们可以让每个x减去其所在行的最大值，核心可以描述为如下公式：</p><p><span class="math display">$$Softmax(x_i) = \frac{e^{x_i}}{\sum_{j}^{N} e^{x_j}} =\frac{e^{x_i-x_{max}}}{\sum_{j}^{N} e^{x_j-x_{max}}}$$</span></p><p>证明比较简单，我们用上一个标准的softmax不能跑的情况测试一次</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br>x = torch.randn(<span class="hljs-number">2</span>,<span class="hljs-number">5</span>)*<span class="hljs-number">100</span><br>x_max,_ = x.<span class="hljs-built_in">max</span>(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>x = x - x_max<br>row_sum = torch.exp(x).<span class="hljs-built_in">sum</span>(dim=-<span class="hljs-number">1</span>,keepdim=<span class="hljs-literal">True</span>)<br>ours_out = torch.exp(x) / row_sum<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;======= ours softmax =======&quot;</span>)<br><span class="hljs-built_in">print</span>(ours_out)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;======= Standard softmax =======&quot;</span>)<br><span class="hljs-built_in">print</span>(F.softmax(x, dim=-<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><p>输出： <figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs subunit">======= ours softmax =======<br>tensor([[0.0000e<span class="hljs-string">+00</span>, 0.0000e<span class="hljs-string">+00</span>, 7.4508e<span class="hljs-string">-32</span>, 9.3580e<span class="hljs-string">-40</span>, 1.0000e<span class="hljs-string">+00</span>],<br>        [5.3247e<span class="hljs-string">-28</span>, 0.0000e<span class="hljs-string">+00</span>, 1.0000e<span class="hljs-string">+00</span>, 2.9268e<span class="hljs-string">-07</span>, 0.0000e<span class="hljs-string">+00</span>]])<br>======= Standard softmax =======<br>tensor([[0.0000e<span class="hljs-string">+00</span>, 0.0000e<span class="hljs-string">+00</span>, 7.4508e<span class="hljs-string">-32</span>, 9.3580e<span class="hljs-string">-40</span>, 1.0000e<span class="hljs-string">+00</span>],<br>        [5.3247e<span class="hljs-string">-28</span>, 0.0000e<span class="hljs-string">+00</span>, 1.0000e<span class="hljs-string">+00</span>, 2.9268e<span class="hljs-string">-07</span>, 0.0000e<span class="hljs-string">+00</span>]])<br></code></pre></td></tr></table></figure></p><hr /><p>目前我们简单解决了Softmax的溢出问题，但是我们会发现其计算过程中，依赖全局的和。这也限制了我们希望能够局部分块处理Attention的计算。由此提出了<code>Online Softmax</code>。</p><h2 id="online-softmax">Online Softmax</h2><p><code>Online Softmax</code>将原来的计算过程，通过两个全局变量转变为了流式计算过程。</p><p>具体而言，我们假设输入<code>X</code>可以分为两个部分，即 <spanclass="math display"><em>X</em> = (<em>X</em><sub>1</sub>,<em>X</em><sub>2</sub>)</span>。我们依次处理两个部分。</p><p>首先针对<span class="math inline"><em>X</em><sub>1</sub></span>,我们通过如下公式计算，其行和最大值 <spanclass="math inline"><em>M</em><sub>1</sub></span>，以及<code>Row_Sum</code>:<spanclass="math inline"><em>L</em><sub>1</sub></span>,</p><!-- $$M_1 = max(X_1)  即每行的最大值$$ --><p><span class="math display">$$L_1 = \sum_{j=1}^{X_1.size(-1)} exp(X_1^j - M_1) , 即局部exp之和$$</span></p><p>由此，基于上述两个全局变量我们能够计算出这个局部的（即<spanclass="math inline"><em>X</em><sub>1</sub></span>）softmax值，同理我们可以得到<spanclass="math inline"><em>M</em><sub>2</sub>, <em>L</em><sub>2</sub></span></p><p>但是想要基于全局准确的Softmax结果还要进一步计算，具体可以做如下推导:</p><p><span class="math display">$$\begin{aligned}M &amp;= Max(M_1, M_2) \\L &amp;= \sum_{j=1}^{X.size(-1)} exp(X^j - M) \\&amp;= \sum_{j=1}^{X_1.size(-1)} exp(X_1^j - M) +\sum_{j=1}^{X_2.size(-1)} exp(X_2^j - M) \\&amp;= exp(M_1 - M) * \sum_{j=1}^{X_1.size(-1)} exp(X_1^j - M_1) +exp(M_2 - M) * \sum_{j=1}^{X_2.size(-1)} exp(X_2^j - M_2) \\&amp; = L_1 * exp(M_1 - M) + L_2 * exp(M_2 - M)\end{aligned}$$</span></p><p>由此我们更具局部的信息计算出来全局的两个信息，当然在实际输入顺序中，<spanclass="math inline"><em>L</em><sub>2</sub></span>可以不用计算，可以计算得到<spanclass="math inline"><em>M</em></span>之后，直接计算局部和， 即：</p><p><span class="math display">$$L = L_1 * exp(M_1 - M) + \sum_{j=1}^{X_2.size(-1)} exp(X_2^j - M)$$</span></p><p>得到两个的全局信息之后，在遍历一遍即可得到最后的Softmax值，具体的代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br>x = torch.randn(<span class="hljs-number">2</span>,<span class="hljs-number">16</span>)<br>out = torch.zeros_like(x)<br>x_blocks = torch.split(x, <span class="hljs-number">4</span>, dim=<span class="hljs-number">1</span>)<br>out_blocks = <span class="hljs-built_in">list</span>(torch.split(out, <span class="hljs-number">4</span>, dim=<span class="hljs-number">1</span>))<br><br>m = torch.ones(x.size(<span class="hljs-number">0</span>)).unsqueeze(-<span class="hljs-number">1</span>)*-<span class="hljs-number">1e9</span><br>l = torch.zeros(x.size(<span class="hljs-number">0</span>)).unsqueeze(-<span class="hljs-number">1</span>)<br><br><span class="hljs-keyword">for</span> xi <span class="hljs-keyword">in</span> x_blocks:<br>    mi,_ = xi.<span class="hljs-built_in">max</span>(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>    m_new = torch.maximum(mi,m)<br>    l = l*torch.exp(m - m_new) + torch.exp(xi-m_new).<span class="hljs-built_in">sum</span>(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>    m = m_new<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(x_blocks)):<br>    out_blocks[i] = torch.exp(x_blocks[i] - m) / l<br>    <br>out = torch.cat(out_blocks, dim=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;======= ours softmax =======&quot;</span>)<br><span class="hljs-built_in">print</span>(out)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;======= Standard softmax =======&quot;</span>)<br><span class="hljs-built_in">print</span>(F.softmax(x, dim=-<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><p>输出：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs lua">======= ours softmax =======<br>tensor(<span class="hljs-string">[[0.0072, 0.1026, 0.0273, 0.0057, 0.2055, 0.1391, 0.0090, 0.0694, 0.0036,</span><br><span class="hljs-string">         0.0191, 0.0087, 0.0462, 0.0069, 0.0355, 0.2882, 0.0259],</span><br><span class="hljs-string">        [0.1437, 0.0477, 0.0249, 0.0123, 0.0210, 0.0544, 0.0244, 0.0082, 0.2192,</span><br><span class="hljs-string">         0.1388, 0.0311, 0.1340, 0.0217, 0.0071, 0.0646, 0.0468]]</span>)<br>======= Standard softmax =======<br>tensor(<span class="hljs-string">[[0.0072, 0.1026, 0.0273, 0.0057, 0.2055, 0.1391, 0.0090, 0.0694, 0.0036,</span><br><span class="hljs-string">         0.0191, 0.0087, 0.0462, 0.0069, 0.0355, 0.2882, 0.0259],</span><br><span class="hljs-string">        [0.1437, 0.0477, 0.0249, 0.0123, 0.0210, 0.0544, 0.0244, 0.0082, 0.2192,</span><br><span class="hljs-string">         0.1388, 0.0311, 0.1340, 0.0217, 0.0071, 0.0646, 0.0468]]</span>)<br></code></pre></td></tr></table></figure><p>最后用一张图片来总结一下onlinesoftmax的思路，我觉得挺贴切的。即可以将原本三次扫描的SafeSoftmax转变为两次扫描的 Online Softmax</p><figure><img src="1.jpg" alt="示意图" /><figcaption aria-hidden="true">示意图</figcaption></figure><blockquote><p>当然在实际的Flash Attention中，可以进一步简化为onepass的扫描，这就下回分解了😋</p></blockquote><h1 id="参考">参考</h1><p><a href="https://zhuanlan.zhihu.com/p/5078640012">【手撕onlinesoftmax】Flash Attention前传，一撕一个不吱声</a></p>]]></content>
    
    
    <categories>
      
      <category>计算机</category>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据挖掘课程复习</title>
    <link href="/BLOG/2025/06/11/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0/"/>
    <url>/BLOG/2025/06/11/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<blockquote><p>用于HITSZ 25春数据挖掘复习</p></blockquote><h1 id="概述">概述</h1><h2 id="数据挖掘的含义">数据挖掘的含义</h2><p>数据挖掘是从海量数据集中发现有趣的（<mark>非平凡的、隐含的、未被发现但有用的</mark>）模式、模型或知识的过程。</p><h2 id="数据挖掘过程">数据挖掘过程</h2><p>知识发现的过程：<br />1. 数据预处理<br /> 1. 数据清洗<br /> 2. 数据集成<br /> 3. 数据选择<br /> 4. 数据变换<br />2. 数据挖掘<br />3. 模式/模型评估<br />4. 知识表示</p><h2 id="数据挖掘的应用">数据挖掘的应用</h2><ul><li>数据分析与决策支持<ul><li>客户画像、需求分析</li><li>交叉市场分析</li></ul></li><li>风险分析与管理<ul><li>竞争分析</li><li>风险评估</li><li>资产管理</li><li>资源规划</li></ul></li><li>欺诈检测与异常模式挖掘<ul><li>医疗、电信等</li></ul></li></ul><h2 id="数据仓库">数据仓库</h2><p><strong>定义</strong>：数据仓库是<mark>面向主题的、集成的、随时间变化的、又相对稳定的</mark>数据集合，它支持管理层的决策过程<br /><strong>数据仓库的关键特征</strong>：</p><ul><li>面向主题的：<ul><li>数据仓库的数据是以分析的主题为中心构建的</li></ul></li><li>集成的：<ul><li>数据仓库的数据来自不同的数据源，需要按照统一的结构、格式、度量、语义进行数据处理，最后集成到数据仓库的主题</li></ul></li><li>随时间变化的：<ul><li>数据仓库的历史数据，需要随时间延长而增加（周期性更新）</li><li>数据仓库的综合数据，需要随时间延长而变化</li></ul></li><li>相对稳定的：<ul><li>数据仓库的数据主要用于支持决策，不会涉及频繁的修改与变化，主要是查询与分析。</li></ul></li></ul><hr /><h1 id="数据特征分析与预处理">数据特征分析与预处理</h1><h2 id="数据类型及其特征">数据类型及其特征</h2><ul><li><strong>记录</strong>：以「行 -列」形式组织，含<strong>结构化和半结构化</strong>形态。<ul><li>例如：关系表、数值矩阵、文本词频、交易项目集合</li></ul></li><li><strong>图和网络</strong>：由「节点（实体）+边（关系）」构成拓扑结构，聚焦实体间关联。<ul><li>例如：如网页链接、社交关系、分子键</li></ul></li><li><strong>有序的</strong>：数据带时间/序列顺序，侧重“顺序依赖”与规律。<ul><li>例如：视频帧的先后、时间序列趋势、交易/遗传序列的顺序</li></ul></li><li><strong>空间/图像/多媒体</strong>：含空间维度或多媒体形态，分析空间分布/内容特征。<ul><li>例如：地图坐标、图像像素矩阵、视频帧序列</li></ul></li></ul><h2 id="数据对象">数据对象：</h2><p>数据集由数据对象构成，数据对象又称之为样本、实例。</p><figure><img src="样本.png" alt="数据样本示意图" /><figcaption aria-hidden="true">数据样本示意图</figcaption></figure><h2 id="属性类型">属性类型</h2><p><strong>定义</strong>：一个数据字段，表示一个数据对象的某个特征</p><p><strong>分类</strong>：</p><ul><li><p><strong>标称属性:</strong> 与名称相关的值、分类或枚举属性<mark>(定性的)</mark></p><ul><li><strong>二元属性:</strong>标称属性的一种特殊情况，只有2个状态的标称属性。</li></ul></li><li><p><strong>序数属性:</strong> 值有一个有意义的顺序<mark>(定性的)</mark></p></li><li><p><strong>数值属性:</strong> 可度量的量 <mark>(定量的)</mark></p><ul><li><strong>区间标度属性:</strong><ul><li>在统一度量的尺度下</li><li>值有序（e.g. 日历日期， 温度计数据）</li><li>没有固有0点</li></ul></li><li><strong>比率标度属性：</strong><ul><li>具有固有零点，可以说一个值是一个值得多少倍</li><li>举例：字数、体重、工作年限</li></ul></li></ul></li></ul><h2 id="数据的描述统计">数据的描述统计</h2><ul><li><strong>数据离散特征：</strong>均值、众数、最值、分位数、方差、离群点……</li><li><strong>排序区间：</strong><ul><li>数据离散度： 多个粒度上得分析</li><li>排序区间得盆图/分位数图分析</li></ul></li></ul><p><strong>描述统计的图形显示：</strong>箱型图、直方图、分位数图、散点图</p><h2 id="数据可视化方法">数据可视化方法</h2><p><strong>数据可视化方法得分类：</strong></p><ul><li>基于像素的可视化技术<ul><li>协方差矩阵的热力图</li></ul></li><li>基于几何投影的可视化技术<ul><li>直接可视化、散点图、透视地形、平行坐标</li></ul></li><li>基于图标的可视化技术<ul><li>脸谱图、人物画像图</li></ul></li><li>基于分层的可视化技术<ul><li>树状图、<em>锥形树</em></li></ul></li><li>可视化复杂数据与关系<ul><li>标签云、线形图、饼图、结构图/流程图、<em>怪图、系统进化图</em></li></ul></li></ul><h2 id="测量数据的相似性和相异性">测量数据的相似性和相异性</h2><ul><li><strong>相似性：</strong> 度量数据的相似程度，越大越相似，通常范围在<span class="math display">[0,1]</span></li><li><strong>相异性：</strong>(e.g. distance)，度量数据的差异，最小值通常为0 ，越大差异越大</li></ul><p><strong>相异度矩阵：</strong>对角线为0的下三角矩阵，元素为对应对象之间的距离 <imgsrc="相异度矩阵.png" alt="相异度矩阵矩阵" /></p><h3 id="标称属性的邻近度量">标称属性的邻近度量</h3><ul><li><p>得分匹配：</p><p><span class="math display">$$d(i,j) = \frac{p - m}{p}$$</span></p><p>其中 <span class="math inline"><em>m</em></span>为相同变量数量，<span class="math inline"><em>p</em></span>为变量总数。</p></li><li><p>独热编码：<br />具体而言即将名称或类别转化为独热编码，随后利用编码计算距离</p></li></ul><h3 id="二进制属性的度量">二进制属性的度量</h3><figure><img src="列联表.png" alt="列联表" /><figcaption aria-hidden="true">列联表</figcaption></figure><p><strong>对称二元变量距离：</strong> <span class="math inline">$d(i,j)= \frac{r+s}{q+r+s+t}$</span></p><p><strong>不对称二元变量距离：</strong> <spanclass="math inline">$d(i,j) = \frac{r+s}{q+r+s}$</span></p><p><strong>Jaccard系数(不对称二元变量相似性)：</strong> <spanclass="math inline">$Sim_{Jaccard}(i,j) = \frac{q}{q+r+s}$</span></p><p><strong>TIP:</strong> 解释一下这个对称的含义，关键在于0是否有意义<img src="解释.png" alt="对称的含义解释" /></p><h4id="例题二进制属性的非对称相异性度量">例题：二进制属性的非对称相异性度量</h4><figure><img src="例题1.png" alt="二进制属性的非对称相异性度量" /><figcaption aria-hidden="true">二进制属性的非对称相异性度量</figcaption></figure><h3 id="数值属性的度量">数值属性的度量</h3><p><strong>闵可夫斯基距离公式：</strong></p><p><span class="math display">$$D(X, Y) = \left( \sum_{i=1}^n |x_i -y_i|^p \right)^{\frac{1}{p}}$$</span></p><p>其中 <spanclass="math inline"><em>p</em> ≥ 1</span>，是可调节的距离参数。</p><p>主要使用上述公式度量，其中有一些特例可以参照下图： <imgsrc="距离.png" alt="闵可夫斯基距离公式特例" /></p><h3 id="区间尺度与有序变量的度量">区间尺度与有序变量的度量</h3><ul><li>区间尺度：进行归一化或标准化处理随后计算闵可夫斯基距离</li><li>有序变量：使用排序代替值，并对排序值做最值归一化，随后计算闵可夫斯基距离</li></ul><h3 id="其他度量方法">其他度量方法：</h3><h4 id="余弦相似度">余弦相似度：</h4><p><span class="math display">$$\text{cos}(\theta) = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n}A_i^2} \cdot \sqrt{\sum_{i=1}^{n} B_i^2}}$$</span></p><p>用于衡量两个向量方向的相似程度，常用于文本分类、推荐系统等。</p><h4 id="kl-散度kullback-leibler-divergence">KL 散度（Kullback-LeiblerDivergence）：</h4><p><span class="math display">$$D_{KL}(P \parallel Q) = \sum_{i=1}^{n} p_i \log \frac{p_i}{q_i}$$</span></p><p>衡量两个概率分布之间的差异，常用于模型分布与目标分布之间的对比。</p><hr /><h2 id="数据预处理">数据预处理</h2><p><strong>目的：</strong>数据存在不完全、噪音、不一致的问题，高质量的数据挖掘依赖高质量的数据</p><h3 id="数据预处理的主要任务">数据预处理的主要任务：</h3><ul><li><strong>数据清理：</strong> 异常值、缺失值、噪音<ul><li>针对缺失值：删除整个样本、均值或聚类填充</li><li>针对异常值与噪音： 通过分箱、聚类、回归去除</li></ul></li><li><strong>数据集成：</strong> 多个来源合到一起</li><li><strong>数据变换：</strong> 归一化等操作</li><li><strong>数据归约：</strong> 减小数据的冗余<ul><li>维度规约： 减少无用特征，或产生新的特征代替原来的<ul><li>决策树规约、PCA</li></ul></li><li>数值规约：减小数据量，例如用模型参数+离群点代替原始数据<ul><li>线性回归、聚类</li></ul></li><li>数据压缩</li></ul></li><li><strong>数据离散化和概念分层：</strong><ul><li>连续变量离散化</li><li>人为指定偏序关系</li></ul></li></ul><hr /><h1 id="关联规则">关联规则</h1><h2 id="关联规则挖掘概念">关联规则挖掘概念</h2><p>一种从事务数据集中发现项与项之间有趣关系的技术，形式为：</p><p><spanclass="math display"><em>X</em> ⇒ <em>Y</em>,  <em>X</em> ∩ <em>Y</em> = ∅</span></p><h3 id="支持度support">支持度（Support）：</h3><p><span class="math display">$$\text{Support}(X \Rightarrow Y) = \frac{\text{包含 } X \cup Y \text{的事务数}}{\text{总事务数}}$$</span></p><h3 id="置信度confidence">置信度（Confidence）：</h3><p><span class="math display">$$\text{Confidence}(X \Rightarrow Y) = \frac{\text{Support}(X \cupY)}{\text{Support}(X)} = P(Y|X)$$</span></p><h3 id="提升度lift">提升度（Lift）：</h3><p><span class="math display">$$\text{Lift}(X \Rightarrow Y) =\frac{\text{Confidence}(X \RightarrowY)}{\text{Support}(Y)} =  \frac{P(Y|X)}{P(Y)}$$</span></p><p>Lift &gt; 1 表示正相关，=1 表示独立，&lt;1 表示负相关。</p><h2 id="频繁项集与其分类">频繁项集与其分类</h2><h3 id="频繁项集">频繁项集：</h3><p>如果一个项集 <span class="math inline"><em>X</em></span>的支持度满足：</p><p><span class="math display">Support(<em>X</em>) ≥ min_sup</span></p><p>则 <span class="math inline"><em>X</em></span> 是频繁项集。</p><h3 id="闭频繁项集closed">闭频繁项集（Closed）：</h3><p>项集 <span class="math inline"><em>X</em></span>是闭的，当不存在一个超集 <spanclass="math inline"><em>Y</em> ⊃ <em>X</em></span>，使得：</p><p><spanclass="math display">Support(<em>Y</em>) = Support(<em>X</em>)</span></p><h3 id="极大频繁项集maximal">极大频繁项集（Maximal）：</h3><p>项集 <span class="math inline"><em>X</em></span>是极大的，当不存在一个超集 <spanclass="math inline"><em>Y</em> ⊃ <em>X</em></span> 是频繁的。</p><h3 id="三者关系">三者关系：</h3><ul><li>极大频繁项集 ⊆ 闭频繁项集 ⊆ 所有频繁项集；</li><li>闭项集压缩信息不丢失，极大项集压缩更彻底但不保留精确支持度。</li></ul><h2 id="关联规则基本模型">关联规则基本模型</h2><p>关联规则是指同时满足最小支持度和最小置信度阈值的规则，形式如下：</p><p><spanclass="math display"><em>X</em> ⇒ <em>Y</em>,  <em>X</em> ∩ <em>Y</em> = ∅</span></p><h3 id="挖掘流程">挖掘流程：</h3><ol type="1"><li>找出所有频繁项集（满足 <spanclass="math inline">Support ≥ min_sup</span>）；</li><li>从频繁项集中生成所有满足 <spanclass="math inline">Confidence ≥ min_conf</span> 的规则。</li></ol><p>只有同时满足这两个条件的规则，才被称为“强规则”。</p><h2 id="apriori算法流程">📌 Apriori算法流程</h2><p>Apriori是一种经典的频繁项集挖掘算法，用于从大量事务数据中发现频繁项集和生成关联规则。</p><h3 id="核心思想apriori原理">核心思想：Apriori原理</h3><blockquote><p><strong>如果一个项集是频繁的，那么它的所有子集也一定是频繁的。</strong></p></blockquote><p>也就是说，<strong>如果某个项集不是频繁的，则其所有超集都不可能是频繁的</strong>，因此可以在搜索中直接剪枝，提升效率。</p><h3 id="算法计算流程">算法计算流程</h3><h4 id="步骤-1扫描数据库找出频繁-1-项集l₁">步骤 1️⃣：扫描数据库，找出频繁1 项集（L₁）</h4><ul><li>统计每个单项的支持度；</li><li>过滤出支持度 ≥ <code>min_sup</code> 的项；</li><li>得到频繁 1 项集集合 L₁。</li></ul><h4 id="步骤-2由-l₁-构造候选-2-项集c₂">步骤 2️⃣：由 L₁ 构造候选 2项集（C₂）</h4><ul><li><strong>连接步骤</strong>：将 L₁ 中的项两两组合生成 2项候选项集；</li><li><strong>剪枝步骤</strong>：剔除那些包含非频繁子集的候选项集；</li><li>重新扫描数据库统计每个候选项集的支持度；</li><li>保留支持度 ≥ <code>min_sup</code> 的项集，得到频繁 2 项集 L₂。</li></ul><h4 id="步骤-3迭代构造更高阶项集-l₃l₄">步骤 3️⃣：迭代构造更高阶项集L₃、L₄…</h4><ul><li>从 L₂ 生成 C₃，再从中提取 L₃；</li><li>重复 <strong>连接-剪枝-计数</strong> 的过程；</li><li>直到某一轮没有产生新的频繁项集。</li></ul><h4 id="步骤-4由频繁项集生成关联规则">步骤4️⃣：由频繁项集生成关联规则</h4><ul><li>对每个频繁项集 <spanclass="math inline"><em>L</em></span>，枚举其所有非空子集 <spanclass="math inline"><em>S</em></span>；</li><li>构造规则 <spanclass="math inline"><em>S</em> ⇒ (<em>L</em>−<em>S</em>)</span>；</li><li>计算置信度（Confidence）：</li></ul><p><span class="math display">$$\text{Confidence}(S \Rightarrow T) = \frac{\text{Support}(S \cupT)}{\text{Support}(S)}$$</span></p><ul><li>只保留满足 <span class="math inline">Confidence ≥ min_conf</span>的规则。</li></ul><h3 id="提高-apriori-算法效率的方法">提高 Apriori 算法效率的方法</h3><p>Apriori算法在面对大规模数据时效率较低，以下是常用的几种优化策略：</p><ol type="1"><li><p>Hash-based itemset counting（基于哈希的项集计数）</p><ul><li>将候选项集映射到哈希桶中；</li><li>如果某个哈希桶中的计数低于最小支持度，则其对应的项集可直接剪枝；</li><li><strong>作用：减少无效候选项集数量。</strong></li></ul></li><li><p>Transaction reduction（事务压缩）</p><ul><li>每轮扫描后，删除不包含任何频繁项集的事务；</li><li>后续迭代无需再扫描这些事务；</li><li><strong>作用：减少数据扫描量，提高效率。</strong></li></ul></li><li><p>Partitioning（划分策略）</p><ul><li>将数据库划分为多个子集，分别挖掘局部频繁项集；</li><li>将局部频繁项集合并，再在全库中验证；</li><li><strong>只需两次完整扫描</strong>，适合大数据和并行处理。</li></ul></li><li><p>Sampling（采样）</p><ul><li>从数据库中随机抽取部分事务作为样本；</li><li>在样本上挖掘频繁项集，再用全体数据验证；</li><li><strong>优点：快速；缺点：可能漏掉边缘频繁项集。</strong></li></ul></li></ol><h2 id="关联规则的度量">关联规则的度量</h2><p>除了上述说到的支持度、置信度、提升度以外，还有一些度量方式。这里主要是四种零不变性度量。</p><p>在关联规则中，某些度量值不受<strong>零事务（nulltransactions）</strong>影响，称为<strong>零不变性度量</strong>。</p><ul><li>零事务：不包含任何待考察项集的事务；</li><li>零不变性：度量值只由 <spanclass="math inline"><em>P</em>(<em>A</em>|<em>B</em>)</span> 和 <spanclass="math inline"><em>P</em>(<em>B</em>|<em>A</em>)</span>决定，与总事务数无关；</li></ul><p>以下四种度量具有<strong>零不变性</strong>，且其值范围都在 <spanclass="math inline">[0,1]</span>，值越大表示 A 与 B 的关联越紧密。</p><ul><li><strong>全置信度</strong> <span class="math display">$$\text{all_confidence}(A, B) = \frac{\text{sup}(A \cupB)}{\max(\text{sup}(A), \text{sup}(B))} = \min(P(A|B), P(B|A))$$</span></li><li><strong>最大置信度 </strong> <spanclass="math display">max_confidence(<em>A</em>,<em>B</em>) = max (<em>P</em>(<em>A</em>|<em>B</em>),<em>P</em>(<em>B</em>|<em>A</em>))</span></li><li><strong>余弦置信度</strong><br /><span class="math display">$$\text{cosine}(A, B) = \frac{P(A \cap B)}{\sqrt{P(A) \cdot P(B)}} =\sqrt{P(B|A) \cdot P(A|B)}$$</span></li><li><strong>Kelu置信度</strong><br /><span class="math display">$$\text{Kulc}(A, B) = \frac{1}{2} \left( P(A|B) + P(B|A) \right)$$</span></li></ul><h1 id="分类">分类</h1><h2 id="模型评估方法">模型评估方法</h2><ul><li>留出法： 按照比例随机划分出训练集和测试集，多次随机抽样取均值</li><li>交叉验证： 把数据划分为k份，每次选择一份作为测试集，重复k次，取均值<ul><li>留一法： 每份仅有一个样本</li></ul></li><li>Bootstrap 自助法:又放回的抽样m次作为训练集，没有被抽到的作为测试集</li></ul><h2 id="分类模型评估指标">分类模型评估指标</h2><p><strong>混淆矩阵（Confusion Matrix）</strong></p><table><thead><tr class="header"><th>实际/预测</th><th>正类（预测）</th><th>负类（预测）</th></tr></thead><tbody><tr class="odd"><td>正类（实际）</td><td>TP（真正例）</td><td>FN（假负例）</td></tr><tr class="even"><td>负类（实际）</td><td>FP（假正例）</td><td>TN（真负例）</td></tr></tbody></table><ul><li><strong>TP</strong>：预测为正，实际为正</li><li><strong>FP</strong>：预测为正，实际为负</li><li><strong>FN</strong>：预测为负，实际为正</li><li><strong>TN</strong>：预测为负，实际为负</li></ul><p><strong>指标：</strong></p><ul><li>准确率</li></ul><p><span class="math display">$$\text{Accuracy} = \frac{TP + TN}{TP + FP + FN + TN}$$</span></p><ul><li>召回率</li></ul><p><span class="math display">$$\text{Recall} = \frac{TP}{TP + FN}$$</span></p><ul><li>精确率</li></ul><p><span class="math display">$$\text{Precision} = \frac{TP}{TP + FP}$$</span></p><ul><li>F1-Score</li></ul><p><span class="math display">$$\text{F1} = \frac{2 \cdot \text{Precision} \cdot\text{Recall}}{\text{Precision} + \text{Recall}}$$</span></p><ul><li>F-beta</li></ul><p><span class="math display">$$\text{F1} = \frac{(1+\beta^2) \cdot \text{Precision} \cdot\text{Recall}}{\beta^2 \cdot \text{Precision} + \text{Recall}}$$</span></p><ul><li><p>P-R 曲线（Precision-Recall Curve）</p><ul><li><strong>横轴：Recall（召回率）</strong></li><li><strong>纵轴：Precision（精确率）</strong></li><li>通过不断调整分类阈值，绘制出一系列 (P, R) 点，形成一条曲线</li></ul></li><li><p>ROC 曲线（Receiver Operating Characteristic）</p><ul><li>横轴：FPR（假正率） = <span class="math inline">$\frac{FP}{FP +TN}$</span></li><li>纵轴：TPR（真正率） = 召回率 = <spanclass="math inline">$\frac{TP}{TP + FN}$</span></li><li>曲线下的面积（AUC）越接近 1，模型越好</li></ul></li></ul><h2 id="决策树归纳算法的流程以-id3c4.5cart-为例">🛠️决策树归纳算法的流程（以 ID3/C4.5/CART 为例）</h2><ol type="1"><li><p><strong>输入训练集</strong> <spanclass="math inline"><em>D</em></span>，每条样本有多个属性 +类别标签。</p></li><li><p><strong>判断停止条件</strong>：</p><ul><li>样本全属于一个类别 → 生成叶节点，停止；</li><li>属性已用完，或样本不足 → 用多数类别作为叶节点标记。</li></ul></li><li><p><strong>选择最优划分属性 A</strong>：</p><ul><li>使用某种<strong>划分指标</strong>（如信息增益、增益率、基尼指数）选择属性A；</li><li>若无可用属性 → 直接创建叶节点。</li></ul></li><li><p><strong>按属性 A 的取值划分数据集</strong>：</p><ul><li>对每个取值 <spanclass="math inline"><em>a</em><sub><em>i</em></sub></span>，将数据集划分为<span class="math inline"><em>D</em><sub><em>i</em></sub></span>；</li><li>为每个子集创建分支子节点。</li></ul></li><li><p><strong>对子节点递归重复上述过程</strong>：</p><ul><li>在子集 <spanclass="math inline"><em>D</em><sub><em>i</em></sub></span>上继续构建子树。</li></ul></li><li><p><strong>形成整棵决策树</strong>：</p><ul><li>直到所有子集都满足停止条件。</li></ul></li></ol><h2 id="属性选择度量">属性选择度量</h2><p>有三种，分别为 信息增益、增益率、基尼系数</p><h3 id="信息增益information-gain">信息增益（Information Gain）</h3><p>信息增益是 ID3 算法使用的度量标准，它衡量的是：使用某个属性 A对数据进行划分后，<strong>系统信息熵（混乱度）减少了多少</strong>。</p><p>公式：</p><p><span class="math display">$$\text{Gain}(D, A) = Ent(D) - \sum_{v \in \text{Values}(A)}\frac{|D_v|}{|D|} \cdot Ent(D_v)$$</span></p><ul><li>其中 <span class="math inline">$Ent(D) = -\sum_{i=1}^{m}p_i\text{log}_2 (p_i)$</span> 是原始数据集的熵 ；</li><li><span class="math inline"><em>D</em><sub><em>v</em></sub></span>表示属性 A 取值为 <span class="math inline"><em>v</em></span>的子集；</li><li>值越大表示“划分后不确定性下降得越多”，越适合作为划分属性。</li></ul><p>缺点：偏向于选择取值多的属性（比如身份证号），容易过拟合。</p><hr /><h3 id="增益率gain-ratio">增益率（Gain Ratio）</h3><p>为了解决信息增益偏好多值属性的问题，C4.5 算法引入了增益率：</p><p><span class="math display">$$\text{GainRatio}(D, A) = \frac{\text{Gain}(D, A)}{IV(A)}$$</span></p><ul><li><span class="math inline"><em>I</em><em>V</em>(<em>A</em>)</span> 是A 的“固有值信息”或“分裂信息”，可以理解为划分的复杂度。 <spanclass="math display">$$IV(A) = - \sum_{j=1}^{v}\frac{|D_j|}{D} \text{log}_2(\frac{|D_j|}{D})$$</span></li><li>增益率在衡量信息增益的同时，<strong>惩罚属性值过多的划分</strong>，更加平衡。</li></ul><hr /><h3 id="基尼指数gini-index">基尼指数（Gini Index）</h3><p>CART算法采用基尼指数来衡量属性划分的纯净程度。基尼指数越小，表示子集越“纯”。</p><p>某个集合 D 的基尼指数定义为：</p><p><span class="math display">$$\begin{align}Gini(D) &amp;= 1 - \sum_{k=1}^{K} p_k^2 \\Gini_{split}(D) &amp;= \frac{|D_1|}{|D|}Gini(D_1) +\frac{|D_2|}{|D|}Gini(D_2)\end{align}$$</span></p><ul><li><span class="math inline"><em>p</em><sub><em>k</em></sub></span>表示属于第 k 类的概率；</li><li>当样本全属于一个类别时，Gini = 0；</li><li>每次选择能<strong>最小化划分后加权 Gini</strong>的属性作为最优划分属性。</li></ul><hr /><p>✅ 总结对比：</p><table><thead><tr class="header"><th>度量方式</th><th>使用算法</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr class="odd"><td>信息增益</td><td>ID3</td><td>简单直观，计算熵有理论基础</td><td>偏向多值属性，可能过拟合</td></tr><tr class="even"><td>增益率</td><td>C4.5</td><td>纠正信息增益偏差，权衡复杂度</td><td>偏向取值较均匀的属性</td></tr><tr class="odd"><td>基尼指数</td><td>CART</td><td>计算更快，适合二叉树构建</td><td>没有熵那样的理论解释</td></tr></tbody></table><h2 id="常见分类方法的主要思想">常见分类方法的主要思想</h2><h3 id="朴素贝叶斯分类naive-bayes-classification">朴素贝叶斯分类（NaiveBayes Classification）</h3><p>朴素贝叶斯是一种基于<strong>贝叶斯定理</strong>并假设属性之间相互条件独立的概率分类方法。</p><ul><li><p>给定训练数据集 <spanclass="math inline"><em>D</em></span>，每个样本表示为一个 <spanclass="math inline"><em>n</em></span> 维属性向量：<br /><spanclass="math display"><em>X</em> = (<em>x</em><sub>1</sub>,<em>x</em><sub>2</sub>,...,<em>x</em><sub><em>n</em></sub>)</span></p></li><li><p>假设总共有 <span class="math inline"><em>m</em></span>个类别：<spanclass="math inline"><em>C</em><sub>1</sub>, <em>C</em><sub>2</sub>, ..., <em>C</em><sub><em>m</em></sub></span><br />目标是确定输入 <span class="math inline"><em>X</em></span>属于哪个类别，即计算后验概率最大的类别： <spanclass="math display"><em>Ĉ</em> = arg max<sub><em>C</em><sub><em>i</em></sub></sub><em>P</em>(<em>C</em><sub><em>i</em></sub>∣<em>X</em>)</span></p></li><li><p>根据<strong>贝叶斯定理</strong>： <span class="math display">$$P(C_i \mid X) = \frac{P(C_i) \cdot P(X \mid C_i)}{P(X)}$$</span></p></li><li><p>因为 <span class="math inline"><em>P</em>(<em>X</em>)</span>对所有类别都相同，所以在分类时只需最大化分子部分： <spanclass="math display"><em>Ĉ</em> = arg max<sub><em>C</em><sub><em>i</em></sub></sub><em>P</em>(<em>C</em><sub><em>i</em></sub>) ⋅ <em>P</em>(<em>X</em>∣<em>C</em><sub><em>i</em></sub>)</span></p></li><li><p>若假设特征条件独立，则有： <span class="math display">$$P(X \mid C_i) = \prod_{k=1}^{n} P(x_k \mid C_i)$$</span></p></li></ul><h4 id="laplacian-correction拉普拉斯校准">LaplacianCorrection（拉普拉斯校准）</h4><p>为了解决0概率问题，引入 <strong>Laplacian 平滑（也叫 Add-OneSmoothing）</strong>，公式调整为：</p><p><span class="math display">$$P(x_k \mid C_i) = \frac{N_{ik} + 1}{N_i + d}$$</span></p><ul><li><spanclass="math inline"><em>N</em><sub><em>i</em><em>k</em></sub></span>：类别<span class="math inline"><em>C</em><sub><em>i</em></sub></span> 中属性<span class="math inline"><em>A</em><sub><em>k</em></sub></span> 取值为<span class="math inline"><em>x</em><sub><em>k</em></sub></span>的样本数<br /></li><li><spanclass="math inline"><em>N</em><sub><em>i</em></sub></span>：类别 <spanclass="math inline"><em>C</em><sub><em>i</em></sub></span>的总样本数<br /></li><li><span class="math inline"><em>d</em></span>：属性 <spanclass="math inline"><em>A</em><sub><em>k</em></sub></span>的可能取值数（即类别数）</li></ul><p>这样，即使某个值从未出现（<spanclass="math inline"><em>N</em><sub><em>i</em><em>k</em></sub> = 0</span>），加1 后也不会导致整个概率为 0。</p><h3 id="knn-分类k-nearest-neighbors">KNN 分类（K-NearestNeighbors）</h3><p>KNN（K近邻）是一种<strong>基于实例的非参数分类方法</strong>，核心思想是：<strong>“相似样本具有相似的类别。”</strong></p><p>基本流程：</p><ul><li>给定一个待分类样本，计算它与训练集中所有样本之间的距离（如欧几里得距离）；</li><li>选出距离最近的 <span class="math inline"><em>K</em></span>个“邻居”；</li><li>让这些邻居“投票”，投票最多的类别即为预测结果。</li></ul><h3 id="逻辑回归logistic-regression">逻辑回归（LogisticRegression）</h3><p>逻辑回归是一种<strong>用于二分类任务的线性模型</strong>，其本质是学习一个函数，输出属于某一类别的概率。</p><h4 id="基本思想">基本思想：</h4><ul><li><p>给定输入特征 <spanclass="math inline"><em>X</em> = (<em>x</em><sub>1</sub>,<em>x</em><sub>2</sub>,...,<em>x</em><sub><em>n</em></sub>)</span>，逻辑回归学习一个线性组合：</p><p><spanclass="math display"><em>z</em> = <em>w</em><sup><em>T</em></sup><em>x</em> + <em>b</em></span></p></li><li><p>然后通过<strong>Sigmoid 函数</strong>将其映射到 <spanclass="math inline">[0,1]</span> 范围，作为正类的概率：</p><p><span class="math display">$$P(y = 1 \mid x) = \frac{1}{1 + e^{-z}} = \frac{1}{1 + e^{-(w^T x + b)}}$$</span></p></li><li><p>预测时，通常设置阈值 <span class="math inline">0.5</span>，若<span class="math inline"><em>P</em> ≥ 0.5</span>则预测为正类，否则为负类。</p></li><li><p>没有闭合解，通常使用梯度下架配合交叉熵损失求解（在训练数据上拟合）</p></li></ul><h3 id="神经网络">神经网络</h3><p><maek>跳过</mark></p><h3 id="支持向量机svm分类">支持向量机（SVM）分类</h3><p>支持向量机是一种<strong>二分类模型</strong>，通过寻找一个最佳分割超平面，将不同类别的数据点分开。</p><h4 id="核心思想">核心思想：</h4><ul><li>寻找一个<strong>最大间隔（Margin）</strong>的超平面，使得距离超平面最近的样本点（支持向量）距离最大化；</li><li>最大间隔有助于提高模型的泛化能力。</li></ul><h4 id="线性可分情况">线性可分情况：</h4><ul><li><p>给定训练样本，SVM 找到一个线性超平面满足：</p><p><spanclass="math display"><em>w</em><sup><em>T</em></sup><em>x</em> + <em>b</em> = 0</span></p></li><li><p>并满足分类约束：</p><p><spanclass="math display"><em>y</em><sub><em>i</em></sub>(<em>w</em><sup><em>T</em></sup><em>x</em><sub><em>i</em></sub>+<em>b</em>) ≥ 1,  <em>i</em> = 1, 2, …, <em>n</em></span></p></li><li><p>目标是最大化间隔，即最小化 <spanclass="math inline">∥<em>w</em>∥<sup>2</sup></span>。</p></li></ul><h4 id="线性不可分情况">线性不可分情况：</h4><ul><li><p>使用<strong>软间隔（SoftMargin）</strong>，允许一定程度的分类错误，加入松弛变量 <spanclass="math inline"><em>ξ</em><sub><em>i</em></sub></span>；</p></li><li><p>目标函数变为：</p><p><span class="math display">$$\min_{w,b} \frac{1}{2}\|w\|^2 + C \sum_{i=1}^n \xi_i$$</span></p></li><li><p>其中 <span class="math inline"><em>C</em></span>控制误差惩罚强度。</p></li></ul><h4 id="非线性分类">非线性分类：</h4><ul><li>通过<strong>核函数（Kernel）</strong>，将数据映射到高维空间，使其线性可分；</li><li>常用核函数包括：<ul><li>线性核：<spanclass="math inline"><em>K</em>(<em>x</em>,<em>z</em>) = <em>x</em><sup><em>T</em></sup><em>z</em></span></li><li>多项式核：<spanclass="math inline"><em>K</em>(<em>x</em>,<em>z</em>) = (<em>x</em><sup><em>T</em></sup><em>z</em>+<em>c</em>)<sup><em>d</em></sup></span></li><li>径向基函数核（RBF）：<spanclass="math inline"><em>K</em>(<em>x</em>,<em>z</em>) = exp (−<em>γ</em>∥<em>x</em>−<em>z</em>∥<sup>2</sup>)</span></li></ul></li></ul><h3 id="集成方法ensemble-methods">集成方法（Ensemble Methods）</h3><p>集成方法是将多个<strong>弱分类器</strong>组合成一个强分类器的技术，旨在提高模型的准确性和稳定性。</p><h4 id="基本思想-1">基本思想：</h4><ul><li>通过结合多个模型的预测结果，减少单个模型的偏差和方差；</li><li>常见方式包括投票（分类）或平均（回归）。</li></ul><h4 id="主要类型">主要类型：</h4><ol type="1"><li><strong>Bagging（Bootstrap Aggregating）</strong><ul><li>通过自助采样（有放回抽样）构建多个训练子集，训练多个模型；<br /></li><li>预测时对多个模型结果投票或平均；<br /></li><li>代表算法：随机森林（Random Forest）。</li></ul></li><li><strong>Boosting</strong><ul><li>逐步训练一系列弱分类器，每个分类器关注前一个分类器错误分类的样本；<br /></li><li>通过加权组合提升整体性能；<br /></li><li>代表算法：AdaBoost、Gradient Boosting、XGBoost。</li></ul></li><li><strong>Stacking（堆叠）</strong><ul><li>训练多个不同类型的基模型；<br /></li><li>使用另一个模型（元学习器）学习如何组合基模型的输出。</li></ul></li></ol><h1 id="聚类">聚类</h1><p><strong>聚类分析：</strong>将数据对象分为多个类或簇，类内对象相似，类间对象相异</p><h2 id="基于划分的聚类方法">基于划分的聚类方法</h2><p>构造n个对象数据集D的划分，将其划分为k个列（所以需要给定K值的是基于划分方法的）</p><h3 id="k-平均聚类算法">K-平均聚类算法</h3><p><strong>步骤：</strong></p><ul><li>选择一个含有随机选择样本的k个簇的初始划分，计算这些簇的质心</li><li>根据欧氏距离把剩余的每个样本分配到距离它最近的簇质心的划分</li><li>计算被分配到每个簇的样本的均值向量，作为新的簇的质心</li><li>重复2,3直到k个簇的质心点不再发生变化或平方误差准则最小<ul><li>这里平方误差是说每个簇内样本到中心的平方误差的总和的总和</li></ul></li></ul><p><strong>优点:</strong> - 相对有效性，复杂度为 <spanclass="math inline"><em>O</em>(<em>k</em><em>n</em><em>t</em>)</span>，其中 n 是对象数目, k 是簇数目, t 是迭代次数</p><p><strong>缺点：</strong> - 需要预先指顶簇的数目k, -不能处理噪音数据和孤立点(outliers) -不适合用来发现具有非凸形状(non-convex shapes)的簇</p><h3 id="k-中心点聚类算法">K-中心点聚类算法</h3><p>主要用于解决K-均值在异常值上的缺点</p><p><strong>k-中心点算法流程:</strong> 1. 任意选取 k 个点作为 中心点</p><ol start="2" type="1"><li><p>按照与中心点最近的原则，将剩余点分配到当前最佳的中心点代表的类中</p></li><li><p>在每一类中，计算每个成员点对应的准则函数，选取准则函数最小时对应的点作为新的中心点</p></li><li><p>重复2-3的过程，直到所有的 中心点点不再发生变化，或已达到设定的最大迭代次数</p></li></ol><blockquote><p>其中准则函数为，一类中，某个成员点和其他成员点的距离之和</p></blockquote><p><strong>优点：</strong> 当存在噪音和孤立点时, K-medoids 比 K-means更健壮。 <strong>缺点：</strong> K-medoids 对于小数据集工作得很好,但不能很好地用于大数据集，计算质心的步骤时间复杂度是<spanclass="math inline"><em>O</em>(<em>n</em><sup>2</sup>)</span>，运行速度较慢</p><h3 id="pam算法">PAM算法</h3><p>是最早提出的k-中心点聚类算法</p><p><strong>PAM算法流程：</strong><br />1. 首先随机选择k个对象作为中心，把每个对象分配给离它最近的中心。<br />2. 对每个Medoid和非Medoid点(<spanclass="math inline"><em>O</em>(<em>k</em>(<em>n</em>−<em>k</em>))</span>)，尝试交换它们的位置，计算新的聚类代价(<spanclass="math inline"><em>O</em>(<em>n</em>−<em>k</em>)</span>)。<br />3.如果总的损失减少，则交换中心对象和非中心对象；如果总的损失增加，则不进行交换</p><p><strong>优点：</strong> 当存在噪音和孤立点时, K-medoids 比 K-means更健壮；<br /><strong>缺点：</strong> 同K-medoids， 每轮复杂度<spanclass="math inline"><em>O</em>(<em>k</em>(<em>n</em>−<em>k</em>)<sup>2</sup>)</span></p><h3 id="clara算法">CLARA算法</h3><p>不考虑整个数据集, 而是选择数据的一小部分作为样本</p><p><strong>CLARA算法流程：</strong><br />1. 从数据集中抽取一个样本集, 并对样本集使用PAM，划分为k个簇<br />2. 将其他未被抽到的样本划分到上述确定的簇中<br />3. 重复上述步骤，选择最好的聚类作为输出</p><p><strong>优点:</strong> 可以处理的数据集比 PAM大</p><p><strong>缺点:</strong> - 有效性依赖于样本集的大小 -基于样本的好的聚类并不一定是 整个数据集的好的聚类, 样本可能发生倾斜</p><h3 id="clarans算法">CLARANS算法</h3><h4 id="输入参数">输入参数：</h4><ul><li>数据集 <span class="math inline"><em>D</em></span>，样本个数 <spanclass="math inline"><em>n</em></span></li><li>聚类数 <span class="math inline"><em>k</em></span></li><li>每轮最大尝试次数 <code>max_neighbor</code></li><li>总共执行的局部搜索次数 <code>num_local</code></li></ul><h4 id="多次随机局部搜索共执行-num_local-次">多次随机局部搜索（共执行<code>num_local</code> 次）：</h4><p>对于每次局部搜索：</p><ol type="1"><li>随机选择初始的 <span class="math inline"><em>k</em></span> 个Medoids<br /></li><li>当前 Medoids 作为初始解，循环尝试邻居（即交换一个 Medoid 和一个非Medoid）：<ul><li>随机选择一个非 Medoid 点，与当前某个 Medoid交换，得到一个“邻居解”</li><li>计算这个新解的聚类代价</li><li>如果代价降低，则更新当前解为这个新解</li></ul></li><li>重复上述步骤，直到连续 <code>max_neighbor</code>次尝试都没有找到更优解</li><li>记录该局部搜索中最好的解</li></ol><h4 id="最终输出">最终输出：</h4><p>在 <code>num_local</code>次局部搜索中，选择代价最低的那个作为最终聚类结果</p><h2 id="基于层次的聚类方法">基于层次的聚类方法</h2><p>层次的聚类方法将数据对象组成一棵聚类的树,可以进一步分为: -凝聚的(agglomerative)层次聚类 (自底向上形成) - 分裂的(divisive)层次聚类(自顶向下形成)</p><h3 id="agnes-agglomerative-nesting算法">AGNES (AgglomerativeNesting)算法</h3><p><strong>算法流程：</strong></p><ul><li>初始化：计算包含每对样本间距离（如欧氏距离）的相似矩阵，把每个样本作为一个簇；<br /></li><li>选择：使用相似矩阵查找最相似的两个簇；<ul><li>两个簇间的相似度由这两个不同簇中距离最近的数据点对的相似度来确定－－单链接方法(Single-link)<br /></li></ul></li><li>更新：<ul><li>将两个簇合并为一个簇，簇的个数通过合并被更新；<br /></li><li>同时更新相似矩阵，将两个簇的两行（两列）距离用1行（1列）距离替换反映合并操作<br /></li></ul></li><li>重复：执行n-1次选择与更新；<br /></li><li>结束：当所有样本都合并成一个簇或满足某个终止条件时，整个过程结束</li></ul><h3 id="diana-divisive-analysis算法">DIANA (Divisive Analysis)算法</h3><p><strong>主要思想：</strong>采用自顶向下策略</p><ul><li>首先将所有样本置于一个簇中；</li><li>然后逐渐细分为越来越小的簇，来增加簇的数目；</li><li>直到每个样本自成一个簇，或者达到某个终结条件。</li><li>例如达到了某个希望的簇的数目或两个最近的簇之间的距离超过了某个阈值。</li></ul><h3 id="birch算法流程">BIRCH算法流程</h3><p>BIRCH 是一种基于层次结构的聚类算法，适合大规模数据，使用 CF-Tree来压缩数据并高效聚类。</p><h4 id="关键数据结构cfclustering-feature">关键数据结构：CF（ClusteringFeature）</h4><p>每个聚类用三元组表示：</p><p><spanclass="math display">CF = (<em>N</em>,<strong>L</strong><strong>S</strong>,<strong>S</strong><strong>S</strong>)</span></p><ul><li><span class="math inline"><em>N</em></span>：簇中点数<br /></li><li><span class="math inline">$\mathbf{LS} = \sum_{i=1}^N\mathbf{x}_i$</span>（点坐标线性和）<br /></li><li><span class="math inline">$\mathbf{SS} = \sum_{i=1}^N\mathbf{x}_i^2$</span>（点坐标平方和）</li></ul><p>利用 CF 可快速计算簇的质心、半径和直径，无需访问原始点。</p><h4 id="cf-tree-结构">CF-Tree 结构</h4><ul><li>一种平衡树，类似 B+ 树<br /></li><li><strong>非叶节点</strong>存储子节点的 CF 信息<br /></li><li><strong>叶子节点</strong>存储实际的 CF 条目，每个 CF表示一个子簇<br /></li><li>叶子节点通过链表相连，支持顺序访问<br /></li><li>主要参数：<ul><li><spanclass="math inline"><em>B</em></span>：非叶节点最大子节点数<br /></li><li><span class="math inline"><em>L</em></span>：叶子节点最大 CF条目数<br /></li><li>阈值 <span class="math inline"><em>T</em></span>：限制每个 CF最大半径</li></ul></li></ul><h4 id="算法流程">算法流程</h4><ol type="1"><li><strong>初始化</strong>：设定阈值 <spanclass="math inline"><em>T</em></span>、<spanclass="math inline"><em>B</em></span>、<spanclass="math inline"><em>L</em></span>，初始化空 CF-Tree。<br /></li><li><strong>逐点插入</strong>：<ul><li>从根节点开始递归，找到距离点最近的叶子节点 CF。<br /></li><li>尝试将点合并入该 CF，若合并后半径 ≤ <spanclass="math inline"><em>T</em></span>，更新 CF；否则新建 CF 条目。<br /></li></ul></li><li><strong>节点分裂</strong>：<ul><li>叶子节点 CF 条目数超过 <span class="math inline"><em>L</em></span>时分裂：<ul><li>选择两个最远的 CF 条目作为种子，重新分配其余 CF 条目。<br /></li><li>生成两个叶子节点，更新父节点。<br /></li></ul></li><li>父节点子节点数超过 <span class="math inline"><em>B</em></span>时递归分裂，直到根节点。<br /></li></ul></li><li><strong>完成构建</strong>。<br /></li><li><strong>全局聚类（可选）</strong>：对叶子节点所有 CF的质心进行聚类（如 K-Means）(在每个叶子节点的簇的层面再聚类)。</li></ol><h4 id="算法优缺点">算法优缺点</h4><ul><li><strong>优点</strong>：<ul><li>只需一次扫描，速度快<br /></li><li>数据压缩存储，节省内存<br /></li><li>支持增量更新和大规模数据<br /></li></ul></li><li><strong>缺点</strong>：<ul><li>对异常点敏感<br /></li><li>结果依赖阈值 <span class="math inline"><em>T</em></span> 选取</li></ul></li></ul><h3 id="cure算法简介含抽样处理">CURE算法简介（含抽样处理）</h3><h4 id="核心思想-1">核心思想</h4><ul><li>用多个代表点描述簇的形状，避免用质心造成的误差<br /></li><li>代表点收缩至簇中心以减少异常点影响<br /></li><li>支持复杂簇结构，抗噪声能力强</li></ul><h4 id="算法流程-1">算法流程</h4><ol type="1"><li><strong>抽样（Sampling）</strong><ul><li>从大数据中随机抽取一个代表样本子集，减小计算量</li></ul></li><li><strong>初始聚类</strong><ul><li>对抽样数据，每个点看作一个簇，或者先用快速算法做粗聚类，得到初始簇集合</li></ul></li><li><strong>选择代表点</strong><ul><li>每个簇选固定数量的代表点（如距离簇质心较远的点）</li></ul></li><li><strong>收缩代表点</strong><ul><li>将代表点沿向簇中心的方向收缩一定比例（如20%-30%）</li></ul></li><li><strong>合并簇</strong><ul><li>计算簇间代表点的最小距离，合并距离最近的两个簇</li></ul></li><li><strong>重复步骤 3-5</strong><ul><li>直到满足聚类数或者其他停止条件</li></ul></li><li><strong>增量聚类（可选）</strong><ul><li>对未抽样的数据，将每个点分配到最近的簇中，实现全量数据聚类</li></ul></li></ol><h4 id="特点">特点</h4><ul><li>抽样减少计算量<br /></li><li>代表点和收缩减少异常点影响<br /></li><li>能处理非球形簇，结构复杂</li></ul><h3 id="rock-算法简介">ROCK 算法简介</h3><p>ROCK（Robust Clustering usinglinKs）是一种基于邻居“链接”关系的层次聚类算法，特别适合处理类别数据和非球形簇。</p><h4 id="关键概念">关键概念</h4><ul><li><p><strong>邻居（Neighbors）</strong><br />如果两个数据点之间的相似度（例如 Jaccard 相似度）超过某个阈值 <spanclass="math display"><em>θ</em></span>，则认为它们是邻居。</p></li><li><p><strong>链接数（Link）</strong><br />两个点的链接数是它们共同邻居的数量。链接数越大，说明两点越有可能属于同一个簇。</p></li><li><p><strong>Jaccard 相似度</strong><br />衡量两个集合相似度的指标，定义为它们交集的大小除以并集的大小，用来计算点之间的相似性。</p></li><li><p><strong>好处度（Goodness Measure）</strong><br />用于评估合并两个簇的合理性。<br />计算公式为：</p><p><span class="math display">$$Goodness(C_i, C_j) = \frac{links(C_i, C_j)}{(size(C_i) + size(C_j))^{1 +2f(\theta)} - size(C_i)^{1 + 2f(\theta)} - size(C_j)^{1 + 2f(\theta)}}$$</span></p><p>其中：</p><ul><li><spanclass="math display"><em>l</em><em>i</em><em>n</em><em>k</em><em>s</em>(<em>C</em><sub><em>i</em></sub>,<em>C</em><sub><em>j</em></sub>)</span>是两个簇间所有点对的链接数之和<br /></li><li><spanclass="math display"><em>s</em><em>i</em><em>z</em><em>e</em>(<em>C</em>)</span>是簇中点的数量<br /></li><li><span class="math display"><em>f</em>(<em>θ</em>)</span>是阈值相关的函数，用于调节合并力度</li></ul></li></ul><h4 id="算法流程-2">算法流程</h4><ol type="1"><li><strong>计算邻居关系</strong><ul><li>对数据集中每对数据点计算相似度（如 Jaccard 相似度）。<br /></li><li>如果相似度大于阈值 <spanclass="math display"><em>θ</em></span>，则认为它们是邻居。</li></ul></li><li><strong>计算链接数</strong><ul><li>对每对数据点，统计它们共同邻居的数量，这个数量称为“链接数”。</li></ul></li><li><strong>初始化簇</strong><ul><li>将每个数据点作为一个单独的簇。</li></ul></li><li><strong>计算簇间好处度</strong><ul><li>根据簇间所有点对的链接数以及簇大小，计算两个簇合并的好处度（GoodnessMeasure）。</li></ul></li><li><strong>合并簇</strong><ul><li>选择好处度最高的两个簇进行合并。</li></ul></li><li><strong>重复合并</strong><ul><li>不断重复计算好处度和合并操作，直到达到预定的簇数或满足其他终止条件。</li></ul></li></ol><h4 id="特点-1">特点</h4><ul><li>适合类别数据和非球形簇<br /></li><li>利用邻居链接捕捉复杂的结构关系<br /></li><li>计算量较大，适合中小规模数据</li></ul><h3 id="chameleon算法">CHAMELEON算法</h3><p>CHAMELEON是一种基于图的层次聚类算法，能够自动发现不同形状和密度的簇，适用于复杂数据结构。它结合了动态模型和多阶段聚类，利用簇的内部连接性和相似性来决定合并策略。</p><h4 id="核心思想-2">核心思想</h4><ul><li>利用<strong>k-近邻图（k-NN Graph）</strong>表示数据点间的关系<br /></li><li>通过度量簇间的<strong>相对连通性（RelativeConnectivity）</strong>和<strong>相对相似性（RelativeCloseness）</strong>，决定簇是否合并<br /></li><li>动态调整合并过程，适应簇的形状和密度差异</li></ul><h4 id="主要概念">主要概念</h4><ul><li><p><strong>k-近邻图</strong><br />构建一个图，点为数据样本，边连接每个点的k个最近邻，边权反映点之间的相似度</p></li><li><p><strong>相对连通性（Relative Connectivity, RC）</strong><br />衡量两个簇间连接强度相对于它们内部连接的强度</p></li><li><p><strong>相对相似性（Relative Closeness, RCl）</strong><br />衡量两个簇之间的距离相对于它们各自的内部距离</p></li></ul><h4 id="算法流程-3">算法流程</h4><ol type="1"><li><strong>构建k-近邻图</strong><ul><li>计算数据点间的相似度，构造加权k-近邻图</li></ul></li><li><strong>初始聚类</strong><ul><li>使用图划分方法（如基于最小割的聚类）将数据划分成多个细粒度的小簇</li></ul></li><li><strong>计算簇间相似度</strong><ul><li>对每对簇计算相对连通性（RC）和相对相似性（RCl）</li></ul></li><li><strong>动态合并簇</strong><ul><li>根据RC和RCl的综合评价，选择合适的簇对进行合并</li></ul></li><li><strong>重复合并</strong><ul><li>直到满足预设的簇数或没有合适的簇对可以合并</li></ul></li></ol><h4 id="特点-2">特点</h4><ul><li>适应不同簇形状和密度<br /></li><li>结合了局部结构和全局信息<br /></li><li>适合处理复杂数据集，但计算复杂度较高</li></ul><h2 id="基于密度的聚类方法">基于密度的聚类方法</h2><h3 id="密度的概念">密度的概念</h3><h4 id="ε-邻域epsilon-neighborhood">1. ε-邻域（EpsilonNeighborhood）</h4><ul><li>给定一个点 <span class="math display"><em>p</em></span>，其<strong>ε-邻域</strong> 是所有与点 <spanclass="math display"><em>p</em></span> 距离不超过 <spanclass="math display"><em>ε</em></span> 的点的集合。</li><li>通俗地说，就是以点 <span class="math display"><em>p</em></span>为圆心，半径为 <span class="math display"><em>ε</em></span>的“圆”里有谁。</li></ul><h4 id="核心对象core-object">2. 核心对象（Core Object）</h4><ul><li>如果一个点的 ε-邻域内至少包含<strong>MinPts</strong>（最小点数，包括它自己）个点，则称它为<strong>核心对象</strong>。</li><li>它说明该区域“密度足够大”。</li></ul><h4 id="直接密度可达directly-density-reachable-ddr">3.直接密度可达（Directly Density-Reachable, DDR）</h4><ul><li>如果点 <span class="math display"><em>p</em></span> 在点 <spanclass="math display"><em>q</em></span> 的 ε-邻域内，<strong>且 <spanclass="math display"><em>q</em></span>是核心对象</strong>，那么我们说：<br /><strong>点 <span class="math display"><em>p</em></span> 是从点 <spanclass="math display"><em>q</em></span> 直接密度可达的</strong>。</li><li>注意：只有从核心对象出发，才能有“直接密度可达”。</li></ul><h4 id="密度可达density-reachable">4. 密度可达（Density-Reachable）</h4><ul><li>如果存在一条由多个点组成的链 <spanclass="math display"><em>p</em><sub>1</sub>, <em>p</em><sub>2</sub>, ..., <em>p</em><sub><em>n</em></sub></span>，使得：<ul><li><spanclass="math display"><em>p</em><sub>1</sub> = <em>q</em></span>，<spanclass="math display"><em>p</em><sub><em>n</em></sub> = <em>p</em></span>；</li><li>对于每对相邻点 <spanclass="math display"><em>p</em><sub><em>i</em></sub></span> 和 <spanclass="math display"><em>p</em><sub><em>i</em> + 1</sub></span>，都有<span class="math display"><em>p</em><sub><em>i</em> + 1</sub></span>是从 <span class="math display"><em>p</em><sub><em>i</em></sub></span><strong>直接密度可达</strong>的；</li></ul></li><li>那么我们说：<strong>点 <span class="math display"><em>p</em></span>是从点 <span class="math display"><em>q</em></span>密度可达的</strong>。</li></ul><blockquote><p>这个关系是<strong>单向的</strong>：即 <spanclass="math display"><em>p</em></span> 从 <spanclass="math display"><em>q</em></span> 可达，不代表 <spanclass="math display"><em>q</em></span> 从 <spanclass="math display"><em>p</em></span> 可达。</p></blockquote><h4 id="密度相连density-connected">5. 密度相连（Density-Connected）</h4><ul><li>如果存在某个点 <span class="math display"><em>o</em></span>，使得：<ul><li>点 <span class="math display"><em>p</em></span> 和点 <spanclass="math display"><em>q</em></span> 都是从 <spanclass="math display"><em>o</em></span> 密度可达的；</li></ul></li><li>那么我们说：<strong>点 <span class="math display"><em>p</em></span>和点 <span class="math display"><em>q</em></span>是密度相连的</strong>。</li></ul><blockquote><p>这个关系是<strong>对称的</strong>，用于判断是否属于同一个簇。</p></blockquote><h4 id="示例">示例</h4><figure><img src="p.png" alt="密度相关概念的例子" /><figcaption aria-hidden="true">密度相关概念的例子</figcaption></figure><h3 id="dbscan算法">DBSCAN算法</h3><h4 id="核心参数">核心参数</h4><ul><li><strong><spanclass="math display"><em>ε</em></span>（eps）</strong>：定义点的“邻域”范围<br /></li><li><strong>MinPts</strong>：最小邻居点数，决定一个点是否为“核心点”</li></ul><h4 id="算法流程-4">算法流程</h4><ol type="1"><li><p><strong>初始化状态</strong><br />所有数据点标记为“未访问”。</p></li><li><p><strong>从一个未访问点 <spanclass="math inline"><em>p</em></span> 开始：</strong></p><ul><li>计算 <span class="math inline"><em>p</em></span> 的 <spanclass="math inline"><em>ε</em></span>-邻域（记为 <spanclass="math inline"><em>N</em><sub><em>ε</em></sub>(<em>p</em>)</span>）；</li><li>如果邻域内的点数 &lt; MinPts：将 <spanclass="math inline"><em>p</em></span> 标记为噪声，跳过；</li><li>否则，将 <span class="math inline"><em>p</em></span>作为<strong>核心点</strong>，创建一个新簇，将 <spanclass="math inline"><em>N</em><sub><em>ε</em></sub>(<em>p</em>)</span>中的所有点加入该簇。</li></ul></li><li><p><strong>扩展当前簇：</strong><br />对刚加入簇的每个点 <spanclass="math inline"><em>q</em></span>（若未访问）执行：</p><ul><li>标记 <span class="math inline"><em>q</em></span> 为已访问；</li><li>若 <span class="math inline"><em>q</em></span>是核心点（其邻域内点数 ≥ MinPts）：<ul><li>将 <spanclass="math inline"><em>N</em><sub><em>ε</em></sub>(<em>q</em>)</span>中的所有点加入当前簇（即密度可达）；</li></ul></li><li>否则（<span class="math inline"><em>q</em></span>是边界点），保留在当前簇中，不继续扩展。</li></ul></li><li><p><strong>重复步骤 2~3</strong></p><ul><li>直到所有点都被访问。</li></ul></li></ol><h4 id="dbscan-的输出">DBSCAN 的输出</h4><ul><li>若干个簇（每个由核心点及其密度可达的点组成）<br /></li><li>一些噪声点（不属于任何簇）</li></ul><h4 id="优点">优点</h4><ul><li>不需要预先指定簇数<br /></li><li>可发现任意形状的簇<br /></li><li>可识别噪声点<br /></li><li>对离群点不敏感</li></ul><h4 id="缺点">缺点</h4><ul><li>需要合适的 <span class="math display"><em>ε</em></span> 和MinPts<br /></li><li>当数据密度差异较大时效果较差<br /></li><li>高维数据中，距离不再可靠（维数灾难）</li></ul><h3 id="optics算法">OPTICS算法</h3><p>OPTICS（Ordering Points To Identify the ClusteringStructure）是一种基于密度的聚类算法，解决了 DBSCAN对参数敏感的问题。它不仅能发现不同密度的簇，还能输出数据的聚类结构排序，方便后续分析。</p><h4 id="相关概念">相关概念</h4><ul><li><p><strong>核心距离（Core Distance）</strong><br />对点 <span class="math display"><em>p</em></span>，如果它的 <spanclass="math display"><em>ε</em></span>-邻域内至少有 MinPts个点，则核心距离是到第 MinPts 个最近邻的距离；否则未定义。</p></li><li><p><strong>可达距离（Reachability Distance）</strong><br />对点 <span class="math display"><em>p</em></span> 和它的邻居 <spanclass="math display"><em>o</em></span>，可达距离定义为：</p><p><spanclass="math display"><em>r</em><em>e</em><em>a</em><em>c</em><em>h</em><em>a</em><em>b</em><em>i</em><em>l</em><em>i</em><em>t</em><em>y</em>_<em>d</em><em>i</em><em>s</em><em>t</em>(<em>o</em>,<em>p</em>) = max (<em>c</em><em>o</em><em>r</em><em>e</em>_<em>d</em><em>i</em><em>s</em><em>t</em>(<em>p</em>),<em>d</em><em>i</em><em>s</em><em>t</em>(<em>p</em>,<em>o</em>))</span></p><p>如果 <span class="math display"><em>p</em></span> 是核心点，且 <spanclass="math display"><em>o</em></span> 在 <spanclass="math display"><em>p</em></span> 的邻域内，否则未定义。</p></li></ul><h4 id="算法流程-5">算法流程</h4><ol type="1"><li><strong>初始化</strong><ul><li>给数据集中每个点设置状态为“未访问”。<br /></li><li>给每个点设置可达距离 <spanclass="math display"><em>r</em><em>e</em><em>a</em><em>c</em><em>h</em><em>a</em><em>b</em><em>i</em><em>l</em><em>i</em><em>t</em><em>y</em>_<em>d</em><em>i</em><em>s</em><em>t</em> = ∞</span>（表示未知）。<br /></li><li>准备一个空的“输出顺序列表”（ordering），用于记录访问点的顺序。</li></ul></li><li><strong>遍历所有点</strong><br />对数据集中的每个点 <span class="math display"><em>p</em></span>：<ul><li>如果 <span class="math display"><em>p</em></span>已经被访问过，跳过；<br /></li><li>否则，开始处理 <span class="math display"><em>p</em></span>：</li></ul></li><li><strong>访问点 <span class="math display"><em>p</em></span></strong><ul><li>将 <span class="math display"><em>p</em></span>标记为“已访问”。<br /></li><li>计算 <span class="math display"><em>p</em></span> 的核心距离 <spanclass="math display"><em>c</em><em>o</em><em>r</em><em>e</em>_<em>d</em><em>i</em><em>s</em><em>t</em>(<em>p</em>)</span>：<ul><li>找出 <span class="math display"><em>p</em></span> 的 <spanclass="math display"><em>ε</em></span>-邻域中的所有点，排序后取第 MinPts个最近邻点的距离作为核心距离。<br /></li><li>如果邻域内点数不足 MinPts，则 <spanclass="math display"><em>p</em></span>不是核心点，核心距离未定义。<br /></li></ul></li><li>把 <span class="math display"><em>p</em></span>加入输出顺序列表（ordering），此时 <spanclass="math display"><em>p</em></span>的可达距离保持为当前值（初始可能为无穷大）。</li></ul></li><li><strong>如果 <span class="math display"><em>p</em></span>是核心点</strong><ul><li>取 <span class="math display"><em>p</em></span> 的 <spanclass="math display"><em>ε</em></span>-邻域内所有<strong>未访问的点</strong>，放入一个优先队列（或类似结构），根据可达距离排序（距离越小优先）。<br /></li><li>对这些邻居点 <span class="math display"><em>o</em></span>：<ul><li>计算从 <span class="math display"><em>p</em></span> 到 <spanclass="math display"><em>o</em></span> 的可达距离：<br /><spanclass="math display"><em>r</em><em>e</em><em>a</em><em>c</em><em>h</em><em>a</em><em>b</em><em>i</em><em>l</em><em>i</em><em>t</em><em>y</em>_<em>d</em><em>i</em><em>s</em><em>t</em>(<em>o</em>) = max (<em>c</em><em>o</em><em>r</em><em>e</em>_<em>d</em><em>i</em><em>s</em><em>t</em>(<em>p</em>),<em>d</em><em>i</em><em>s</em><em>t</em>(<em>p</em>,<em>o</em>))</span><br /></li><li>如果 <span class="math display"><em>o</em></span>当前的可达距离大于这个新值，则更新 <spanclass="math display"><em>o</em></span> 的可达距离为新值，并将 <spanclass="math display"><em>o</em></span> 加入优先队列。<br /></li></ul></li><li>依次从优先队列中取出可达距离最小的点 <spanclass="math display"><em>q</em></span>，重复步骤 3 和4，直到优先队列空。</li></ul></li><li><strong>继续处理剩余点</strong><ul><li>返回步骤 2，处理下一个未访问的点，直到所有点都被访问。</li></ul></li></ol><h4 id="结果输出">结果输出</h4><ul><li>一条有序的点列表，反映数据的密度聚类结构。<br /></li><li>通过绘制 <strong>可达距离图（ReachabilityPlot）</strong>，可以直观观察不同密度簇的分布和边界。</li></ul><figure><img src="可达距离.jpg" alt="可达距离图示意" /><figcaption aria-hidden="true">可达距离图示意</figcaption></figure><h4 id="优点-1">优点</h4><ul><li>不需要像 DBSCAN 那样对 <span class="math display"><em>ε</em></span>选取非常敏感。<br /></li><li>能发现不同密度的簇。<br /></li><li>输出的可达距离序列方便后续聚类分析。</li></ul><h2 id="基于网格的聚类方法">基于网格的聚类方法</h2><h3 id="sting算法">STING算法</h3><p>STING是一种基于网格（Grid-based）的聚类方法，通过将空间划分为不同层次的网格结构，并基于每个网格单元中的统计信息来进行聚类判断。</p><h4 id="核心思想-3">核心思想</h4><ul><li>将数据空间预先划分成多个<strong>固定大小的单元格（网格）</strong>。</li><li>构建一个多层次的网格索引结构（层次树），每一层网格的粒度逐渐增大（上层粒度粗，下层粒度细）。</li><li>每个网格单元保存统计信息，如：<ul><li>对象数（count）</li><li>平均值（mean）</li><li>方差（variance）</li><li>最小值、最大值等</li></ul></li></ul><h4 id="算法流程-6">算法流程</h4><ol type="1"><li><strong>构建网格结构</strong><ul><li>划分空间：将数据空间划分为若干固定大小的单元格。</li><li>构建多层结构，每层是上一层的粗粒度抽象。</li></ul></li><li><strong>统计每个单元格的属性</strong><ul><li>在数据预处理阶段，统计每个单元格内的数据统计量，并自底向上传递到上层节点（汇总统计）。</li></ul></li><li><strong>选择合适的层次作为分析起点</strong><ul><li>通常选择中间层开始，粒度适中，既不太粗也不太细。</li></ul></li><li><strong>过滤和合并网格单元</strong><ul><li>对目标查询区域内的单元格：<ul><li>如果统计指标满足某些条件（如密度高），则进一步下钻到下一层；</li><li>否则停止，或直接排除该区域；</li></ul></li><li>递归进行，直到达到最低层或不能再分。</li></ul></li><li><strong>输出聚类区域</strong><ul><li>最终，在最底层（最细的网格）里，把满足“高密度”的小单元格合并在一起，作为一个簇。</li><li>类似地，另一个位置的网格也可能形成另一个簇。</li></ul></li></ol><h4 id="通俗例子">通俗例子</h4><p>假设我们有一个二维空间，数据分布如下：</p><ul><li>在左上角有一堆密集点<br /></li><li>在右下角又有一堆密集点<br /></li><li>其它区域是稀疏的</li></ul><p><strong>STING 做法：</strong></p><ul><li>初步划分整个区域为 4×4 网格，每个网格记录点的数量<br /></li><li>找出有密集数据的几个网格,对这些网格再细分（下钻）</li></ul><p><strong>最终：</strong></p><ul><li>左上角细分后合并成“簇 A”<br /></li><li>右下角细分后合并成“簇 B”<br /></li><li>其它区域被排除，不属于任何簇</li></ul><h4 id="优点-2">优点</h4><ul><li><strong>高效</strong>：由于只操作统计摘要，而非原始数据，效率高。</li><li><strong>可扩展性好</strong>：支持大规模数据集。</li><li><strong>支持多层次分析</strong>：可以灵活选择不同粒度的层级进行聚类。</li></ul><h4 id="缺点-1">缺点</h4><ul><li>聚类结果依赖于网格划分的方式，不够灵活。</li><li>网格边界可能造成聚类割裂（称为边界效应）。</li></ul><h3 id="clique-算法clustering-in-quest">CLIQUE 算法（Clustering InQUEst）</h3><p>CLIQUE 是一种 <strong>基于网格 +子空间探索的聚类算法</strong>，专门用于发现<strong>高维数据中的密度簇</strong>。</p><h4 id="核心思想-4">核心思想</h4><ul><li>将每个维度划分为固定数量的等宽区间（网格单元格）。</li><li>构建所有可能的低维子空间，并在这些子空间中寻找密度足够高的网格单元。</li><li>利用 <strong>Apriori原则</strong>：高维的密度区域必定由低维的密度区域组合而成。</li><li>通过递归方式找到所有“密度簇”所在的维度子空间，并形成聚类。</li></ul><h4 id="算法流程-7">算法流程</h4><ol type="1"><li><strong>空间划分</strong><ul><li>对每个维度划分为 <span class="math display"><em>ξ</em></span>个等宽区间（即网格单元）。</li><li>在 <span class="math display"><em>d</em></span> 维空间中，初始共形成<span class="math display"><em>ξ</em><sup><em>d</em></sup></span>个单位网格。</li></ul></li><li><strong>密度单元检测（初始在 1 维）</strong><ul><li>统计每个 1 维网格单元中的点数，若超过阈值 <spanclass="math display"><em>τ</em></span>，称为“密度单元”。</li></ul></li><li><strong>子空间扩展（类似 Apriori）</strong><ul><li>逐步组合多个维度（如 2 维、3 维…）形成更高维的子空间。</li><li>只对在低维中都是“密度单元”的组合进行尝试，减少搜索空间。</li></ul></li><li><strong>聚类形成</strong><ul><li>将在所有子空间中找到的密度单元格相邻的部分连接起来，构成聚类。</li><li>可以用连通分量的方式将多个密度单元“连通”成一个簇。</li></ul></li><li><strong>输出聚类结果</strong><ul><li>返回所有簇的点集和对应的子空间（说明在哪些维度中形成簇）。</li><li>CLIQUE 支持自动检测出<strong>不同维度的簇结构</strong>，适合高维数据分析。</li></ul></li></ol><h4 id="优点-3">优点</h4><ul><li>能发现<strong>任意维度子空间中的簇</strong>，适合处理高维稀疏数据；</li><li>网格结构使得它对噪声具有鲁棒性；</li><li>自动确定簇的数量和维度。</li></ul><h4 id="缺点-2">缺点</h4><ul><li>网格分辨率（ξ）选得不合适会影响聚类效果；</li><li>枚举所有子空间仍可能面临维度灾难，适合中等维度使用。</li></ul><h2 id="基于模型的聚类方法">基于模型的聚类方法</h2><h3 id="em-算法expectation-maximization">EM算法（Expectation-Maximization）</h3><p>EM是一种<strong>迭代优化算法</strong>，用于含有“隐藏变量”的概率模型，常用于估计最大似然参数。聚类中常用于<strong>高斯混合模型（GMM）</strong>。</p><h4 id="核心思想-5">核心思想</h4><ul><li>数据可能来自多个概率分布（比如多个高斯分布），但我们不知道每个样本来自哪个分布。</li><li>EM 用“猜测 → 估计 → 更新”的方式迭代：<ul><li><strong>E步</strong>（期望步）：根据当前参数，估计每个样本来自各个分布的概率（软聚类）。</li><li><strong>M步</strong>（最大化步）：根据上一步的估计，重新计算模型参数，使其更符合当前数据。</li></ul></li></ul><h4 id="应用于聚类的流程以-gmm-为例">应用于聚类的流程（以 GMM为例）</h4><p>设数据集为 <spanclass="math display"><em>X</em> = {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>n</em></sub>}</span>，假设它由<span class="math display"><em>k</em></span> 个高斯分布混合而成。</p><ol type="1"><li><strong>初始化参数</strong><ul><li>初始化每个分布的参数：均值 <spanclass="math display"><em>μ</em><sub><em>k</em></sub></span>、协方差<span class="math display"><em>Σ</em><sub><em>k</em></sub></span>和混合系数 <spanclass="math display"><em>π</em><sub><em>k</em></sub></span>（即每个簇的概率）。</li></ul></li><li><strong>E 步（Expectation）</strong><ul><li>计算每个样本 <spanclass="math display"><em>x</em><sub><em>i</em></sub></span> 属于每个簇<span class="math display"><em>k</em></span>的<strong>后验概率</strong>（即“软分类”）： <spanclass="math display">$$\gamma_{ik} = P(z_i = k \mid x_i) = \frac{\pi_k \cdot \mathcal{N}(x_i\mid \mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_j \cdot \mathcal{N}(x_i \mid\mu_j, \Sigma_j)}$$</span></li></ul></li><li><strong>M 步（Maximization）</strong><ul><li>用 E 步得到的“软分类”更新参数：<ul><li>更新均值： <span class="math display">$$\mu_k = \frac{\sum_{i=1}^n \gamma_{ik} x_i}{\sum_{i=1}^n \gamma_{ik}}$$</span></li><li>更新协方差： <span class="math display">$$\Sigma_k = \frac{\sum_{i=1}^n \gamma_{ik} (x_i - \mu_k)(x_i -\mu_k)^T}{\sum_{i=1}^n \gamma_{ik}}$$</span></li><li>更新混合系数： <span class="math display">$$\pi_k = \frac{1}{n} \sum_{i=1}^n \gamma_{ik}$$</span></li></ul></li></ul></li><li><strong>判断是否收敛</strong><ul><li>如果对数似然函数变化很小，就停止迭代；否则回到 E 步。</li></ul></li></ol><h4 id="简单例子二维聚类">简单例子（二维聚类）</h4><p>假设你有一组二维数据（比如年龄 vs收入），你猜它由两个高斯分布组成：</p><ul><li>初始：<ul><li>猜两个中心点和协方差</li></ul></li><li>E 步：<ul><li>对每个点，算它属于两个簇的概率（可能是 60% 属于 A，40% 属于 B）</li></ul></li><li>M 步：<ul><li>用这些概率更新两个高斯分布的参数（比如中心位置向有更多点的方向移动）</li></ul></li><li>重复，直到稳定</li></ul><h4 id="特点-3">特点</h4><table><thead><tr class="header"><th>优点</th><th>缺点</th></tr></thead><tbody><tr class="odd"><td>能做“软聚类”</td><td>对初始值敏感，易陷入局部最优</td></tr><tr class="even"><td>能适用于复杂概率模型</td><td>假设数据满足特定分布（如高斯）</td></tr><tr class="odd"><td>理论成熟，适用于密度估计、图像分割等</td><td>不适合非凸或非高斯的聚类结构</td></tr></tbody></table><h2 id="聚类的稳定性评估">聚类的稳定性评估</h2><p>使用 Jaccard 系数 + Bootstrap 评估聚类稳定性</p><h3 id="步骤">步骤</h3><ol type="1"><li>原始聚类：<ul><li>用你的聚类算法（如 KMeans、DBSCAN、GMM）对原始数据 D做聚类，得到原始簇集合 C = {C1, C2, …, Ck}。</li></ul></li><li>多次 bootstrap 重采样：<ul><li>对 D 重采样 B 次，得到 D1, D2, …, DB。</li><li>每次都用相同聚类算法得到聚类结果 C1’, C2’, …, CB’。</li></ul></li><li>计算每个原始簇 Cᵢ 的 Jaccard 稳定性：<ul><li>对每次 bootstrap，找一个和 Cᵢ 最相似的簇 C’ᵢ（通常用最大 Jaccard匹配）。</li><li>计算 Jaccard 指数： <span class="math display">$$J(C_i, C_i') = \frac{|C_i \cap C_i'|}{|C_i \cup C_i'|}$$</span></li><li>重复 B 次，得到 Cᵢ 的 Jaccard 系数分布。</li></ul></li><li>聚类稳定性评估：<ul><li>对每个簇，计算 Jaccard 系数的平均值或分布。</li><li>平均值高 → 表明该簇在不同样本下“经常出现”，稳定性好。</li></ul></li></ol><h2 id="聚类质量外在评估方法">聚类质量外在评估方法</h2><p>外部方法指的是存在类别划分的ground true，有以下几种方法。</p><hr /><h3 id="纯度puritymatching-based-methods">纯度（Purity）：Matching-basedmethods</h3><p>Purity 是一种<strong>外部聚类评估指标</strong>，用于衡量聚类结果与真实标签（GroundTruth）之间的一致性程度。</p><p><strong>Purity 的计算公式如下：</strong></p><p><span class="math display">$$\text{Purity} = \frac{1}{n} \sum_{i=1}^{k} \max_j |C_i \cap L_j|$$</span></p><p><strong>公式说明：</strong></p><ul><li><span class="math display"><em>n</em></span>：样本总数<br /></li><li><span class="math display"><em>k</em></span>：聚类簇的数量<br /></li><li><spanclass="math display"><em>C</em><sub><em>i</em></sub></span>：第 <spanclass="math display"><em>i</em></span> 个聚类簇<br /></li><li><spanclass="math display"><em>L</em><sub><em>j</em></sub></span>：真实的第<span class="math display"><em>j</em></span> 类<br /></li><li><spanclass="math display">|<em>C</em><sub><em>i</em></sub>∩<em>L</em><sub><em>j</em></sub>|</span>：簇<span class="math display"><em>C</em><sub><em>i</em></sub></span>中属于类 <spanclass="math display"><em>L</em><sub><em>j</em></sub></span>的样本数量</li></ul><p>换句话说：<br />对于每个簇 <spanclass="math display"><em>C</em><sub><em>i</em></sub></span>，我们找到出现最多的真实类别，把这部分样本视为“正确分类”。所有簇的最大匹配数相加，再除以总样本数<span class="math display"><em>n</em></span>，就是 Purity。</p><h4 id="举例说明">举例说明</h4><p>假设有 10 个样本，聚类为 3 个簇，真实类别也有 3 类：</p><table><thead><tr class="header"><th>簇编号</th><th>类 A 数</th><th>类 B 数</th><th>类 C 数</th><th>总数</th></tr></thead><tbody><tr class="odd"><td>C1</td><td>3</td><td>2</td><td>0</td><td>5</td></tr><tr class="even"><td>C2</td><td>0</td><td>0</td><td>3</td><td>3</td></tr><tr class="odd"><td>C3</td><td>0</td><td>1</td><td>1</td><td>2</td></tr></tbody></table><p>计算每个簇中“最多的类别”：</p><ul><li>C1 中最多是类 A（3 个）</li><li>C2 中最多是类 C（3 个）</li><li>C3 中最多的是类 B 或类 C（都为 1 个，任选）</li></ul><p>所以：</p><p><span class="math display">$$\text{Purity} = \frac{3 + 3 + 1}{10} = \frac{7}{10} = 0.7$$</span></p><h4 id="特点总结">特点总结</h4><ul><li><strong>取值范围</strong>：<spanclass="math display">[0,1]</span>，越高表示聚类越纯<br /></li><li><strong>优点</strong>：简单、直观，适合快速评估<br /></li><li><strong>缺点</strong>：对簇的数量敏感，簇数越多，Purity越容易升高（比如每个点单独成簇则 Purity = 1）</li></ul><hr /><h3id="条件熵conditional-entropyinformation-theory-based-methods">条件熵（ConditionalEntropy）：Information Theory-Based Methods</h3><p>条件熵（ConditionalEntropy）是信息论中的一个重要概念，衡量的是在已知一个随机变量（如聚类结果）情况下，另一个变量（如真实标签）的不确定性。</p><h4 id="数学定义">数学定义</h4><p>设：</p><ul><li><span class="math display"><em>C</em></span>：聚类结果，共有 <spanclass="math display"><em>k</em></span> 个簇：<spanclass="math display"><em>C</em><sub>1</sub>, <em>C</em><sub>2</sub>, …, <em>C</em><sub><em>k</em></sub></span><br /></li><li><span class="math display"><em>L</em></span>：真实类别，共有 <spanclass="math display"><em>r</em></span> 个类：<spanclass="math display"><em>L</em><sub>1</sub>, <em>L</em><sub>2</sub>, …, <em>L</em><sub><em>r</em></sub></span></li></ul><p>条件熵定义为：</p><p><span class="math display">$$H(L|C) = - \sum_{i=1}^{k} P(C_i) \sum_{j=1}^{r} P(L_j|C_i) \logP(L_j|C_i)$$</span></p><p>其中：</p><ul><li><spanclass="math display"><em>P</em>(<em>C</em><sub><em>i</em></sub>)</span>：样本落在聚类簇<span class="math display"><em>C</em><sub><em>i</em></sub></span>的概率<br /></li><li><spanclass="math display"><em>P</em>(<em>L</em><sub><em>j</em></sub>|<em>C</em><sub><em>i</em></sub>)</span>：在<span class="math display"><em>C</em><sub><em>i</em></sub></span>中属于真实类别 <spanclass="math display"><em>L</em><sub><em>j</em></sub></span>的条件概率</li></ul><h4 id="直观理解">直观理解</h4><ul><li>如果每个簇只包含一种真实标签，则 <spanclass="math display"><em>P</em>(<em>L</em><sub><em>j</em></sub>|<em>C</em><sub><em>i</em></sub>) = 1</span>，熵为0，表示完全确定。</li><li>如果每个簇中标签很混乱（多类平均分布），则 <spanclass="math display"><em>P</em>(<em>L</em><sub><em>j</em></sub>|<em>C</em><sub><em>i</em></sub>)</span>分布接近均匀，熵值更大。</li></ul><p>因此：</p><blockquote><p>条件熵越小，聚类结果与真实标签越一致。</p></blockquote><figure><img src="l2.png" alt="纯度与交叉熵计算示例" /><figcaption aria-hidden="true">纯度与交叉熵计算示例</figcaption></figure><h3id="归一化互信息nmi-information-theory-based-methods">归一化互信息(NMI):Information Theory-Based Methods</h3><p>ormalized MutualInformation（归一化互信息）是一种衡量聚类结果与真实标签之间“共享信息量”的指标。它对Mutual Information 进行了标准化，使得结果更具可比性。</p><h4 id="mutual-information-mi">Mutual Information (MI)</h4><p>互信息用于衡量聚类结果 <span class="math display"><em>C</em></span>和真实分类 <span class="math display"><em>G</em></span>之间的关联程度：</p><p><span class="math display">$$I(C, G) = \sum_{i=1}^{r} \sum_{j=1}^{k} p_{ij} \log \left(\frac{p_{ij}}{p_{C_i} \cdot p_{G_j}} \right)$$</span></p><p>其中：</p><ul><li><spanclass="math display"><em>p</em><sub><em>i</em><em>j</em></sub></span>表示同时属于簇 <spanclass="math display"><em>C</em><sub><em>i</em></sub></span> 和真实类<span class="math display"><em>G</em><sub><em>j</em></sub></span>的样本比例；</li><li><spanclass="math display"><em>p</em><sub><em>C</em><sub><em>i</em></sub></sub></span>是属于聚类簇 <spanclass="math display"><em>C</em><sub><em>i</em></sub></span>的样本比例；</li><li><spanclass="math display"><em>p</em><sub><em>G</em><sub><em>j</em></sub></sub></span>是属于真实类别 <spanclass="math display"><em>G</em><sub><em>j</em></sub></span>的样本比例。</li></ul><p>📌 说明：</p><ul><li>如果聚类和真实标签完全无关（独立），则 <spanclass="math display"><em>p</em><sub><em>i</em><em>j</em></sub> = <em>p</em><sub><em>C</em><sub><em>i</em></sub></sub> ⋅ <em>p</em><sub><em>G</em><sub><em>j</em></sub></sub></span>，从而<spanclass="math display"><em>I</em>(<em>C</em>,<em>G</em>) = 0</span>；</li><li>互信息没有上限，不适合直接比较不同数据集结果，因此需要归一化。</li></ul><h4 id="normalized-mutual-information-nmi">Normalized Mutual Information(NMI)</h4><p>为了解决 MI 没有上界的问题，引入归一化互信息：</p><p><span class="math display">$$\text{NMI}(C, G) = \frac{2 \cdot I(C, G)}{H(C) + H(G)}$$</span></p><p>或：</p><p><span class="math display">$$\text{NMI}(C, G) = \frac{I(C, G)}{\sqrt{H(C) \cdot H(G)}}$$</span></p><p>其中：</p><ul><li><span class="math display"><em>H</em>(<em>C</em>)</span>为聚类结果的熵；</li><li><span class="math display"><em>H</em>(<em>G</em>)</span>为真实标签的熵。</li></ul><p>熵的定义为：</p><p><span class="math display">$$H(C) = - \sum_{i=1}^{r} p_{C_i} \log p_{C_i} \quad , \quad H(G) = -\sum_{j=1}^{k} p_{G_j} \log p_{G_j}$$</span></p><h4 id="nmi-特性">NMI 特性</h4><ul><li><strong>取值范围：</strong> <spanclass="math display">[0,1]</span></li><li><strong>NMI 越接近 1：</strong> 表示聚类结果越接近真实标签。</li><li><strong>NMI = 0：</strong> 表示聚类与标签完全独立。</li><li>可用于不同聚类算法或数据集之间的结果对比。</li></ul><h3 id="基于成对对比">基于成对对比</h3><p>假设：</p><ul><li>TP：同类同簇的对象对数量（True Positive）（预测同类，真实同类）</li><li>FP：不同类同簇的对象对数量（FalsePositive）（预测同类，真实不同）<br /></li><li>FN：同类不同簇的对象对数量（FalseNegative）（预测不同，真实同类）</li><li>TN：不同类不同簇的对象对数量（TrueNegative）（预测不同，真实不同）</li><li>总对数：所有对象对数量 = <spanclass="math display">$$\frac{n(n-1)}{2}$$</span></li></ul><hr /><h4 id="jaccard系数">Jaccard系数</h4><p><span class="math display">$$\text{Jaccard} = \frac{TP}{TP + FP + FN}$$</span></p><p>只考虑正例（同簇）相关的三种情况，忽略TN。</p><h4 id="rand指数">Rand指数</h4><p><span class="math display">$$\text{Rand Index} = \frac{TP + TN}{TP + FP + FN + TN}$$</span></p><p>计算所有正确对的比例（同簇同类 + 不同簇不同类）。</p><h4 id="fowlkes-mallows指数fm">Fowlkes-Mallows指数（FM）</h4><p>定义为：</p><p><span class="math display">$$\text{FM} = \sqrt{ \frac{TP}{TP + FP} \times \frac{TP}{TP + FN} }$$</span></p><p>即 <strong>precision</strong> 和 <strong>recall</strong>的调和平均。</p><h2 id="聚类质量内在评估方法">聚类质量内在评估方法</h2><p>没有groundtrue,直接评价同一簇的样本尽可能的相似，不同簇的样本尽可能不同的水平</p><h3 id="轮廓系数-silhouette-coefficient">轮廓系数 (SilhouetteCoefficient)</h3><p>对每个样本点计算：</p><ul><li><span class="math inline"><em>a</em>(<em>i</em>)</span>：样本 <spanclass="math inline"><em>i</em></span>与同簇内其它点的平均距离（簇内距离）</li><li><span class="math inline"><em>b</em>(<em>i</em>)</span>：样本 <spanclass="math inline"><em>i</em></span>与最近的其他簇中所有点的平均距离（簇间距离）</li></ul><p>轮廓系数定义为：</p><p><span class="math display">$$s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}$$</span></p><ul><li><span class="math inline"><em>s</em>(<em>i</em>)</span> 范围在 <spanclass="math inline">[−1,1]</span></li><li>越接近 1 表示该点聚类效果越好</li><li>聚类整体轮廓系数是所有点的平均值</li></ul><hr /><h3 id="db指数-davies-bouldin-index-dbi">DB指数 (Davies-Bouldin Index,DBI)</h3><p>计算簇间的相似度，定义为：</p><p><span class="math display">$$DBI = \frac{1}{k} \sum_{i=1}^k \max_{j \neq i} \left( \frac{S_i +S_j}{M_{ij}} \right)$$</span></p><ul><li><span class="math inline"><em>S</em><sub><em>i</em></sub></span>：簇<span class="math inline"><em>i</em></span>内所有点到簇中心的平均距离（簇内散度）</li><li><spanclass="math inline"><em>M</em><sub><em>i</em><em>j</em></sub></span>：簇<span class="math inline"><em>i</em></span> 和簇 <spanclass="math inline"><em>j</em></span> 之间质心的距离（簇间距离）</li><li>DBI 越小表示聚类效果越好</li></ul><hr /><h3 id="dunn指数-dunn-index-di">Dunn指数 (Dunn Index, DI)</h3><p>衡量簇间最小距离与簇内最大直径的比值，定义为：</p><p><span class="math display">$$DI = \frac{\min_{i \neq j} d(C_i, C_j)}{\max_{1 \leq k \leq K}diam(C_k)}$$</span></p><ul><li><spanclass="math inline"><em>d</em>(<em>C</em><sub><em>i</em></sub>,<em>C</em><sub><em>j</em></sub>)</span>：簇<span class="math inline"><em>i</em></span> 和 <spanclass="math inline"><em>j</em></span>之间的距离（通常取质心距离或最小点对距离）</li><li><spanclass="math inline"><em>d</em><em>i</em><em>a</em><em>m</em>(<em>C</em><sub><em>k</em></sub>)</span>：簇<span class="math inline"><em>k</em></span> 内最大距离（簇直径）</li><li>DI 越大表示聚类效果越好</li></ul><hr /><h1 id="离群点检测">离群点检测</h1><h2 id="离群点的基本概念与分类">离群点的基本概念与分类</h2><h3 id="离群点outlier的基本概念">离群点（Outlier）的基本概念</h3><p>离群点是指数据集中明显偏离其他观测值的异常样本点。这些点通常表现为：</p><ul><li>与大多数数据点差异较大</li><li>不符合数据的整体分布模式</li><li>可能是测量错误、数据录入错误，也可能是真实的罕见事件</li></ul><p>离群点的识别对于数据清洗、异常检测和数据分析非常重要。</p><hr /><h3 id="离群点的分类">离群点的分类</h3><ol type="1"><li><p><strong>全局点异常（Point Outlier）</strong></p><ul><li>单个样本点偏离整体数据分布，显著异常。</li><li>例如：一个人的身高为3米（明显偏离正常范围）。</li></ul></li><li><p><strong>上下文异常点（Contextual Outlier 或 ConditionalOutlier）</strong></p><ul><li>数据点在某些上下文环境下表现异常，但在整体数据中未必异常。</li><li>例如：气温30℃在夏天正常，但冬天异常。</li></ul></li><li><p><strong>集体异常点（Collective Outlier）</strong></p><ul><li>一组数据点整体偏离正常行为，即使单个点看起来不异常。</li><li>例如：一段时间内网络异常流量的突然增长。</li></ul></li></ol><h2 id="基于统计模型的概率测试-model-based">基于统计模型的概率测试(Model-based)</h2><p><strong>基本思想</strong><br />统计检测（Statistical-basedMethods）属于模型驱动（Model-based）方法。其核心在于：<br /><strong>构建某种已知的概率分布模型（如正态分布），将显著偏离该分布的点识别为离群点（Outliers）</strong>。</p><hr /><h3 id="常见方法">常见方法</h3><h4 id="单变量正态分布检测z-score">1. 单变量正态分布检测（Z-Score）</h4><p><strong>原理</strong>：</p><ul><li><p>假设变量服从正态分布，计算每个点的 Z 分数：</p><p><span class="math display">$$z = \frac{x - \mu}{\sigma}$$</span></p></li><li><p>设定一个阈值（如 |z| &gt;3），超出该范围的点被认为是离群点。</p></li></ul><p><strong>适用场景</strong>：</p><ul><li>仅适用于<strong>一维数据</strong>。</li><li>要求数据接近<strong>正态分布</strong>。</li></ul><hr /><h4 id="grubbs-检验grubbs-test">2. Grubbs 检验（Grubbs’ Test）</h4><p><strong>原理</strong>：</p><ul><li><p>检测一个数据集中的<strong>最大偏差值是否显著异常</strong>。</p></li><li><p>假设数据服从正态分布，计算最大/最小值与均值的偏差：</p><p><span class="math display">$$G = \frac{ \max |x_i - \bar{x}| }{s}$$</span></p></li><li><p>然后与临界值比较（由 t 分布决定）。</p></li></ul><p><strong>特点</strong>：</p><ul><li>适用于<strong>单个离群值检测</strong>。</li><li>每次只能检测<strong>一个异常点</strong>，需反复执行。</li></ul><hr /><h4 id="马氏距离mahalanobis-distance">3. 马氏距离（MahalanobisDistance）</h4><p><strong>原理</strong>：</p><ul><li><p>多维数据的统计检测方法，考虑协方差之间的相关性。</p></li><li><p>对于样本点 <spanclass="math display"><em>x</em></span>，其马氏距离定义为：</p><p><span class="math display">$$D_M(x) = \sqrt{ (x - \mu)^T \Sigma^{-1} (x - \mu) }$$</span></p></li><li><p>根据卡方分布或者分位数法确定阈值，马氏距离大于阈值的点即为异常点。</p></li></ul><p><strong>优点</strong>：</p><ul><li>考虑了变量间的<strong>相关性</strong>。</li><li>适合<strong>多维正态分布</strong>数据。</li></ul><hr /><h4 id="参数混合模型parametric-mixture-models">4.参数混合模型（Parametric Mixture Models）</h4><p><strong>原理</strong>：</p><ul><li><p>假设数据由多个分布混合而成（如双高斯分布：主群 +异常群）。</p></li><li><p>使用 EM 算法估计每个点属于哪一类的概率：</p><ul><li>若一个点对“正常类”的后验概率非常低，则判为异常。</li></ul></li></ul><p><strong>适用场景</strong>：</p><ul><li>异常点服从与主数据不同的分布。</li></ul><hr /><h4 id="非参数方法">5. 非参数方法</h4><ul><li><p><strong>直方图法</strong>：将数据分箱，箱中频率低的区间被视为异常区域。</p></li><li><p><strong>核密度估计（KDE）</strong>：</p><ul><li>估计每个点的概率密度。</li><li>若某点处的密度估计极低，则为离群点。</li></ul></li></ul><p><strong>优点</strong>：</p><ul><li>无需分布假设。</li><li>适合非正态、非结构化分布。</li></ul><hr /><h3 id="优缺点总结">优缺点总结</h3><table><thead><tr class="header"><th>优点</th><th>缺点</th></tr></thead><tbody><tr class="odd"><td>✅ 理论基础扎实</td><td>❌ 对分布假设敏感（如必须服从正态分布）</td></tr><tr class="even"><td>✅ 可提供显著性水平（如p值）</td><td>❌ 不适用于高维数据（维度灾难）</td></tr><tr class="odd"><td>✅ 易于解释</td><td>❌ 不适合数据分布复杂或有多个模式的情况</td></tr></tbody></table><hr /><h3 id="总结">总结</h3><blockquote><p>统计检测方法适用于数据量不大、维度不高、且分布已知（或可合理假设为正态分布）的场景。如果数据维度高、分布未知或含有复杂结构，建议使用<strong>基于距离、密度或机器学习的方法</strong>进行异常检测。</p></blockquote><h2 id="基于深度的方法model-based">基于深度的方法(Model-based)</h2><h3 id="基本思想-2">基本思想</h3><p>基于深度的离群点检测方法属于 <strong>模型驱动（Model-based）</strong>方法，其核心思想是：</p><blockquote><p><strong>离群点通常远离“数据中心”，处在数据分布的边缘。</strong></p></blockquote><p>这些方法通过计算样本点的“<strong>深度</strong>”（depth）来判断其是否为离群点，深度越小，越靠近边界，越可能是离群点。</p><hr /><h3 id="方法原理">方法原理</h3><ol type="1"><li><p><strong>构建数据层次结构（如多层凸包 Convex Hull）</strong></p><ul><li>将数据包裹在一个凸壳中，移除最外层样本点。</li><li>剩下的点构成“内层”。</li><li>重复该过程，形成类似洋葱结构的多层数据分布。</li></ul></li><li><p><strong>计算数据深度（Data Depth）</strong></p><ul><li>每个点所在的“层数”即为其深度（Depth）。</li><li>深度越小表示越外层，可能为异常点。</li></ul></li><li><p><strong>根据设定的深度阈值检测离群点</strong></p><ul><li>通常，设定一个最小深度阈值，如 <code>depth ≤ 1</code>。</li></ul></li></ol><hr /><h3 id="典型方法">典型方法</h3><ul><li><p><strong>ISODEPTH（Isotonic Depth）</strong><br />基于等深线估计数据密度，深度等高线越远的样本越异常。</p></li><li><p><strong>FDC（Fast Depth Contours）</strong><br />通过快速构造深度轮廓线（contours）以识别边界样本点。</p></li></ul><hr /><h3 id="优点与缺点">优点与缺点</h3><table><thead><tr class="header"><th>优点</th><th>缺点</th></tr></thead><tbody><tr class="odd"><td>✅ 无需对数据分布进行假设</td><td>❌ 主要适用于 2D 或 3D 空间，扩展性差</td></tr><tr class="even"><td>✅ 对离群点定位自然直观</td><td>❌ 计算复杂度高，特别是高维数据</td></tr><tr class="odd"><td>✅ 可以可视化解释（特别是低维）</td><td>❌ 不适合大规模或高维数据场景</td></tr></tbody></table><h2 id="基于偏差的方法model-based">基于偏差的方法(Model-based)</h2><h3 id="基本思想-3">基本思想</h3><p>该方法属于 <strong>Model-based（模型驱动）</strong>异常检测方法，核心思想为：</p><blockquote><p><strong>寻找能最大程度降低整体数据集“偏差”或“方差”的子集，认为这些样本是异常点。</strong></p></blockquote><ul><li>正常数据应具有某种“紧凑”结构（如低方差）</li><li>异常点会“拉大”整体分布</li><li>删除某些点后，如果整体变得更“紧凑”，那么这些被删除的点很可能是离群点</li></ul><hr /><h3 id="典型方法sfi-模型arning-等">典型方法：SF(I) 模型（Arning等）</h3><ul><li><p><strong>提出者</strong>：Arning, Agrawal 和 Raghavan(1996)</p></li><li><p><strong>SF(I) 指标定义</strong>：</p><p>设数据集为 <spanclass="math display"><em>D</em></span>，我们希望找出一个子集 <spanclass="math display"><em>I</em> ⊆ <em>D</em></span>，其移除后会带来<strong>最大的信息压缩或方差降低</strong>。</p><ul><li><p>定义一个<strong>目标函数</strong>：</p><p><spanclass="math display"><em>S</em><em>F</em>(<em>I</em>) = Var(<em>D</em>) − Var(<em>D</em>−<em>I</em>)</span></p></li><li><p>其中 Var(·) 表示数据集的方差或某种误差度量</p></li><li><p><spanclass="math display"><em>S</em><em>F</em>(<em>I</em>)</span> 越大，说明<span class="math display"><em>I</em></span>中的点是影响整体结构的“异常值”可能性越高</p></li></ul></li></ul><hr /><h3 id="优点-4">优点</h3><ul><li>✅<strong>概念直观</strong>：只需比较方差的变化即可判断是否为离群点</li><li>✅<strong>适用于任意类型数据</strong>（数值型、类别型、混合型）</li></ul><hr /><h3 id="缺点-3">缺点</h3><ul><li>❌ <strong>计算复杂度极高</strong>：<ul><li>在最坏情况下，需要检查所有可能的子集 <spanclass="math display"><em>I</em></span>，即时间复杂度为 <spanclass="math display"><em>O</em>(2<sup><em>n</em></sup>)</span></li><li>因此，需要借助<strong>启发式算法</strong>进行优化（如贪心、剪枝、局部搜索等）</li></ul></li></ul><h2id="基于距离的方法proximity-based">基于距离的方法（Proximity-based）</h2><h3 id="基本思想-4">基本思想</h3><p>基于距离的异常检测方法属于<strong>Proximity-based（基于邻近度）</strong> 方法，核心观点是：</p><ul><li>异常点距离其邻居较远，邻域较为空旷<br /></li><li>正常点的邻域内样本密集，距离较近</li></ul><p>通过计算每个点与其周围点的距离，判断是否离群。</p><hr /><h3 id="核心模型与指标">核心模型与指标</h3><ul><li><p><strong>DB(ε, π)模型</strong><br />设定一个半径阈值 <span class="math display"><em>ε</em></span>和邻居比例阈值 <span class="math display"><em>π</em></span>。<br />如果一个点在半径 <span class="math display"><em>ε</em></span>内的邻居数量小于 <spanclass="math display"><em>π</em></span>，则该点被判定为异常。</p></li><li><p><strong>kNN距离评分</strong><br />计算点 <span class="math display"><em>p</em></span> 到其第 <spanclass="math display"><em>k</em></span> 近邻的距离 <spanclass="math display"><em>d</em><sub><em>k</em></sub>(<em>p</em>)</span>。<br />距离越大，说明该点越孤立，异常可能性越大。</p></li><li><p><strong>k-近邻图（kNN Graph）入度判断</strong><br />构建一个图，点之间连接到最近的 <spanclass="math display"><em>k</em></span> 个邻居。<br />统计某点被多少其他点作为邻居（入度）。入度低的点通常是离群点。</p></li><li><p><strong>分辨率因子（ROF）</strong></p><ol type="1"><li><p><strong>定义分辨率层次</strong><br />设定一组分辨率阈值（如 <spanclass="math display"><em>ε</em><sub>1</sub> &lt; <em>ε</em><sub>2</sub> &lt; ⋯ &lt; <em>ε</em><sub><em>m</em></sub></span>），用于逐层分析数据结构。</p></li><li><p><strong>分辨率下邻域聚类</strong><br />对于每个分辨率阈值 <spanclass="math display"><em>ε</em><sub><em>i</em></sub></span>：</p><ul><li>根据距离和阈值 <spanclass="math display"><em>ε</em><sub><em>i</em></sub></span>将数据点聚类。<br /></li><li>若两个点距离不超过 <spanclass="math display"><em>ε</em><sub><em>i</em></sub></span>，则它们属于同一簇的邻居。</li></ul></li><li><p><strong>计算聚类大小变化</strong></p><ul><li>在不同分辨率 <spanclass="math display"><em>ε</em><sub><em>i</em></sub></span>下，记录每个数据点所在聚类的大小（即簇中点的数量）。<br /></li><li>观察聚类大小随分辨率变化的趋势。</li></ul></li><li><p><strong>计算累积异常值因子（ROF）</strong></p><ul><li>根据数据点在各分辨率层次聚类大小的变化，计算ROF值。<br /></li><li>ROF值反映数据点在多层分辨率结构中“聚类稳定性”的变化情况。<br /></li><li>变化较大的点（ROF值较低）通常是异常点。</li></ul></li><li><p><strong>异常点排序与选择</strong></p><ul><li>根据ROF值对所有数据点排序。<br /></li><li>选择ROF值最低的前 <span class="math display"><em>k</em></span>个点作为异常值。</li></ul></li></ol></li></ul><hr /><h3 id="代表算法">代表算法</h3><ul><li><p><strong>ORCA</strong><br />通过利用距离剪枝技术快速计算kNN距离，实现高效的异常检测。</p><ul><li><p><strong>特点</strong><br />利用三角不等式和剪枝技术，减少不必要的距离计算。</p></li><li><p><strong>流程</strong></p><ol type="1"><li>计算数据集中点的kNN距离。<br /></li><li>对于每个点，若kNN距离大于阈值，则标记为异常。</li></ol></li><li><p><strong>优势</strong><br />相比暴力计算，效率提升显著，适合大数据。</p></li></ul></li><li><p><strong>RBRP</strong><br />基于空间分区的方法，先划分数据空间，再局部检测异常，降低计算复杂度。</p><ul><li><p><strong>思路</strong><br />先将数据空间划分成若干区域（分区），减少邻居搜索范围。</p></li><li><p><strong>流程</strong></p><ol type="1"><li>对数据做空间划分。<br /></li><li>在每个分区内局部计算kNN距离。<br /></li><li>综合各分区结果，标记异常。</li></ol></li><li><p><strong>优势</strong><br />降低全局搜索复杂度，提升速度。</p></li></ul></li><li><p><strong>基于分区的算法</strong><br />利用数据划分，将复杂问题分解，便于局部检测异常，适合大规模数据。</p><ul><li>将数据空间划分成多个小块，分别检测异常。<br /></li><li>适合高维数据或大规模数据集。<br /></li><li>结合局部异常检测，能发现局部稀疏区域的异常。</li></ul></li></ul><h3 id="优缺点">优缺点</h3><table><thead><tr class="header"><th>优点</th><th>缺点</th></tr></thead><tbody><tr class="odd"><td>灵活，能检测局部和全局异常点</td><td>受“维度灾难”影响，高维数据效果差</td></tr><tr class="even"><td>直观，易理解，适合多种数据类型</td><td>计算复杂度高，距离计算耗时较多</td></tr><tr class="odd"><td>适用于多种数据分布，无需分布假设</td><td>需要合理选择参数，如 <span class="math display"><em>ε</em></span> 和<span class="math display"><em>k</em></span></td></tr></tbody></table><h2id="基于密度的方法proximity-based">基于密度的方法（Proximity-based）</h2><h3 id="基本思想-5">基本思想</h3><p>基于密度的方法通过比较一个点与其邻域的密度来判断是否异常：<br />- 如果某点的密度远低于周围点的密度，则可能是离群点。<br />- 通常基于 <strong>相对密度</strong>，更适用于具有不同密度的区域。</p><hr /><h3 id="代表方法与简要流程">代表方法与简要流程</h3><h4 id="loflocal-outlier-factor">1. LOF（Local Outlier Factor）</h4><p><strong>思想</strong>：与邻居的局部密度相比，若某点显著稀疏则为离群点。</p><p><strong>流程</strong>：</p><ol type="1"><li>设定邻居数 <spanclass="math display"><em>k</em></span>，计算每个点的 <spanclass="math display"><em>k</em></span>-距离邻域。</li><li>计算每个点的 <strong>可达距离（Reachability Distance）</strong>：<spanclass="math display">reach-dist<sub><em>k</em></sub>(<em>p</em>,<em>o</em>) = max {<em>k</em>-dist(<em>o</em>), <em>d</em>(<em>p</em>,<em>o</em>)}</span></li><li>计算 <strong>局部可达密度（LRD）</strong>： <spanclass="math display">$$\text{LRD}_k(p) = \left( \frac{1}{|N_k(p)|} \sum_{o \in N_k(p)}\text{reach-dist}_k(p, o) \right)^{-1}$$</span></li><li>计算 <strong>LOF值</strong>： <span class="math display">$$\text{LOF}_k(p) = \frac{1}{|N_k(p)|} \sum_{o \in N_k(p)}\frac{\text{LRD}_k(o)}{\text{LRD}_k(p)}$$</span></li><li>LOF值越大，越可能是离群点。</li></ol><hr /><h4 id="cofconnectivity-based-outlier-factor">2. COF（Connectivity-basedOutlier Factor）</h4><p><strong>核心思想：</strong></p><p>COF（连通性异常因子）是一种用于离群点检测的密度方法，旨在通过衡量数据点与其邻域之间的“连通性”强弱来判断是否为异常点。</p><ul><li>正常点在局部区域连通性强；</li><li>异常点的连通性较弱，其与邻居之间的路径通常更长；</li><li>COF值越大，表示该点越可能是异常值。</li></ul><hr /><p><strong>基本定义：</strong></p><ul><li><strong>k-最近邻（kNN）</strong>：点 <spanclass="math display"><em>p</em></span> 的最近 <spanclass="math display"><em>k</em></span> 个邻居集合，记作 <spanclass="math display"><em>N</em><sub><em>k</em></sub>(<em>p</em>)</span>。</li><li><strong>SBN-Path（Set-based Nearest Path）</strong>：从点 <spanclass="math display"><em>p</em></span> 到 <spanclass="math display"><em>N</em><sub><em>k</em></sub>(<em>p</em>)</span>中所有点的“连通路径”。</li><li><strong>链式距离（Chaining Distance）</strong>：从点 <spanclass="math display"><em>p</em></span>到每个邻居的连接路径的总距离。</li><li><strong>平均链式距离（ac-dist）</strong>：点 <spanclass="math display"><em>p</em></span> 的 SBN-Path中所有路径长度的平均值。</li></ul><hr /><p><strong>算法流程：</strong></p><ol type="1"><li><p><strong>寻找 k 个最近邻</strong><br />对于数据集中的每个点 <spanclass="math display"><em>p</em></span>，找到其 <spanclass="math display"><em>k</em></span> 个最近邻组成集合 <spanclass="math display"><em>N</em><sub><em>k</em></sub>(<em>p</em>)</span>。</p></li><li><p><strong>构建 SBN-Path 并计算链式距离</strong><br />使用贪心策略，逐步从 <span class="math display"><em>p</em></span>开始连接未访问的邻居，形成一条连通路径，计算该路径的总长度。</p></li><li><p><strong>计算平均链式距离</strong><br />对于每个点 <spanclass="math display"><em>p</em></span>，计算其平均链式距离（ac-dist）：<span class="math display">$$\text{ac-dist}(p) = \frac{1}{|N_k(p)|} \sum_{\text{链上每一步}} d(x_i,x_{i+1})$$</span></p></li><li><p><strong>计算 COF 值</strong><br />计算点 <span class="math display"><em>p</em></span> 的 COF 值： <spanclass="math display">$$\text{COF}(p) = \frac{\text{ac-dist}(p)}{\frac{1}{|N_k(p)|} \sum_{q \inN_k(p)} \text{ac-dist}(q)}$$</span> 如果 <spanclass="math display">COF(<em>p</em>) ≫ 1</span>，说明 <spanclass="math display"><em>p</em></span>的连通性显著弱于邻域点，是潜在异常值。</p></li><li><p><strong>异常值排序与检测</strong><br />将所有点按 COF 值降序排序，取前 <spanclass="math display"><em>n</em></span> 个作为异常点。</p></li></ol><hr /><h4 id="infloinfluenced-outlierness">3. INFLO（InfluencedOutlierness）</h4><p><strong>算法动机：</strong></p><ul><li><strong>LOF 的问题</strong>：在不同密度的簇之间没有明显边界时，LOF可能将“稀疏簇内的正常点”错误地判为离群点。</li><li><strong>INFLO的改进思路</strong>：考虑对称的邻域关系，将“反向邻居”也纳入密度计算，更合理地估计点的局部密度。</li></ul><hr /><p><strong>核心概念：</strong></p><ol type="1"><li>k-近邻 (kNN)<ul><li><spanclass="math display"><em>k</em><em>N</em><em>N</em>(<em>p</em>)</span>：点<span class="math display"><em>p</em></span> 的 <spanclass="math display"><em>k</em></span> 个最近邻集合。</li></ul></li><li>反向 k-近邻 (Reverse kNN)<ul><li><spanclass="math display"><em>R</em><em>k</em><em>N</em><em>N</em>(<em>p</em>)</span>：将<span class="math display"><em>p</em></span>看作最近邻的所有其他点的集合，即 <spanclass="math display"><em>p</em></span> 是这些点的 kNN 成员。</li></ul></li><li>影响空间 (Influence Space)<ul><li><spanclass="math display"><em>k</em><em>I</em><em>S</em>(<em>p</em>) = <em>k</em><em>N</em><em>N</em>(<em>p</em>) ∪ <em>R</em><em>k</em><em>N</em><em>N</em>(<em>p</em>)</span></li><li>表示“与点 <span class="math display"><em>p</em></span>存在双向邻近关系”的点集合。</li></ul></li></ol><hr /><p><strong>密度定义：</strong></p><ul><li><strong>点 <span class="math display"><em>p</em></span>的密度定义</strong>： <span class="math display">$$den(p) = \frac{1}{k\text{-distance}(p)}$$</span><ul><li>即 <span class="math display"><em>p</em></span> 到第 <spanclass="math display"><em>k</em></span> 个最近邻的距离的倒数。</li></ul></li></ul><hr /><p><strong>INFLO 值计算：</strong></p><ol type="1"><li><p>计算影响空间中每个点的密度：<br />对每个 <spanclass="math display"><em>o</em> ∈ <em>k</em><em>I</em><em>S</em>(<em>p</em>)</span>，计算：<br /><span class="math display">$$den(o) = \frac{1}{k\text{-distance}(o)}$$</span></p></li><li><p>计算 INFLO：<br /><span class="math display">$$  INFLO(p) = \frac{1}{|kIS(p)|} \sum_{o \in kIS(p)}\frac{den(o)}{den(p)}  $$</span></p></li></ol><hr /><p><strong>算法流程：</strong></p><ol type="1"><li><strong>计算每个点的 kNN 和 RkNN</strong>，合并得到影响空间 <spanclass="math display"><em>k</em><em>I</em><em>S</em>(<em>p</em>)</span>；</li><li><strong>根据 k-distance 计算每个点的密度 den(p)</strong>；</li><li><strong>对每个点 <span class="math display"><em>p</em></span>计算其影响空间中点的平均密度，并与自身密度做比值</strong>，即计算 INFLO值；</li><li><strong>对所有点根据 INFLO 值排序</strong>，INFLO值越大，离群可能性越高。</li></ol><p><strong>性质与解释：</strong></p><ul><li>若 <spanclass="math display"><em>I</em><em>N</em><em>F</em><em>L</em><em>O</em>(<em>p</em>) ≈ 1</span>：<spanclass="math display"><em>p</em></span> 处于某个密度一致的簇中；</li><li>若 <spanclass="math display"><em>I</em><em>N</em><em>F</em><em>L</em><em>O</em>(<em>p</em>) ≫ 1</span>：<spanclass="math display"><em>p</em></span>密度显著低于影响空间中的其他点，可能是离群点。</li></ul><p><strong>优势</strong>：</p><ul><li>可处理非均匀密度分布的数据</li><li>使密度评估更加对称、全面</li></ul><hr /><h4 id="locilocal-correlation-integral">4. LOCI（Local CorrelationIntegral）</h4><p><strong>基本思想：</strong></p><ul><li>与 LOF 类似，LOCI判断一个点是否为异常值，依据是：该点的局部密度是否显著低于其邻域的平均密度。</li><li><strong>区别在于：</strong><ul><li>LOF 使用 kNN，LOCI 使用 <strong>ε-邻域</strong></li><li>LOF 固定尺度，LOCI 在 <strong>多个分辨率（granularities）</strong>下做检测</li></ul></li><li>目的是提高算法稳定性，并避免手动设定参数 k 或 ε。</li></ul><hr /><p><strong>核心定义：</strong></p><ol type="1"><li>ε-邻域<ul><li><spanclass="math display"><em>N</em>(<em>p</em>,<em>ε</em>) = {<em>q</em> ∣ dist(<em>p</em>,<em>q</em>) ≤ <em>ε</em>}</span></li><li>点 <span class="math display"><em>p</em></span> 的ε-邻域中包含所有与 <span class="math display"><em>p</em></span>距离不超过 <span class="math display"><em>ε</em></span> 的点。</li></ul></li><li>α·ε-邻域<ul><li><spanclass="math display"><em>N</em>(<em>q</em>,<em>α</em>⋅<em>ε</em>)</span>：点<span class="math display"><em>q</em></span> 的缩放邻域，<spanclass="math display">0 &lt; <em>α</em> ≤ 1</span>，常取 <spanclass="math display"><em>α</em> = 0.5</span></li></ul></li></ol><hr /><p><strong>局部密度计算：</strong></p><ol type="1"><li><p>点 p 的局部密度： <span class="math display">$$  \text{den}(p, \varepsilon, \alpha) = \frac{1}{|N(p, \varepsilon)|}\sum_{q \in N(p, \varepsilon)} |N(q, \alpha \cdot \varepsilon)|  $$</span></p><ul><li>含义：点 <span class="math display"><em>p</em></span>的密度由其邻域中各点自身局部邻域大小的平均值决定。</li></ul></li></ol><hr /><p><strong>异常度量：MDEF（Multi-granularity DeviationFactor）</strong></p><ul><li><p>定义： <span class="math display">$$\text{MDEF}(p, \varepsilon, \alpha) = 1 - \frac{|N(p, \alpha \cdot\varepsilon)|}{\text{den}(p, \varepsilon, \alpha)}$$</span></p></li><li><p>直观理解：</p><ul><li>如果 <span class="math display"><em>p</em></span>的邻域密度远低于其邻域平均密度，MDEF 值较大 ⇒ 更可能是异常点。</li><li>如果 <span class="math display"><em>p</em></span> 密度与邻域一致，则<span class="math display">MDEF ≈ 0</span> ⇒ 正常点。</li></ul></li></ul><hr /><p><strong>标准差：σ_MDEF</strong></p><ul><li><p><spanclass="math display"><em>σ</em><sub>MDEF</sub>(<em>p</em>,<em>ε</em>,<em>α</em>)</span>表示局部密度的标准差，用于评估 MDEF 的显著性。</p></li><li><p>异常点判定规则：</p><ul><li>若 <spanclass="math display">MDEF(<em>p</em>,<em>ε</em>,<em>α</em>) &gt; 3 ⋅ <em>σ</em><sub>MDEF</sub></span>，则判为异常点（基于3σ 原则）。</li></ul></li></ul><hr /><p><strong>算法流程：</strong></p><ol type="1"><li>遍历所有点<ul><li>对数据集中每个点 <span class="math display"><em>p</em></span>：</li></ul></li><li>遍历多尺度（多个 ε 值）<ul><li>设定多个 <span class="math display"><em>ε</em></span>分辨率（从小到大），逐个测试：</li></ul></li><li>计算 ε-邻域和 α·ε-邻域大小<ul><li><span class="math display"><em>N</em>(<em>p</em>,<em>ε</em>)</span>和 <spanclass="math display"><em>N</em>(<em>q</em>,<em>α</em>⋅<em>ε</em>)</span></li></ul></li><li>计算密度、MDEF、σ_MDEF<ul><li>得到 <spanclass="math display">den(<em>p</em>,<em>ε</em>,<em>α</em>)</span></li><li>得到 <spanclass="math display">MDEF(<em>p</em>,<em>ε</em>,<em>α</em>)</span></li><li>得到 <spanclass="math display"><em>σ</em><sub>MDEF</sub>(<em>p</em>,<em>ε</em>,<em>α</em>)</span></li></ul></li><li>判断异常点<ul><li>若 <spanclass="math display">MDEF(<em>p</em>,<em>ε</em>,<em>α</em>) &gt; 3 ⋅ <em>σ</em><sub>MDEF</sub></span>，则在该分辨率下，<spanclass="math display"><em>p</em></span> 是异常点。</li></ul></li></ol><hr /><p><strong>LOCI Plot 可视化:</strong></p><ul><li>对某个点 <spanclass="math display"><em>p</em></span>，可画出多尺度下的 LOCI曲线，显示：<ul><li><spanclass="math display">|<em>N</em>(<em>p</em>,<em>α</em>⋅<em>ε</em>)|</span></li><li><spanclass="math display">den(<em>p</em>,<em>ε</em>,<em>α</em>) ± 3<em>σ</em></span>上下界</li></ul></li><li>可用图表找出在哪些尺度下 <spanclass="math display"><em>p</em></span> 被判定为异常点。</li></ul><hr /><p><strong>输出结果:</strong></p><ul><li><strong>异常评分</strong>：MDEF 值，可排序。</li><li><strong>异常标签</strong>：是否满足 <spanclass="math display">MDEF &gt; 3<em>σ</em></span> 的条件。</li><li><strong>可解释图表</strong>：LOCI Plot 可展示异常发生的尺度</li></ul><p><strong>特点</strong>：</p><ul><li>不依赖具体参数（如 <spanclass="math display"><em>k</em></span>）</li><li>提供异常的置信度范围（MDEF）</li></ul><h3 id="特点总结-1">特点总结</h3><ul><li>✅ 更适合处理不同密度区域<br /></li><li>✅ 输出连续的异常评分，可排序选择异常点<br /></li><li>❗ 参数敏感，如 <span class="math display"><em>k</em></span> 或<span class="math display"><em>ε</em></span> 需合理设置<br /></li><li>❗ 计算复杂度通常较高（如 <spanclass="math display"><em>O</em>(<em>n</em><sup>2</sup>)</span>）</li></ul><hr /><h2 id="基于重构的方法其他类型">基于重构的方法(其他类型)</h2><h3 id="基本思想-6">基本思想</h3><p>假设数据可由一个简洁的模型有效表示或重构。<br />若某个数据点无法被模型很好重构（即重构误差高），则可能是异常点。</p><hr /><h3 id="常见技术">常见技术</h3><h4 id="矩阵分解matrix-factorization">1. 矩阵分解（MatrixFactorization）</h4><ul><li><strong>代表方法：</strong>奇异值分解（SVD）或主成分分析（PCA）<br /></li><li><strong>原理：</strong>将原始高维数据映射到一个低维空间并重构，如果某个样本的重构误差较大，则可能是异常。</li><li><strong>异常度计算：</strong> <spanclass="math display">error(<em>x</em>) = ∥<em>x</em> − <em>x̂</em>∥<sub>2</sub></span>其中 <span class="math display"><em>x̂</em></span>是降维后的重构结果。</li></ul><h4 id="码表压缩法codebookcompression-based">2.码表压缩法（Codebook/Compression-based）</h4><ul><li><strong>代表思想：</strong>通过对数据压缩（如MDL理论），来衡量异常性。</li><li><strong>核心假设：</strong>正常数据可以被压缩得很好，而异常点不容易压缩，需要更长的编码长度。</li><li><strong>典型方法：</strong><ul><li>Lempel-Ziv 编码长度分析<br /></li><li>BZIP2/GZIP等压缩算法结合</li></ul></li><li><strong>异常度：</strong> 用编码长度 <spanclass="math display"><em>L</em>(<em>x</em>)</span>表示，编码长度越长越异常。</li></ul><h4 id="自编码器autoencoder">3. 自编码器（Autoencoder）</h4><ul><li><strong>结构：</strong>包括编码器（Encoder）和解码器（Decoder），构建一个神经网络模型学习 <spanclass="math display"><em>x</em> → <em>x̂</em></span>。</li><li><strong>核心思想：</strong>正常样本的重构误差较低，异常样本无法通过训练好的网络精确还原。</li><li><strong>重构误差：</strong> <spanclass="math display">Loss(<em>x</em>) = ∥<em>x</em> − <em>x̂</em>∥<sup>2</sup>  或  BCE(<em>x</em>,<em>x̂</em>)</span></li><li><strong>改进变种：</strong><ul><li>稀疏自编码器（Sparse AE）<br /></li><li>卷积自编码器（CAE）<br /></li><li>变分自编码器（VAE）</li></ul></li></ul><hr /><h3 id="优点-5">优点</h3><ul><li>可捕捉数据的复杂非线性结构（特别是深度学习方法）</li><li>适合连续属性和高维数据场景</li></ul><h3 id="缺点-4">缺点</h3><ul><li>训练成本较高（尤其是深度网络）</li><li>类别型数据支持较差</li><li>可能需要较多超参数调整</li></ul><hr /><h2 id="高维方法high-dimensional-approaches">高维方法（High-dimensionalApproaches）</h2><h3 id="问题背景">问题背景</h3><p>在高维空间中，传统的距离或密度度量变得不可靠，容易出现“维数灾难”问题：</p><ul><li>距离集中效应（所有点间距离趋于相同）</li><li>密度稀疏，难以定义邻域或聚类结构</li></ul><hr /><h3 id="常见解决方法">常见解决方法</h3><h4 id="降维-异常检测">1. 降维 + 异常检测</h4><ul><li><strong>核心思路：</strong>先将高维数据压缩到低维，再在低维空间中进行异常检测。</li><li><strong>常用模型：</strong><ul><li>自编码器（Autoencoder）</li><li>OC-NN（One-Class Neural Network）</li><li>DevNet（Deep Semi-Supervised Anomaly Detection）</li></ul></li><li><strong>流程：</strong><ol type="1"><li>用神经网络学习高维数据的低维嵌入表示。</li><li>在嵌入空间中使用距离、密度或边界方法识别异常点。</li></ol></li></ul><h4 id="abodangle-based-outlier-detection">2. ABOD（Angle-Based OutlierDetection）</h4><ul><li><strong>核心思想：</strong>异常点与其它点形成的角度分布差异小。</li><li><strong>解释：</strong><ul><li>正常点通常被“包围”，与其它点的角度变化小。</li><li>异常点在边缘，与其它点形成的向量夹角分布变化大。</li></ul></li><li><strong>计算方法：</strong> 对每个点 <spanclass="math display"><em>p</em></span>，计算其与其它点对 <spanclass="math display">(<em>o</em><sub><em>i</em></sub>,<em>o</em><sub><em>j</em></sub>)</span>构成的角度的方差： <spanclass="math display"><em>A</em><em>B</em><em>O</em><em>D</em>(<em>p</em>) = Variance(cos∠(<em>p</em>,<em>o</em><sub><em>i</em></sub>,<em>o</em><sub><em>j</em></sub>))</span><ul><li>方差越大 → 越像正常点<br /></li><li>方差越小 → 越可能是离群点</li></ul></li></ul><hr /><h3 id="优点-6">优点</h3><ul><li>适合高维数据（尤其是深度学习方法）</li><li>能挖掘复杂的非线性结构</li></ul><h3 id="缺点-5">缺点</h3><ul><li>算法复杂度高，训练和推理时间较长</li><li>可解释性较差（尤其是深度模型）</li></ul><h1 id="不会的都不考">不会的都不考！</h1>]]></content>
    
    
    <categories>
      
      <category>计算机</category>
      
      <category>数据挖掘</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机</tag>
      
      <tag>数据挖掘</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>线性代数：矩阵与线性方程组</title>
    <link href="/BLOG/2025/05/25/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%EF%BC%9A%E7%9F%A9%E9%98%B5%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/"/>
    <url>/BLOG/2025/05/25/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%EF%BC%9A%E7%9F%A9%E9%98%B5%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<blockquote><p>从这个一个博客开始，我们将进行线性代数的复习。由于是用于夏令营以及与推免考核所用，我们一些简单的定义和定理将直接带过</p></blockquote><h1 id="线性代数矩阵与线性方程组">线性代数：矩阵与线性方程组</h1><p>我们从矩阵开始复习，同时带到线性相关、子空间等概念。</p><p>我们先介绍最简单的概念，线性相关与线性无关</p><h2 id="线性相关与线性无关">线性相关与线性无关</h2><p>我们定义列向量 <spanclass="math inline">{<em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, <em>a</em><sub>3</sub>, …，<em>a</em><sub><em>n</em></sub>}</span></p><ul><li><p><strong>线性相关(Linear Dependent)</strong>：若是存在不全为<strong>0</strong> 的向量 <spanclass="math inline"><em>X</em> = {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, <em>x</em><sub>3</sub>, …, <em>x</em><sub><em>n</em></sub>}</span>满足 <spanclass="math inline"><em>x</em><sub>1</sub><em>a</em><sub>1</sub> + <em>x</em><sub>2</sub><em>a</em><sub>2</sub> + <em>x</em><sub>3</sub><em>a</em><sub>3</sub> + … + <em>x</em><sub><em>n</em></sub><em>a</em><sub><em>n</em></sub> = 0</span>，则称列向量<spanclass="math inline">{<em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, <em>a</em><sub>3</sub>, …，<em>a</em><sub><em>n</em></sub>}</span>线性相关</p></li><li><p><strong>线性无关(Linear Independent)</strong>：与上述相反，如果不存在不全为 <strong>0</strong> 的向量 <spanclass="math inline"><em>X</em> = {<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, <em>x</em><sub>3</sub>, …, <em>x</em><sub><em>n</em></sub>}</span>满足条件，则称之为线性无关</p></li></ul><p>随后我们简单回顾矩阵，矩阵的定义从向量导入，如果矩阵有m行和n列，我们就说矩阵的大小为<spanclass="math inline"><em>m</em> * <em>n</em></span>，如果<spanclass="math inline"><em>m</em> = <em>n</em></span>，我们称为方阵（<strong>squarematrix</strong>）</p><h2 id="矩阵的计算">矩阵的计算</h2><p>这里我们简单介绍矩阵的而一些计算与属性</p><ul><li><p>矩阵和标量相乘，即每个元素和标量相乘</p></li><li><p>矩阵（向量）和矩阵（向量）相乘， 即按照顺序依次相乘</p><ul><li>若<spanclass="math inline"><em>A</em><em>B</em> = <em>B</em><em>A</em></span>，则称A与B乘法可交换。n阶单位矩阵E与任何n阶矩阵乘法可交换</li></ul></li><li><p>矩阵的分块乘法</p></li><li><p>矩阵<strong>转置</strong></p></li><li><p>矩阵<strong>共轭</strong>:即矩阵内每个元素取其共轭的结果</p></li><li><p>矩阵的<strong>秩（Rank)</strong>:矩阵列向量中最大的线性无关的数目（The maximum number of LinearIndependent columns)</p></li><li><p>矩阵的<strong>零化度（Nullity）：</strong>矩阵的列数减去矩阵的秩</p></li></ul><h3 id="矩阵的逆">矩阵的逆</h3><p>如果两个方阵A和B的乘积是单位矩阵，AB=I，那么A和B就是互为逆矩阵</p><p>矩阵的逆有如下定义：</p><ul><li><p><spanclass="math inline">(<em>A</em><sup>−1</sup>)<sup>−1</sup> = <em>A</em></span></p></li><li><p><spanclass="math inline">(<em>k</em><em>A</em>)<sup>−1</sup> = <em>k</em><sup>−1</sup><em>A</em><sup>−1</sup></span></p></li><li><p><spanclass="math inline">(<em>A</em><sup><em>T</em></sup>)<sup>−1</sup> = (<em>A</em><sup>−1</sup>)<sup><em>T</em></sup></span></p></li><li><p><spanclass="math inline">(<em>A</em><em>B</em>)<sup>−1</sup> = <em>B</em><sup>−1</sup><em>A</em><sup>−1</sup></span></p></li></ul><p>矩阵的逆还有如下的性质（需要牢记）：</p><figure><img src="Rank.png" alt="矩阵逆的性质" /><figcaption aria-hidden="true">矩阵逆的性质</figcaption></figure><p>对于一个方阵，其可逆判别可用如下任意一条：</p><figure><img src="invert.jpg" alt="矩阵可逆的判别" /><figcaption aria-hidden="true">矩阵可逆的判别</figcaption></figure><h3 id="矩阵的行列式">矩阵的行列式</h3><p>具体计算我们带过，我们讨论一下行列式的性质</p><ul><li><p>交换任意两行，行列式取负号</p></li><li><p>行列式的标量乘法： <spanclass="math inline"><em>d</em><em>e</em><em>t</em>(2<em>A</em>) = 2<sup><em>n</em></sup><em>d</em><em>e</em><em>t</em>(<em>A</em>)</span></p></li><li><p>矩阵乘法的行列式：</p></li></ul><figure><img src="det.jpg" alt="矩阵乘法的行列式" /><figcaption aria-hidden="true">矩阵乘法的行列式</figcaption></figure><ul><li><p>如果一个方阵的行列式不为0，那么它是可逆的，反之，如果一个方阵可逆，那么它的行列式不为0</p></li><li><p><span class="math inline">$\bulletA^{-1}=\frac{1}{det(A)}C^T$</span> 其中<spanclass="math inline"><em>C</em></span>为伴随矩阵，具体如下</p></li></ul><p><span class="math display">$$C=\begin{bmatrix}c_{11} &amp; \cdots &amp; c_{1n} \\\vdots &amp; \ddots &amp; \vdots \\c_{n1} &amp; \cdots &amp; c_{nn}\end{bmatrix}$$</span></p><p>其中 <spanclass="math inline"><em>c</em><sub><em>i</em><em>j</em></sub> = (−1)<sup><em>i</em> + <em>j</em></sup><em>d</em><em>e</em><em>t</em>(<em>A</em><sub><em>i</em><em>j</em></sub>)</span>为代数余子式</p><h3 id="初等矩阵">初等矩阵</h3><p><strong>初等矩阵</strong>： 对单位矩阵进行一次变换后的矩阵</p><p>相关定理（<strong>左行右列</strong>）：</p><ul><li><p>对A施行初等<strong>行变换</strong>，其结果等于在A左边乘以相应的初等矩阵；</p></li><li><p>对A施行初等<strong>列变换</strong>，其结果等于在A右边乘以相应的初等矩阵</p></li><li><p>n阶方阵可逆的<strong>充要条件</strong>是它能表示成一些初等矩阵的乘积</p></li></ul><h2 id="线性方程组">线性方程组</h2><p>我们借助矩阵带一下线性方程组的相关概念与性质</p><p><strong>我们可以用矩阵表达一个线性方程组，即： <spanclass="math inline"><em>A</em><em>x</em> = <em>b</em></span>,其中A为矩阵，<spanclass="math inline"><em>x</em>, <em>b</em></span>分别为对应维度的列向量</strong></p><h3 id="解的情况与相容性">解的情况与相容性</h3><p>线性方程组解 <span class="math inline"><em>x</em></span>的情况有三种：<strong>无解、有唯一解、有无穷解</strong></p><p>若方程组有解，则称其为 <strong>相容（Consistent）</strong>；若是无解，则称之为 <strong>不相容（Inconsistent）</strong></p><h3 id="线性方程组有解的解释">线性方程组有解的解释</h3><p>首先介绍一个概念</p><ul><li><strong>张成的空间</strong>：对于一个向量集 <spanclass="math inline"><em>S</em></span> , 其向量的所有线性组合组成的向量集<span class="math inline"><em>V</em></span> ，称之为 <spanclass="math inline"><em>S</em></span> 张成的空间，记作 <spanclass="math inline"><em>S</em><em>p</em><em>a</em><em>n</em>(<em>S</em>)</span></li></ul><p>因此若 <spanclass="math inline"><em>A</em><em>x</em> = <em>b</em></span>有解，可以解释为：</p><ul><li><p><span class="math inline"><em>b</em></span> 是可以表示为矩阵<span class="math inline"><em>A</em></span> 的列向量的线性组合</p></li><li><p><span class="math inline"><em>b</em></span> 再矩阵 <spanclass="math inline"><em>A</em></span> 的列向量的所张成的空间里</p></li></ul><h3 id="解的数量与矩阵a的关系">解的数量与矩阵A的关系</h3><ul><li><p>有唯一解：矩阵A的列向量线性无关，即 <spanclass="math inline"><em>R</em><em>a</em><em>n</em><em>k</em>(<em>A</em>) = <em>n</em>(<em>N</em><em>u</em><em>l</em><em>l</em><em>i</em><em>t</em><em>y</em>(<em>A</em>)=0)</span></p></li><li><p>有无穷解：矩阵A的列向量线性相关，即 <spanclass="math inline"><em>R</em><em>a</em><em>n</em><em>k</em>(<em>A</em>) &lt; <em>n</em>(<em>N</em><em>u</em><em>l</em><em>l</em><em>i</em><em>t</em><em>y</em>(<em>A</em>)&gt;0)</span></p></li></ul><h3id="线性方程组求解高斯消元法gaussian-elimination">线性方程组求解：高斯消元法（GaussianElimination）</h3><p>首先有线性方程组等价定理：两个线性方程组等价 <spanclass="math inline">⇔</span> 两个线性方程组有相同的解</p><p><strong>高斯消元法</strong>：将增广矩阵化简为简约型阶梯形式，进而求解的方法</p><ul><li><p><strong>增广矩阵</strong>：将矩阵A与列向量b横向拼接起来，即 <spanclass="math inline">[<em>A</em>|<em>b</em>]</span></p></li><li><p><strong>简约型阶梯形式</strong>：即呈阶梯状，并且每个阶梯第一位为1，具体如下图：</p></li></ul><figure><img src="Echelon.jpg" alt="简约型阶梯形式示例" /><figcaption aria-hidden="true">简约型阶梯形式示例</figcaption></figure><p>若是最后化简得结果存在只有最后一列不为0得情况，则线性方程组无解;反之则有解，若是有一行全部为0，则存在无数解。</p><figure><img src="result.jpg" alt="解的分类" /><figcaption aria-hidden="true">解的分类</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>数学</category>
      
      <category>线性代数</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学</tag>
      
      <tag>线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>博客写入教程</title>
    <link href="/BLOG/2025/05/24/%E5%8D%9A%E5%AE%A2%E5%86%99%E5%85%A5%E6%95%99%E7%A8%8B/"/>
    <url>/BLOG/2025/05/24/%E5%8D%9A%E5%AE%A2%E5%86%99%E5%85%A5%E6%95%99%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="markdown-写作模板含-front-matter-与语法指南">📘 Markdown写作模板（含 Front Matter 与语法指南）</h1><hr /><h2 id="第一部分-front-matter-配置说明">第一部分：📄 Front Matter配置说明</h2><blockquote><p>Front Matter 是 Markdown文件顶部的元信息，用于配置文章标题、标签、分类、日期等内容。通常写在<code>---</code> 包裹的 YAML 格式中。</p></blockquote><h3 id="示例模板">🧾 示例模板</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">post</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">我的第一篇博客</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2025-05-24 10:00:00</span><br><span class="hljs-attr">updated:</span> <span class="hljs-number">2025-05-24 12:00:00</span><br><span class="hljs-attr">comments:</span> <span class="hljs-literal">true</span><br><span class="hljs-attr">tags:</span> [<span class="hljs-string">Markdown</span>, <span class="hljs-string">教程</span>]<br><span class="hljs-attr">categories:</span> [<span class="hljs-string">技术笔记</span>]<br><span class="hljs-attr">permalink:</span> <span class="hljs-string">/markdown-guide.html</span><br><span class="hljs-attr">excerpt:</span> <span class="hljs-string">本文介绍了</span> <span class="hljs-string">Markdown</span> <span class="hljs-string">的语法与博客头部配置。</span><br><span class="hljs-attr">disableNunjucks:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">lang:</span> <span class="hljs-string">zh-CN</span><br><span class="hljs-attr">published:</span> <span class="hljs-literal">true</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><h3 id="字段说明表">📋 字段说明表</h3><table><thead><tr class="header"><th>字段名</th><th>说明</th><th>默认值</th></tr></thead><tbody><tr class="odd"><td><code>layout</code></td><td>页面布局模板（如 <code>post</code>、<code>page</code>）</td><td><code>config.default_layout</code></td></tr><tr class="even"><td><code>title</code></td><td>页面标题</td><td>文件名</td></tr><tr class="odd"><td><code>date</code></td><td>建立日期</td><td>文件创建时间</td></tr><tr class="even"><td><code>updated</code></td><td>最近更新日期</td><td>文件更新时间</td></tr><tr class="odd"><td><code>comments</code></td><td>是否启用评论</td><td><code>true</code></td></tr><tr class="even"><td><code>tags</code></td><td>标签数组，用于分类内容</td><td>空</td></tr><tr class="odd"><td><code>categories</code></td><td>分类数组（适用于归档）</td><td>空</td></tr><tr class="even"><td><code>permalink</code></td><td>自定义永久链接（应以 <code>/</code> 或 <code>.html</code>结尾）</td><td><code>null</code></td></tr><tr class="odd"><td><code>excerpt</code></td><td>页面摘要，建议结合插件处理</td><td>空</td></tr><tr class="even"><td><code>disableNunjucks</code></td><td>禁用 Nunjucks模板语法（<code>&#123;&#123; &#125;&#125;</code>、<code>&#123;% %&#125;</code>）</td><td><code>false</code></td></tr><tr class="odd"><td><code>lang</code></td><td>页面语言，覆盖默认配置</td><td>继承自 <code>_config.yml</code></td></tr><tr class="even"><td><code>published</code></td><td>是否发布该文章</td><td><code>_posts</code> 下为 <code>true</code>，草稿为<code>false</code></td></tr></tbody></table><hr /><h2 id="第二部分-markdown-语法速查手册">第二部分：📝 Markdown语法速查手册</h2><h3 id="标题">1️⃣ 标题</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-section"># 一级标题</span><br><span class="hljs-section">## 二级标题</span><br><span class="hljs-section">### 三级标题</span><br></code></pre></td></tr></table></figure><hr /><h3 id="段落与换行">2️⃣ 段落与换行</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs markdown">这是第一段。<br><br>这是第二段，  <br>这里使用两个空格加换行。<br></code></pre></td></tr></table></figure><hr /><h3 id="文本格式">3️⃣ 文本格式</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-strong">**加粗文本**</span><br><span class="hljs-emphasis">*斜体文本*</span><br>~~删除线文本~~<br><span class="hljs-code">`行内代码`</span><br></code></pre></td></tr></table></figure><hr /><h3 id="引用块">4️⃣ 引用块</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-quote">&gt; 一级引用</span><br>&gt;&gt; 二级引用<br></code></pre></td></tr></table></figure><hr /><h3 id="列表">5️⃣ 列表</h3><ul><li><strong>无序列表</strong></li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">-</span> 项目一<br><span class="hljs-bullet">-</span> 项目二<br></code></pre></td></tr></table></figure><ul><li><strong>有序列表</strong></li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">1.</span> 步骤一<br><span class="hljs-bullet">2.</span> 步骤二<br></code></pre></td></tr></table></figure><hr /><h3 id="代码块">6️⃣ 代码块</h3><ul><li><strong>行内代码</strong></li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">这是 <span class="hljs-code">`inline code`</span><br></code></pre></td></tr></table></figure><ul><li><strong>多行代码块</strong></li></ul><details><summary>Python 示例</summary><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">greet</span>():<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Hello Markdown!&quot;</span>)<br></code></pre></td></tr></table></figure></details><hr /><h3 id="表格">7️⃣ 表格</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs markdown">| 名称    | 类型   | 描述       |<br>|---------|--------|------------|<br>| title   | 字符串 | 文章标题   |<br>| date    | 日期   | 创建时间   |<br></code></pre></td></tr></table></figure><hr /><h3 id="链接与图片">8️⃣ 链接与图片</h3><ul><li><strong>链接</strong></li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">[<span class="hljs-string">OpenAI 官网</span>](<span class="hljs-link">https://www.openai.com</span>)<br></code></pre></td></tr></table></figure><ul><li><strong>图片</strong></li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">![<span class="hljs-string">替代文字</span>](<span class="hljs-link">https://via.placeholder.com/150</span>)<br></code></pre></td></tr></table></figure><hr /><h3 id="分隔线">9️⃣ 分隔线</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">---<br></code></pre></td></tr></table></figure><hr /><h3 id="任务列表github-支持">🔟 任务列表（GitHub 支持）</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-bullet">-</span> [x] 学习 Markdown<br><span class="hljs-bullet">-</span> [ ] 写一篇博客<br></code></pre></td></tr></table></figure><hr /><h3 id="转义字符防止符号被识别">🧩 转义字符（防止符号被识别）</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown">\<span class="hljs-emphasis">*星号\*</span> 不会加粗  <br>\<span class="hljs-code">`反引号\`</span> 不会变成代码  <br></code></pre></td></tr></table></figure><h3 id="公式说明">公式说明</h3><blockquote><p>$$ E = mc^2 $$ 可以实现公式的书写</p><p><spanclass="math display"><em>E</em> = <em>m</em><em>c</em><sup>2</sup></span></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>技术笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Markdown</tag>
      
      <tag>教程</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
