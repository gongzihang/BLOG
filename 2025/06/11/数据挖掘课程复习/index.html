

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <meta name="google-site-verification" content="L88GD0MJqoib8TSIMKbg-ZX9Cb5GL0DvQt_bzdI1u1c" />

  <link rel="apple-touch-icon" sizes="76x76" href="/BLOG/img/fluid.png">
  <link rel="icon" href="/BLOG/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Zihang Gong">
  <meta name="keywords" content="">
  
    <meta name="description" content="用于HITSZ 25春数据挖掘复习  概述 数据挖掘的含义 数据挖掘是从海量数据集中发现有趣的（非平凡的、隐含的、未被发现但有用的）模式、模型或知识的过程。 数据挖掘过程 知识发现的过程： 1. 数据预处理  1. 数据清洗  2. 数据集成  3. 数据选择  4. 数据变换 2. 数据挖掘 3. 模式&#x2F;模型评估 4. 知识表示 数据挖掘的应用  数据分析与">
<meta property="og:type" content="article">
<meta property="og:title" content="数据挖掘课程复习">
<meta property="og:url" content="https://gongzihang.github.io/2025/06/11/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0/index.html">
<meta property="og:site_name" content="Zihang Gong&#39;s Blog">
<meta property="og:description" content="用于HITSZ 25春数据挖掘复习  概述 数据挖掘的含义 数据挖掘是从海量数据集中发现有趣的（非平凡的、隐含的、未被发现但有用的）模式、模型或知识的过程。 数据挖掘过程 知识发现的过程： 1. 数据预处理  1. 数据清洗  2. 数据集成  3. 数据选择  4. 数据变换 2. 数据挖掘 3. 模式&#x2F;模型评估 4. 知识表示 数据挖掘的应用  数据分析与">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gongzihang.github.io/2025/06/11/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0/样本.png">
<meta property="og:image" content="https://gongzihang.github.io/2025/06/11/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0/列联表.png">
<meta property="og:image" content="https://gongzihang.github.io/2025/06/11/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0/解释.png">
<meta property="og:image" content="https://gongzihang.github.io/2025/06/11/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0/例题1.png">
<meta property="og:image" content="https://gongzihang.github.io/2025/06/11/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0/p.png">
<meta property="og:image" content="https://gongzihang.github.io/2025/06/11/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0/可达距离.jpg">
<meta property="article:published_time" content="2025-06-10T16:24:29.000Z">
<meta property="article:modified_time" content="2025-06-12T02:55:02.351Z">
<meta property="article:author" content="Zihang Gong">
<meta property="article:tag" content="计算机">
<meta property="article:tag" content="数据挖掘">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://gongzihang.github.io/2025/06/11/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0/样本.png">
  
  
  
  <title>数据挖掘课程复习 - Zihang Gong&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/BLOG/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/BLOG/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/BLOG/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"gongzihang.github.io","root":"/BLOG/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":"KkofgzAxn6fOOSXjKQhAz5KQ-gzGzoHsz","app_key":"gZuiRbaJsIMdptuIXKhe1KdK","server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/BLOG/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/BLOG/js/utils.js" ></script>
  <script  src="/BLOG/js/color-schema.js" ></script>
  

  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 5.4.2"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/BLOG/">
      <strong>Zihang Gong&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/BLOG/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/BLOG/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/BLOG/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/BLOG/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/BLOG/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/BLOG/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="数据挖掘课程复习"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-06-11 00:24" pubdate>
          2025年6月11日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          9.4k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          79 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">数据挖掘课程复习</h1>
            
            
              <div class="markdown-body">
                
                <blockquote>
<p>用于HITSZ 25春数据挖掘复习</p>
</blockquote>
<h1 id="概述">概述</h1>
<h2 id="数据挖掘的含义">数据挖掘的含义</h2>
<p>数据挖掘是从海量数据集中发现有趣的（<mark>非平凡的、隐含的、未被发现但有用的</mark>）模式、模型或知识的过程。</p>
<h2 id="数据挖掘过程">数据挖掘过程</h2>
<p>知识发现的过程：<br />
1. 数据预处理<br />
 1. 数据清洗<br />
 2. 数据集成<br />
 3. 数据选择<br />
 4. 数据变换<br />
2. 数据挖掘<br />
3. 模式/模型评估<br />
4. 知识表示</p>
<h2 id="数据挖掘的应用">数据挖掘的应用</h2>
<ul>
<li>数据分析与决策支持
<ul>
<li>客户画像、需求分析</li>
<li>交叉市场分析</li>
</ul></li>
<li>风险分析与管理
<ul>
<li>竞争分析</li>
<li>风险评估</li>
<li>资产管理</li>
<li>资源规划</li>
</ul></li>
<li>欺诈检测与异常模式挖掘
<ul>
<li>医疗、电信等</li>
</ul></li>
</ul>
<h2 id="数据仓库">数据仓库</h2>
<p><strong>定义</strong>：数据仓库是<mark>面向主题的、集成的、随时间变化的、又相对稳定的</mark>数据集合，它支持管理层的决策过程<br />
<strong>数据仓库的关键特征</strong>：</p>
<ul>
<li>面向主题的：
<ul>
<li>数据仓库的数据是以分析的主题为中心构建的</li>
</ul></li>
<li>集成的：
<ul>
<li>数据仓库的数据来自不同的数据源，需要按照统一的结构、格式、度量、语义进行数据处理，最后集成到数据仓库的主题</li>
</ul></li>
<li>随时间变化的：
<ul>
<li>数据仓库的历史数据，需要随时间延长而增加（周期性更新）</li>
<li>数据仓库的综合数据，需要随时间延长而变化</li>
</ul></li>
<li>相对稳定的：
<ul>
<li>数据仓库的数据主要用于支持决策，不会涉及频繁的修改与变化，主要是查询与分析。</li>
</ul></li>
</ul>
<hr />
<h1 id="数据特征分析与预处理">数据特征分析与预处理</h1>
<h2 id="数据类型及其特征">数据类型及其特征</h2>
<h2 id="数据对象">数据对象：</h2>
<p>数据集由数据对象构成，数据对象又称之为样本、实例。</p>
<figure>
<img src="样本.png" srcset="/BLOG/img/loading.gif" lazyload alt="数据样本示意图" />
<figcaption aria-hidden="true">数据样本示意图</figcaption>
</figure>
<h2 id="属性类型">属性类型</h2>
<p><strong>定义</strong>：一个数据字段，表示一个数据对象的某个特征</p>
<p><strong>分类</strong>：</p>
<ul>
<li><p><strong>标称属性:</strong> 与名称相关的值、分类或枚举属性
<mark>(定性的)</mark></p>
<ul>
<li><strong>二元属性:</strong>
标称属性的一种特殊情况，只有2个状态的标称属性。</li>
</ul></li>
<li><p><strong>序数属性:</strong> 值有一个有意义的顺序
<mark>(定性的)</mark></p></li>
<li><p><strong>数值属性:</strong> 可度量的量 <mark>(定量的)</mark></p>
<ul>
<li><strong>区间标度属性:</strong>
<ul>
<li>在统一度量的尺度下</li>
<li>值有序（e.g. 日历日期， 温度计数据）</li>
<li>没有固有0点</li>
</ul></li>
<li><strong>比率标度属性：</strong>
<ul>
<li>具有固有零点，可以说一个值是一个值得多少倍</li>
<li>举例：字数、体重、工作年限</li>
</ul></li>
</ul></li>
</ul>
<h2 id="数据的描述统计">数据的描述统计</h2>
<ul>
<li><strong>数据离散特征：</strong>
均值、众数、最值、分位数、方差、离群点……</li>
<li><strong>排序区间：</strong>
<ul>
<li>数据离散度： 多个粒度上得分析</li>
<li>排序区间得盆图/分位数图分析</li>
</ul></li>
</ul>
<p><strong>描述统计的图形显示：</strong>
箱型图、直方图、分位数图、散点图</p>
<h2 id="数据可视化方法">数据可视化方法</h2>
<p><strong>数据可视化方法得分类：</strong></p>
<ul>
<li>基于像素的可视化技术
<ul>
<li>协方差矩阵的热力图</li>
</ul></li>
<li>基于几何投影的可视化技术
<ul>
<li>直接可视化、散点图、透视地形、平行坐标</li>
</ul></li>
<li>基于图标的可视化技术
<ul>
<li>脸谱图、人物画像图</li>
</ul></li>
<li>基于分层的可视化技术
<ul>
<li>树状图、<em>锥形树</em></li>
</ul></li>
<li>可视化复杂数据与关系
<ul>
<li>标签云、线形图、饼图、结构图/流程图、<em>怪图、系统进化图</em></li>
</ul></li>
</ul>
<h2 id="测量数据的相似性和相异性">测量数据的相似性和相异性</h2>
<ul>
<li><strong>相似性：</strong> 度量数据的相似程度，越大越相似，通常范围在
<span class="math display">[0,1]</span></li>
<li><strong>相异性：</strong>
(e.g. distance)，度量数据的差异，最小值通常为0 ，越大差异越大</li>
</ul>
<p><strong>相异度矩阵：</strong>
对角线为0的下三角矩阵，元素为对应对象之间的距离 <img
src="相异度矩阵.png" srcset="/BLOG/img/loading.gif" lazyload alt="相异度矩阵矩阵" /></p>
<h3 id="标称属性的邻近度量">标称属性的邻近度量</h3>
<ul>
<li><p>得分匹配：</p>
<p><span class="math display">$$d(i,j) = \frac{p - m}{p}$$</span></p>
<p>其中 <span class="math inline"><em>m</em></span>
为相同变量数量，<span class="math inline"><em>p</em></span>
为变量总数。</p></li>
<li><p>独热编码：<br />
具体而言即将名称或类别转化为独热编码，随后利用编码计算距离</p></li>
</ul>
<h3 id="二进制属性的度量">二进制属性的度量</h3>
<figure>
<img src="列联表.png" srcset="/BLOG/img/loading.gif" lazyload alt="列联表" />
<figcaption aria-hidden="true">列联表</figcaption>
</figure>
<p><strong>对称二元变量距离：</strong> <span class="math inline">$d(i,j)
= \frac{r+s}{q+r+s+t}$</span></p>
<p><strong>不对称二元变量距离：</strong> <span
class="math inline">$d(i,j) = \frac{r+s}{q+r+s}$</span></p>
<p><strong>Jaccard系数(不对称二元变量相似性)：</strong> <span
class="math inline">$Sim_{Jaccard}(i,j) = \frac{q}{q+r+s}$</span></p>
<p><strong>TIP:</strong> 解释一下这个对称的含义，关键在于0是否有意义
<img src="解释.png" srcset="/BLOG/img/loading.gif" lazyload alt="对称的含义解释" /></p>
<h4
id="例题二进制属性的非对称相异性度量">例题：二进制属性的非对称相异性度量</h4>
<figure>
<img src="例题1.png" srcset="/BLOG/img/loading.gif" lazyload alt="二进制属性的非对称相异性度量" />
<figcaption aria-hidden="true">二进制属性的非对称相异性度量</figcaption>
</figure>
<h3 id="数值属性的度量">数值属性的度量</h3>
<p><strong>闵可夫斯基距离公式：</strong></p>
<p><span class="math display">$$D(X, Y) = \left( \sum_{i=1}^n |x_i -
y_i|^p \right)^{\frac{1}{p}}$$</span></p>
<p>其中 <span
class="math inline"><em>p</em> ≥ 1</span>，是可调节的距离参数。</p>
<p>主要使用上述公式度量，其中有一些特例可以参照下图： <img
src="距离.png" srcset="/BLOG/img/loading.gif" lazyload alt="闵可夫斯基距离公式特例" /></p>
<h3 id="区间尺度与有序变量的度量">区间尺度与有序变量的度量</h3>
<ul>
<li>区间尺度：进行归一化或标准化处理随后计算闵可夫斯基距离</li>
<li>有序变量：使用排序代替值，并对排序值做最值归一化，随后计算闵可夫斯基距离</li>
</ul>
<h3 id="其他度量方法">其他度量方法：</h3>
<h4 id="余弦相似度">余弦相似度：</h4>
<p><span class="math display">$$
\text{cos}(\theta) = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n}
A_i^2} \cdot \sqrt{\sum_{i=1}^{n} B_i^2}}
$$</span></p>
<p>用于衡量两个向量方向的相似程度，常用于文本分类、推荐系统等。</p>
<h4 id="kl-散度kullback-leibler-divergence">KL 散度（Kullback-Leibler
Divergence）：</h4>
<p><span class="math display">$$
D_{KL}(P \parallel Q) = \sum_{i=1}^{n} p_i \log \frac{p_i}{q_i}
$$</span></p>
<p>衡量两个概率分布之间的差异，常用于模型分布与目标分布之间的对比。</p>
<hr />
<h2 id="数据预处理">数据预处理</h2>
<p><strong>目的：</strong>
数据存在不完全、噪音、不一致的问题，高质量的数据挖掘依赖高质量的数据</p>
<h3 id="数据预处理的主要任务">数据预处理的主要任务：</h3>
<ul>
<li><strong>数据清理：</strong> 异常值、缺失值、噪音
<ul>
<li>针对缺失值：删除整个样本、均值或聚类填充</li>
<li>针对异常值与噪音： 通过分箱、聚类、回归去除</li>
</ul></li>
<li><strong>数据集成：</strong> 多个来源合到一起</li>
<li><strong>数据变换：</strong> 归一化等操作</li>
<li><strong>数据归约：</strong> 减小数据的冗余
<ul>
<li>维度规约： 减少无用特征，或产生新的特征代替原来的
<ul>
<li>决策树规约、PCA</li>
</ul></li>
<li>数值规约：减小数据量，例如用模型参数+离群点代替原始数据
<ul>
<li>线性回归、聚类</li>
</ul></li>
<li>数据压缩</li>
</ul></li>
<li><strong>数据离散化和概念分层：</strong>
<ul>
<li>连续变量离散化</li>
<li>人为指定偏序关系</li>
</ul></li>
</ul>
<hr />
<h1 id="关联规则">关联规则</h1>
<h2 id="关联规则挖掘概念">关联规则挖掘概念</h2>
<p>一种从事务数据集中发现项与项之间有趣关系的技术，形式为：</p>
<p><span
class="math display"><em>X</em> ⇒ <em>Y</em>,  <em>X</em> ∩ <em>Y</em> = ∅</span></p>
<h3 id="支持度support">支持度（Support）：</h3>
<p><span class="math display">$$
\text{Support}(X \Rightarrow Y) = \frac{\text{包含 } X \cup Y \text{
的事务数}}{\text{总事务数}}
$$</span></p>
<h3 id="置信度confidence">置信度（Confidence）：</h3>
<p><span class="math display">$$
\text{Confidence}(X \Rightarrow Y) = \frac{\text{Support}(X \cup
Y)}{\text{Support}(X)} = P(Y|X)
$$</span></p>
<h3 id="提升度lift">提升度（Lift）：</h3>
<p><span class="math display">$$
\text{Lift}(X \Rightarrow Y) =\frac{\text{Confidence}(X \Rightarrow
Y)}{\text{Support}(Y)} =  \frac{P(Y|X)}{P(Y)}
$$</span></p>
<p>Lift &gt; 1 表示正相关，=1 表示独立，&lt;1 表示负相关。</p>
<h2 id="频繁项集与其分类">频繁项集与其分类</h2>
<h3 id="频繁项集">频繁项集：</h3>
<p>如果一个项集 <span class="math inline"><em>X</em></span>
的支持度满足：</p>
<p><span class="math display">Support(<em>X</em>) ≥ min_sup</span></p>
<p>则 <span class="math inline"><em>X</em></span> 是频繁项集。</p>
<h3 id="闭频繁项集closed">闭频繁项集（Closed）：</h3>
<p>项集 <span class="math inline"><em>X</em></span>
是闭的，当不存在一个超集 <span
class="math inline"><em>Y</em> ⊃ <em>X</em></span>，使得：</p>
<p><span
class="math display">Support(<em>Y</em>) = Support(<em>X</em>)</span></p>
<h3 id="极大频繁项集maximal">极大频繁项集（Maximal）：</h3>
<p>项集 <span class="math inline"><em>X</em></span>
是极大的，当不存在一个超集 <span
class="math inline"><em>Y</em> ⊃ <em>X</em></span> 是频繁的。</p>
<h3 id="三者关系">三者关系：</h3>
<ul>
<li>极大频繁项集 ⊆ 闭频繁项集 ⊆ 所有频繁项集；</li>
<li>闭项集压缩信息不丢失，极大项集压缩更彻底但不保留精确支持度。</li>
</ul>
<h2 id="关联规则基本模型">关联规则基本模型</h2>
<p>关联规则是指同时满足最小支持度和最小置信度阈值的规则，形式如下：</p>
<p><span
class="math display"><em>X</em> ⇒ <em>Y</em>,  <em>X</em> ∩ <em>Y</em> = ∅</span></p>
<h3 id="挖掘流程">挖掘流程：</h3>
<ol type="1">
<li>找出所有频繁项集（满足 <span
class="math inline">Support ≥ min_sup</span>）；</li>
<li>从频繁项集中生成所有满足 <span
class="math inline">Confidence ≥ min_conf</span> 的规则。</li>
</ol>
<p>只有同时满足这两个条件的规则，才被称为“强规则”。</p>
<h2 id="apriori算法流程">📌 Apriori算法流程</h2>
<p>Apriori
是一种经典的频繁项集挖掘算法，用于从大量事务数据中发现频繁项集和生成关联规则。</p>
<h3 id="核心思想apriori原理">核心思想：Apriori原理</h3>
<blockquote>
<p><strong>如果一个项集是频繁的，那么它的所有子集也一定是频繁的。</strong></p>
</blockquote>
<p>也就是说，<strong>如果某个项集不是频繁的，则其所有超集都不可能是频繁的</strong>，因此可以在搜索中直接剪枝，提升效率。</p>
<h3 id="算法计算流程">算法计算流程</h3>
<h4 id="步骤-1扫描数据库找出频繁-1-项集l₁">步骤 1️⃣：扫描数据库，找出频繁
1 项集（L₁）</h4>
<ul>
<li>统计每个单项的支持度；</li>
<li>过滤出支持度 ≥ <code>min_sup</code> 的项；</li>
<li>得到频繁 1 项集集合 L₁。</li>
</ul>
<h4 id="步骤-2由-l₁-构造候选-2-项集c₂">步骤 2️⃣：由 L₁ 构造候选 2
项集（C₂）</h4>
<ul>
<li><strong>连接步骤</strong>：将 L₁ 中的项两两组合生成 2
项候选项集；</li>
<li><strong>剪枝步骤</strong>：剔除那些包含非频繁子集的候选项集；</li>
<li>重新扫描数据库统计每个候选项集的支持度；</li>
<li>保留支持度 ≥ <code>min_sup</code> 的项集，得到频繁 2 项集 L₂。</li>
</ul>
<h4 id="步骤-3迭代构造更高阶项集-l₃l₄">步骤 3️⃣：迭代构造更高阶项集
L₃、L₄…</h4>
<ul>
<li>从 L₂ 生成 C₃，再从中提取 L₃；</li>
<li>重复 <strong>连接-剪枝-计数</strong> 的过程；</li>
<li>直到某一轮没有产生新的频繁项集。</li>
</ul>
<h4 id="步骤-4由频繁项集生成关联规则">步骤
4️⃣：由频繁项集生成关联规则</h4>
<ul>
<li>对每个频繁项集 <span
class="math inline"><em>L</em></span>，枚举其所有非空子集 <span
class="math inline"><em>S</em></span>；</li>
<li>构造规则 <span
class="math inline"><em>S</em> ⇒ (<em>L</em>−<em>S</em>)</span>；</li>
<li>计算置信度（Confidence）：</li>
</ul>
<p><span class="math display">$$
\text{Confidence}(S \Rightarrow T) = \frac{\text{Support}(S \cup
T)}{\text{Support}(S)}
$$</span></p>
<ul>
<li>只保留满足 <span class="math inline">Confidence ≥ min_conf</span>
的规则。</li>
</ul>
<h3 id="提高-apriori-算法效率的方法">提高 Apriori 算法效率的方法</h3>
<p>Apriori
算法在面对大规模数据时效率较低，以下是常用的几种优化策略：</p>
<ol type="1">
<li><p>Hash-based itemset counting（基于哈希的项集计数）</p>
<ul>
<li>将候选项集映射到哈希桶中；</li>
<li>如果某个哈希桶中的计数低于最小支持度，则其对应的项集可直接剪枝；</li>
<li><strong>作用：减少无效候选项集数量。</strong></li>
</ul></li>
<li><p>Transaction reduction（事务压缩）</p>
<ul>
<li>每轮扫描后，删除不包含任何频繁项集的事务；</li>
<li>后续迭代无需再扫描这些事务；</li>
<li><strong>作用：减少数据扫描量，提高效率。</strong></li>
</ul></li>
<li><p>Partitioning（划分策略）</p>
<ul>
<li>将数据库划分为多个子集，分别挖掘局部频繁项集；</li>
<li>将局部频繁项集合并，再在全库中验证；</li>
<li><strong>只需两次完整扫描</strong>，适合大数据和并行处理。</li>
</ul></li>
<li><p>Sampling（采样）</p>
<ul>
<li>从数据库中随机抽取部分事务作为样本；</li>
<li>在样本上挖掘频繁项集，再用全体数据验证；</li>
<li><strong>优点：快速；缺点：可能漏掉边缘频繁项集。</strong></li>
</ul></li>
</ol>
<h2 id="关联规则的度量">关联规则的度量</h2>
<p>除了上述说到的支持度、置信度、提升度以外，还有一些度量方式。这里主要是四种零不变性度量。</p>
<p>在关联规则中，某些度量值不受<strong>零事务（null
transactions）</strong>影响，称为<strong>零不变性度量</strong>。</p>
<ul>
<li>零事务：不包含任何待考察项集的事务；</li>
<li>零不变性：度量值只由 <span
class="math inline"><em>P</em>(<em>A</em>|<em>B</em>)</span> 和 <span
class="math inline"><em>P</em>(<em>B</em>|<em>A</em>)</span>
决定，与总事务数无关；</li>
</ul>
<p>以下四种度量具有<strong>零不变性</strong>，且其值范围都在 <span
class="math inline">[0,1]</span>，值越大表示 A 与 B 的关联越紧密。</p>
<ul>
<li><strong>全置信度</strong> <span class="math display">$$
\text{all_confidence}(A, B) = \frac{\text{sup}(A \cup
B)}{\max(\text{sup}(A), \text{sup}(B))} = \min(P(A|B), P(B|A))
$$</span></li>
<li><strong>最大置信度 </strong> <span
class="math display">max_confidence(<em>A</em>,<em>B</em>) = max (<em>P</em>(<em>A</em>|<em>B</em>),<em>P</em>(<em>B</em>|<em>A</em>))</span></li>
<li><strong>余弦置信度</strong><br />
<span class="math display">$$
\text{cosine}(A, B) = \frac{P(A \cap B)}{\sqrt{P(A) \cdot P(B)}} =
\sqrt{P(B|A) \cdot P(A|B)}
$$</span></li>
<li><strong>Kelu置信度</strong><br />
<span class="math display">$$
\text{Kulc}(A, B) = \frac{1}{2} \left( P(A|B) + P(B|A) \right)
$$</span></li>
</ul>
<h1 id="分类">分类</h1>
<h2 id="模型评估方法">模型评估方法</h2>
<ul>
<li>留出法： 按照比例随机划分出训练集和测试集，多次随机抽样取均值</li>
<li>交叉验证： 把数据划分为k份，每次选择一份作为测试集，重复k次，取均值
<ul>
<li>留一法： 每份仅有一个样本</li>
</ul></li>
<li>Bootstrap 自助法:
又放回的抽样m次作为训练集，没有被抽到的作为测试集</li>
</ul>
<h2 id="分类模型评估指标">分类模型评估指标</h2>
<p><strong>混淆矩阵（Confusion Matrix）</strong></p>
<table>
<thead>
<tr class="header">
<th>实际/预测</th>
<th>正类（预测）</th>
<th>负类（预测）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>正类（实际）</td>
<td>TP（真正例）</td>
<td>FN（假负例）</td>
</tr>
<tr class="even">
<td>负类（实际）</td>
<td>FP（假正例）</td>
<td>TN（真负例）</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>TP</strong>：预测为正，实际为正</li>
<li><strong>FP</strong>：预测为正，实际为负</li>
<li><strong>FN</strong>：预测为负，实际为正</li>
<li><strong>TN</strong>：预测为负，实际为负</li>
</ul>
<p><strong>指标：</strong></p>
<ul>
<li>准确率</li>
</ul>
<p><span class="math display">$$
\text{Accuracy} = \frac{TP + TN}{TP + FP + FN + TN}
$$</span></p>
<ul>
<li>召回率</li>
</ul>
<p><span class="math display">$$
\text{Recall} = \frac{TP}{TP + FN}
$$</span></p>
<ul>
<li>精确率</li>
</ul>
<p><span class="math display">$$
\text{Precision} = \frac{TP}{TP + FP}
$$</span></p>
<ul>
<li>F1-Score</li>
</ul>
<p><span class="math display">$$
\text{F1} = \frac{2 \cdot \text{Precision} \cdot
\text{Recall}}{\text{Precision} + \text{Recall}}
$$</span></p>
<ul>
<li>F-beta</li>
</ul>
<p><span class="math display">$$
\text{F1} = \frac{(1+\beta^2) \cdot \text{Precision} \cdot
\text{Recall}}{\beta^2 \cdot \text{Precision} + \text{Recall}}
$$</span></p>
<ul>
<li><p>P-R 曲线（Precision-Recall Curve）</p>
<ul>
<li><strong>横轴：Recall（召回率）</strong></li>
<li><strong>纵轴：Precision（精确率）</strong></li>
<li>通过不断调整分类阈值，绘制出一系列 (P, R) 点，形成一条曲线</li>
</ul></li>
<li><p>ROC 曲线（Receiver Operating Characteristic）</p>
<ul>
<li>横轴：FPR（假正率） = <span class="math inline">$\frac{FP}{FP +
TN}$</span></li>
<li>纵轴：TPR（真正率） = 召回率 = <span
class="math inline">$\frac{TP}{TP + FN}$</span></li>
<li>曲线下的面积（AUC）越接近 1，模型越好</li>
</ul></li>
</ul>
<h2 id="决策树归纳算法的流程以-id3c4.5cart-为例">🛠️
决策树归纳算法的流程（以 ID3/C4.5/CART 为例）</h2>
<ol type="1">
<li><p><strong>输入训练集</strong> <span
class="math inline"><em>D</em></span>，每条样本有多个属性 +
类别标签。</p></li>
<li><p><strong>判断停止条件</strong>：</p>
<ul>
<li>样本全属于一个类别 → 生成叶节点，停止；</li>
<li>属性已用完，或样本不足 → 用多数类别作为叶节点标记。</li>
</ul></li>
<li><p><strong>选择最优划分属性 A</strong>：</p>
<ul>
<li>使用某种<strong>划分指标</strong>（如信息增益、增益率、基尼指数）选择属性
A；</li>
<li>若无可用属性 → 直接创建叶节点。</li>
</ul></li>
<li><p><strong>按属性 A 的取值划分数据集</strong>：</p>
<ul>
<li>对每个取值 <span
class="math inline"><em>a</em><sub><em>i</em></sub></span>，将数据集划分为
<span class="math inline"><em>D</em><sub><em>i</em></sub></span>；</li>
<li>为每个子集创建分支子节点。</li>
</ul></li>
<li><p><strong>对子节点递归重复上述过程</strong>：</p>
<ul>
<li>在子集 <span
class="math inline"><em>D</em><sub><em>i</em></sub></span>
上继续构建子树。</li>
</ul></li>
<li><p><strong>形成整棵决策树</strong>：</p>
<ul>
<li>直到所有子集都满足停止条件。</li>
</ul></li>
</ol>
<h2 id="属性选择度量">属性选择度量</h2>
<p>有三种，分别为 信息增益、增益率、基尼系数</p>
<h3 id="信息增益information-gain">信息增益（Information Gain）</h3>
<p>信息增益是 ID3 算法使用的度量标准，它衡量的是：使用某个属性 A
对数据进行划分后，<strong>系统信息熵（混乱度）减少了多少</strong>。</p>
<p>公式：</p>
<p><span class="math display">$$
\text{Gain}(D, A) = Ent(D) - \sum_{v \in \text{Values}(A)}
\frac{|D_v|}{|D|} \cdot Ent(D_v)
$$</span></p>
<ul>
<li>其中 <span class="math inline">$Ent(D) = -\sum_{i=1}^{m}p_i
\text{log}_2 (p_i)$</span> 是原始数据集的熵 ；</li>
<li><span class="math inline"><em>D</em><sub><em>v</em></sub></span>
表示属性 A 取值为 <span class="math inline"><em>v</em></span>
的子集；</li>
<li>值越大表示“划分后不确定性下降得越多”，越适合作为划分属性。</li>
</ul>
<p>缺点：偏向于选择取值多的属性（比如身份证号），容易过拟合。</p>
<hr />
<h3 id="增益率gain-ratio">增益率（Gain Ratio）</h3>
<p>为了解决信息增益偏好多值属性的问题，C4.5 算法引入了增益率：</p>
<p><span class="math display">$$
\text{GainRatio}(D, A) = \frac{\text{Gain}(D, A)}{IV(A)}
$$</span></p>
<ul>
<li><span class="math inline"><em>I</em><em>V</em>(<em>A</em>)</span> 是
A 的“固有值信息”或“分裂信息”，可以理解为划分的复杂度。 <span
class="math display">$$
IV(A) = - \sum_{j=1}^{v}\frac{|D_j|}{D} \text{log}_2(\frac{|D_j|}{D})
$$</span></li>
<li>增益率在衡量信息增益的同时，<strong>惩罚属性值过多的划分</strong>，更加平衡。</li>
</ul>
<hr />
<h3 id="基尼指数gini-index">基尼指数（Gini Index）</h3>
<p>CART
算法采用基尼指数来衡量属性划分的纯净程度。基尼指数越小，表示子集越“纯”。</p>
<p>某个集合 D 的基尼指数定义为：</p>
<p><span class="math display">$$
\begin{align}
Gini(D) &amp;= 1 - \sum_{k=1}^{K} p_k^2 \\
Gini_{split}(D) &amp;= \frac{|D_1|}{|D|}Gini(D_1) +
\frac{|D_2|}{|D|}Gini(D_2)
\end{align}
$$</span></p>
<ul>
<li><span class="math inline"><em>p</em><sub><em>k</em></sub></span>
表示属于第 k 类的概率；</li>
<li>当样本全属于一个类别时，Gini = 0；</li>
<li>每次选择能<strong>最小化划分后加权 Gini</strong>
的属性作为最优划分属性。</li>
</ul>
<hr />
<p>✅ 总结对比：</p>
<table>

<thead>
<tr class="header">
<th>度量方式</th>
<th>使用算法</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>信息增益</td>
<td>ID3</td>
<td>简单直观，计算熵有理论基础</td>
<td>偏向多值属性，可能过拟合</td>
</tr>
<tr class="even">
<td>增益率</td>
<td>C4.5</td>
<td>纠正信息增益偏差，权衡复杂度</td>
<td>偏向取值较均匀的属性</td>
</tr>
<tr class="odd">
<td>基尼指数</td>
<td>CART</td>
<td>计算更快，适合二叉树构建</td>
<td>没有熵那样的理论解释</td>
</tr>
</tbody>
</table>
<h2 id="常见分类方法的主要思想">常见分类方法的主要思想</h2>
<h3 id="朴素贝叶斯分类naive-bayes-classification">朴素贝叶斯分类（Naive
Bayes Classification）</h3>
<p>朴素贝叶斯是一种基于<strong>贝叶斯定理</strong>并假设属性之间相互条件独立的概率分类方法。</p>
<ul>
<li><p>给定训练数据集 <span
class="math inline"><em>D</em></span>，每个样本表示为一个 <span
class="math inline"><em>n</em></span> 维属性向量：<br />
<span
class="math display"><em>X</em> = (<em>x</em><sub>1</sub>,<em>x</em><sub>2</sub>,...,<em>x</em><sub><em>n</em></sub>)</span></p></li>
<li><p>假设总共有 <span class="math inline"><em>m</em></span>
个类别：<span
class="math inline"><em>C</em><sub>1</sub>, <em>C</em><sub>2</sub>, ..., <em>C</em><sub><em>m</em></sub></span><br />
目标是确定输入 <span class="math inline"><em>X</em></span>
属于哪个类别，即计算后验概率最大的类别： <span
class="math display"><em>Ĉ</em> = arg max<sub><em>C</em><sub><em>i</em></sub></sub><em>P</em>(<em>C</em><sub><em>i</em></sub>∣<em>X</em>)</span></p></li>
<li><p>根据<strong>贝叶斯定理</strong>： <span class="math display">$$
P(C_i \mid X) = \frac{P(C_i) \cdot P(X \mid C_i)}{P(X)}
$$</span></p></li>
<li><p>因为 <span class="math inline"><em>P</em>(<em>X</em>)</span>
对所有类别都相同，所以在分类时只需最大化分子部分： <span
class="math display"><em>Ĉ</em> = arg max<sub><em>C</em><sub><em>i</em></sub></sub><em>P</em>(<em>C</em><sub><em>i</em></sub>) ⋅ <em>P</em>(<em>X</em>∣<em>C</em><sub><em>i</em></sub>)</span></p></li>
<li><p>若假设特征条件独立，则有： <span class="math display">$$
P(X \mid C_i) = \prod_{k=1}^{n} P(x_k \mid C_i)
$$</span></p></li>
</ul>
<h4 id="laplacian-correction拉普拉斯校准">Laplacian
Correction（拉普拉斯校准）</h4>
<p>为了解决0概率问题，引入 <strong>Laplacian 平滑（也叫 Add-One
Smoothing）</strong>，公式调整为：</p>
<p><span class="math display">$$
P(x_k \mid C_i) = \frac{N_{ik} + 1}{N_i + d}
$$</span></p>
<ul>
<li><span
class="math inline"><em>N</em><sub><em>i</em><em>k</em></sub></span>：类别
<span class="math inline"><em>C</em><sub><em>i</em></sub></span> 中属性
<span class="math inline"><em>A</em><sub><em>k</em></sub></span> 取值为
<span class="math inline"><em>x</em><sub><em>k</em></sub></span>
的样本数<br />
</li>
<li><span
class="math inline"><em>N</em><sub><em>i</em></sub></span>：类别 <span
class="math inline"><em>C</em><sub><em>i</em></sub></span>
的总样本数<br />
</li>
<li><span class="math inline"><em>d</em></span>：属性 <span
class="math inline"><em>A</em><sub><em>k</em></sub></span>
的可能取值数（即类别数）</li>
</ul>
<p>这样，即使某个值从未出现（<span
class="math inline"><em>N</em><sub><em>i</em><em>k</em></sub> = 0</span>），加
1 后也不会导致整个概率为 0。</p>
<h3 id="knn-分类k-nearest-neighbors">KNN 分类（K-Nearest
Neighbors）</h3>
<p>KNN（K
近邻）是一种<strong>基于实例的非参数分类方法</strong>，核心思想是：
<strong>“相似样本具有相似的类别。”</strong></p>
<p>基本流程：</p>
<ul>
<li>给定一个待分类样本，计算它与训练集中所有样本之间的距离（如欧几里得距离）；</li>
<li>选出距离最近的 <span class="math inline"><em>K</em></span>
个“邻居”；</li>
<li>让这些邻居“投票”，投票最多的类别即为预测结果。</li>
</ul>
<h3 id="逻辑回归logistic-regression">逻辑回归（Logistic
Regression）</h3>
<p>逻辑回归是一种<strong>用于二分类任务的线性模型</strong>，其本质是学习一个函数，输出属于某一类别的概率。</p>
<h4 id="基本思想">基本思想：</h4>
<ul>
<li><p>给定输入特征 <span
class="math inline"><em>X</em> = (<em>x</em><sub>1</sub>,<em>x</em><sub>2</sub>,...,<em>x</em><sub><em>n</em></sub>)</span>，逻辑回归学习一个线性组合：</p>
<p><span
class="math display"><em>z</em> = <em>w</em><sup><em>T</em></sup><em>x</em> + <em>b</em></span></p></li>
<li><p>然后通过<strong>Sigmoid 函数</strong>将其映射到 <span
class="math inline">[0,1]</span> 范围，作为正类的概率：</p>
<p><span class="math display">$$
P(y = 1 \mid x) = \frac{1}{1 + e^{-z}} = \frac{1}{1 + e^{-(w^T x + b)}}
$$</span></p></li>
<li><p>预测时，通常设置阈值 <span class="math inline">0.5</span>，若
<span class="math inline"><em>P</em> ≥ 0.5</span>
则预测为正类，否则为负类。</p></li>
<li><p>没有闭合解，通常使用梯度下架配合交叉熵损失求解（在训练数据上拟合）</p></li>
</ul>
<h3 id="神经网络">神经网络</h3>
<p><maek>跳过</mark></p>
<h3 id="支持向量机svm分类">支持向量机（SVM）分类</h3>
<p>支持向量机是一种<strong>二分类模型</strong>，通过寻找一个最佳分割超平面，将不同类别的数据点分开。</p>
<h4 id="核心思想">核心思想：</h4>
<ul>
<li>寻找一个<strong>最大间隔（Margin）</strong>的超平面，使得距离超平面最近的样本点（支持向量）距离最大化；</li>
<li>最大间隔有助于提高模型的泛化能力。</li>
</ul>
<h4 id="线性可分情况">线性可分情况：</h4>
<ul>
<li><p>给定训练样本，SVM 找到一个线性超平面满足：</p>
<p><span
class="math display"><em>w</em><sup><em>T</em></sup><em>x</em> + <em>b</em> = 0</span></p></li>
<li><p>并满足分类约束：</p>
<p><span
class="math display"><em>y</em><sub><em>i</em></sub>(<em>w</em><sup><em>T</em></sup><em>x</em><sub><em>i</em></sub>+<em>b</em>) ≥ 1,  <em>i</em> = 1, 2, …, <em>n</em></span></p></li>
<li><p>目标是最大化间隔，即最小化 <span
class="math inline">∥<em>w</em>∥<sup>2</sup></span>。</p></li>
</ul>
<h4 id="线性不可分情况">线性不可分情况：</h4>
<ul>
<li><p>使用<strong>软间隔（Soft
Margin）</strong>，允许一定程度的分类错误，加入松弛变量 <span
class="math inline"><em>ξ</em><sub><em>i</em></sub></span>；</p></li>
<li><p>目标函数变为：</p>
<p><span class="math display">$$
\min_{w,b} \frac{1}{2}\|w\|^2 + C \sum_{i=1}^n \xi_i
$$</span></p></li>
<li><p>其中 <span class="math inline"><em>C</em></span>
控制误差惩罚强度。</p></li>
</ul>
<h4 id="非线性分类">非线性分类：</h4>
<ul>
<li>通过<strong>核函数（Kernel）</strong>，将数据映射到高维空间，使其线性可分；</li>
<li>常用核函数包括：
<ul>
<li>线性核：<span
class="math inline"><em>K</em>(<em>x</em>,<em>z</em>) = <em>x</em><sup><em>T</em></sup><em>z</em></span></li>
<li>多项式核：<span
class="math inline"><em>K</em>(<em>x</em>,<em>z</em>) = (<em>x</em><sup><em>T</em></sup><em>z</em>+<em>c</em>)<sup><em>d</em></sup></span></li>
<li>径向基函数核（RBF）：<span
class="math inline"><em>K</em>(<em>x</em>,<em>z</em>) = exp (−<em>γ</em>∥<em>x</em>−<em>z</em>∥<sup>2</sup>)</span></li>
</ul></li>
</ul>
<h3 id="集成方法ensemble-methods">集成方法（Ensemble Methods）</h3>
<p>集成方法是将多个<strong>弱分类器</strong>组合成一个强分类器的技术，旨在提高模型的准确性和稳定性。</p>
<h4 id="基本思想-1">基本思想：</h4>
<ul>
<li>通过结合多个模型的预测结果，减少单个模型的偏差和方差；</li>
<li>常见方式包括投票（分类）或平均（回归）。</li>
</ul>
<h4 id="主要类型">主要类型：</h4>
<ol type="1">
<li><strong>Bagging（Bootstrap Aggregating）</strong>
<ul>
<li>通过自助采样（有放回抽样）构建多个训练子集，训练多个模型；<br />
</li>
<li>预测时对多个模型结果投票或平均；<br />
</li>
<li>代表算法：随机森林（Random Forest）。</li>
</ul></li>
<li><strong>Boosting</strong>
<ul>
<li>逐步训练一系列弱分类器，每个分类器关注前一个分类器错误分类的样本；<br />
</li>
<li>通过加权组合提升整体性能；<br />
</li>
<li>代表算法：AdaBoost、Gradient Boosting、XGBoost。</li>
</ul></li>
<li><strong>Stacking（堆叠）</strong>
<ul>
<li>训练多个不同类型的基模型；<br />
</li>
<li>使用另一个模型（元学习器）学习如何组合基模型的输出。</li>
</ul></li>
</ol>
<h1 id="聚类">聚类</h1>
<p><strong>聚类分析：</strong>将数据对象分为多个类或簇，类内对象相似，类间对象相异</p>
<h2 id="基于划分的聚类方法">基于划分的聚类方法</h2>
<p>构造n个对象数据集D的划分，将其划分为k个列（所以需要给定K值的是基于划分方法的）</p>
<h3 id="k-平均聚类算法">K-平均聚类算法</h3>
<p><strong>步骤：</strong></p>
<ul>
<li>选择一个含有随机选择样本的k个簇的初始划分，计算这些簇的质心</li>
<li>根据欧氏距离把剩余的每个样本分配到距离它最近的簇质心的划分</li>
<li>计算被分配到每个簇的样本的均值向量，作为新的簇的质心</li>
<li>重复2,3直到k个簇的质心点不再发生变化或平方误差准则最小
<ul>
<li>这里平方误差是说每个簇内样本到中心的平方误差的总和的总和</li>
</ul></li>
</ul>
<p><strong>优点:</strong> - 相对有效性，复杂度为 <span
class="math inline"><em>O</em>(<em>k</em><em>n</em><em>t</em>)</span>，
其中 n 是对象数目, k 是簇数目, t 是迭代次数</p>
<p><strong>缺点：</strong> - 需要预先指顶簇的数目k, -
不能处理噪音数据和孤立点(outliers) -
不适合用来发现具有非凸形状(non-convex shapes)的簇</p>
<h3 id="k-中心点聚类算法">K-中心点聚类算法</h3>
<p>主要用于解决K-均值在异常值上的缺点</p>
<p><strong>k-中心点算法流程:</strong> 1. 任意选取 k 个点作为 中心点</p>
<ol start="2" type="1">
<li><p>按照与中心点最近的原则，将剩余点分配到当前最佳的中心点代表的类中</p></li>
<li><p>在每一类中，计算每个成员点对应的准则函数，选取准则函数最小时对应的点作为新的
中心点</p></li>
<li><p>重复2-3的过程，直到所有的 中心点
点不再发生变化，或已达到设定的最大迭代次数</p></li>
</ol>
<blockquote>
<p>其中准则函数为，一类中，某个成员点和其他成员点的距离之和</p>
</blockquote>
<p><strong>优点：</strong> 当存在噪音和孤立点时, K-medoids 比 K-means
更健壮。 <strong>缺点：</strong> K-medoids 对于小数据集工作得很好,
但不能很好地用于大数据集，计算质心的步骤时间复杂度是<span
class="math inline"><em>O</em>(<em>n</em><sup>2</sup>)</span>，运行速度较慢</p>
<h3 id="pam算法">PAM算法</h3>
<p>是最早提出的k-中心点聚类算法</p>
<p><strong>PAM算法流程：</strong><br />
1. 首先随机选择k个对象作为中心，把每个对象分配给离它最近的中心。<br />
2. 对每个Medoid和非Medoid点(<span
class="math inline"><em>O</em>(<em>k</em>(<em>n</em>−<em>k</em>))</span>)，尝试交换它们的位置，计算新的聚类代价(<span
class="math inline"><em>O</em>(<em>n</em>−<em>k</em>)</span>)。<br />
3.
如果总的损失减少，则交换中心对象和非中心对象；如果总的损失增加，则不进行交换</p>
<p><strong>优点：</strong> 当存在噪音和孤立点时, K-medoids 比 K-means
更健壮；<br />
<strong>缺点：</strong> 同K-medoids， 每轮复杂度<span
class="math inline"><em>O</em>(<em>k</em>(<em>n</em>−<em>k</em>)<sup>2</sup>)</span></p>
<h3 id="clara算法">CLARA算法</h3>
<p>不考虑整个数据集, 而是选择数据的一小部分作为样本</p>
<p><strong>CLARA算法流程：</strong><br />
1. 从数据集中抽取一个样本集, 并对样本集使用PAM，划分为k个簇<br />
2. 将其他未被抽到的样本划分到上述确定的簇中<br />
3. 重复上述步骤，选择最好的聚类作为输出</p>
<p><strong>优点:</strong> 可以处理的数据集比 PAM大</p>
<p><strong>缺点:</strong> - 有效性依赖于样本集的大小 -
基于样本的好的聚类并不一定是 整个数据集的好的聚类, 样本可能发生倾斜</p>
<h3 id="clarans算法">CLARANS算法</h3>
<h4 id="输入参数">输入参数：</h4>
<ul>
<li>数据集 <span class="math inline"><em>D</em></span>，样本个数 <span
class="math inline"><em>n</em></span></li>
<li>聚类数 <span class="math inline"><em>k</em></span></li>
<li>每轮最大尝试次数 <code>max_neighbor</code></li>
<li>总共执行的局部搜索次数 <code>num_local</code></li>
</ul>
<h4 id="多次随机局部搜索共执行-num_local-次">多次随机局部搜索（共执行
<code>num_local</code> 次）：</h4>
<p>对于每次局部搜索：</p>
<ol type="1">
<li>随机选择初始的 <span class="math inline"><em>k</em></span> 个
Medoids<br />
</li>
<li>当前 Medoids 作为初始解，循环尝试邻居（即交换一个 Medoid 和一个非
Medoid）：
<ul>
<li>随机选择一个非 Medoid 点，与当前某个 Medoid
交换，得到一个“邻居解”</li>
<li>计算这个新解的聚类代价</li>
<li>如果代价降低，则更新当前解为这个新解</li>
</ul></li>
<li>重复上述步骤，直到连续 <code>max_neighbor</code>
次尝试都没有找到更优解</li>
<li>记录该局部搜索中最好的解</li>
</ol>
<h4 id="最终输出">最终输出：</h4>
<p>在 <code>num_local</code>
次局部搜索中，选择代价最低的那个作为最终聚类结果</p>
<h2 id="基于层次的聚类方法">基于层次的聚类方法</h2>
<p>层次的聚类方法将数据对象组成一棵聚类的树,可以进一步分为: -
凝聚的(agglomerative)层次聚类 (自底向上形成) - 分裂的(divisive)层次聚类
(自顶向下形成)</p>
<h3 id="agnes-agglomerative-nesting算法">AGNES (Agglomerative
Nesting)算法</h3>
<p><strong>算法流程：</strong></p>
<ul>
<li>初始化：计算包含每对样本间距离（如欧氏距离）的相似矩阵，把每个样本作为一个簇；<br />
</li>
<li>选择：使用相似矩阵查找最相似的两个簇；
<ul>
<li>两个簇间的相似度由这两个不同簇中距离最近的数据点对的相似度来确定－－单链接方法(Single-link)<br />
</li>
</ul></li>
<li>更新：
<ul>
<li>将两个簇合并为一个簇，簇的个数通过合并被更新；<br />
</li>
<li>同时更新相似矩阵，将两个簇的两行（两列）距离用1行（1列）距离替换反映合并操作<br />
</li>
</ul></li>
<li>重复：执行n-1次选择与更新；<br />
</li>
<li>结束：当所有样本都合并成一个簇或满足某个终止条件时，整个过程结束</li>
</ul>
<h3 id="diana-divisive-analysis算法">DIANA (Divisive Analysis)算法</h3>
<p><strong>主要思想：</strong>采用自顶向下策略</p>
<ul>
<li>首先将所有样本置于一个簇中；</li>
<li>然后逐渐细分为越来越小的簇，来增加簇的数目；</li>
<li>直到每个样本自成一个簇，或者达到某个终结条件。</li>
<li>例如达到了某个希望的簇的数目或两个最近的簇之间的
距离超过了某个阈值。</li>
</ul>
<h3 id="birch算法流程">BIRCH算法流程</h3>
<p>BIRCH 是一种基于层次结构的聚类算法，适合大规模数据，使用 CF-Tree
来压缩数据并高效聚类。</p>
<h4 id="关键数据结构cfclustering-feature">关键数据结构：CF（Clustering
Feature）</h4>
<p>每个聚类用三元组表示：</p>
<p><span
class="math display">CF = (<em>N</em>,<strong>L</strong><strong>S</strong>,<strong>S</strong><strong>S</strong>)</span></p>
<ul>
<li><span class="math inline"><em>N</em></span>：簇中点数<br />
</li>
<li><span class="math inline">$\mathbf{LS} = \sum_{i=1}^N
\mathbf{x}_i$</span>（点坐标线性和）<br />
</li>
<li><span class="math inline">$\mathbf{SS} = \sum_{i=1}^N
\mathbf{x}_i^2$</span>（点坐标平方和）</li>
</ul>
<p>利用 CF 可快速计算簇的质心、半径和直径，无需访问原始点。</p>
<h4 id="cf-tree-结构">CF-Tree 结构</h4>
<ul>
<li>一种平衡树，类似 B+ 树<br />
</li>
<li><strong>非叶节点</strong>存储子节点的 CF 信息<br />
</li>
<li><strong>叶子节点</strong>存储实际的 CF 条目，每个 CF
表示一个子簇<br />
</li>
<li>叶子节点通过链表相连，支持顺序访问<br />
</li>
<li>主要参数：
<ul>
<li><span
class="math inline"><em>B</em></span>：非叶节点最大子节点数<br />
</li>
<li><span class="math inline"><em>L</em></span>：叶子节点最大 CF
条目数<br />
</li>
<li>阈值 <span class="math inline"><em>T</em></span>：限制每个 CF
最大半径</li>
</ul></li>
</ul>
<h4 id="算法流程">算法流程</h4>
<ol type="1">
<li><strong>初始化</strong>：设定阈值 <span
class="math inline"><em>T</em></span>、<span
class="math inline"><em>B</em></span>、<span
class="math inline"><em>L</em></span>，初始化空 CF-Tree。<br />
</li>
<li><strong>逐点插入</strong>：
<ul>
<li>从根节点开始递归，找到距离点最近的叶子节点 CF。<br />
</li>
<li>尝试将点合并入该 CF，若合并后半径 ≤ <span
class="math inline"><em>T</em></span>，更新 CF；否则新建 CF 条目。<br />
</li>
</ul></li>
<li><strong>节点分裂</strong>：
<ul>
<li>叶子节点 CF 条目数超过 <span class="math inline"><em>L</em></span>
时分裂：
<ul>
<li>选择两个最远的 CF 条目作为种子，重新分配其余 CF 条目。<br />
</li>
<li>生成两个叶子节点，更新父节点。<br />
</li>
</ul></li>
<li>父节点子节点数超过 <span class="math inline"><em>B</em></span>
时递归分裂，直到根节点。<br />
</li>
</ul></li>
<li><strong>完成构建</strong>。<br />
</li>
<li><strong>全局聚类（可选）</strong>：对叶子节点所有 CF
的质心进行聚类（如 K-Means）(在每个叶子节点的簇的层面再聚类)。</li>
</ol>
<h4 id="算法优缺点">算法优缺点</h4>
<ul>
<li><strong>优点</strong>：
<ul>
<li>只需一次扫描，速度快<br />
</li>
<li>数据压缩存储，节省内存<br />
</li>
<li>支持增量更新和大规模数据<br />
</li>
</ul></li>
<li><strong>缺点</strong>：
<ul>
<li>对异常点敏感<br />
</li>
<li>结果依赖阈值 <span class="math inline"><em>T</em></span> 选取</li>
</ul></li>
</ul>
<h3 id="cure算法简介含抽样处理">CURE算法简介（含抽样处理）</h3>
<h4 id="核心思想-1">核心思想</h4>
<ul>
<li>用多个代表点描述簇的形状，避免用质心造成的误差<br />
</li>
<li>代表点收缩至簇中心以减少异常点影响<br />
</li>
<li>支持复杂簇结构，抗噪声能力强</li>
</ul>
<h4 id="算法流程-1">算法流程</h4>
<ol type="1">
<li><strong>抽样（Sampling）</strong>
<ul>
<li>从大数据中随机抽取一个代表样本子集，减小计算量</li>
</ul></li>
<li><strong>初始聚类</strong>
<ul>
<li>对抽样数据，每个点看作一个簇，或者先用快速算法做粗聚类，得到初始簇集合</li>
</ul></li>
<li><strong>选择代表点</strong>
<ul>
<li>每个簇选固定数量的代表点（如距离簇质心较远的点）</li>
</ul></li>
<li><strong>收缩代表点</strong>
<ul>
<li>将代表点沿向簇中心的方向收缩一定比例（如20%-30%）</li>
</ul></li>
<li><strong>合并簇</strong>
<ul>
<li>计算簇间代表点的最小距离，合并距离最近的两个簇</li>
</ul></li>
<li><strong>重复步骤 3-5</strong>
<ul>
<li>直到满足聚类数或者其他停止条件</li>
</ul></li>
<li><strong>增量聚类（可选）</strong>
<ul>
<li>对未抽样的数据，将每个点分配到最近的簇中，实现全量数据聚类</li>
</ul></li>
</ol>
<h4 id="特点">特点</h4>
<ul>
<li>抽样减少计算量<br />
</li>
<li>代表点和收缩减少异常点影响<br />
</li>
<li>能处理非球形簇，结构复杂</li>
</ul>
<h3 id="rock-算法简介">ROCK 算法简介</h3>
<p>ROCK（Robust Clustering using
linKs）是一种基于邻居“链接”关系的层次聚类算法，特别适合处理类别数据和非球形簇。</p>
<h4 id="关键概念">关键概念</h4>
<ul>
<li><p><strong>邻居（Neighbors）</strong><br />
如果两个数据点之间的相似度（例如 Jaccard 相似度）超过某个阈值 <span
class="math display"><em>θ</em></span>，则认为它们是邻居。</p></li>
<li><p><strong>链接数（Link）</strong><br />
两个点的链接数是它们共同邻居的数量。链接数越大，说明两点越有可能属于同一个簇。</p></li>
<li><p><strong>Jaccard 相似度</strong><br />
衡量两个集合相似度的指标，定义为它们交集的大小除以并集的大小，用来计算点之间的相似性。</p></li>
<li><p><strong>好处度（Goodness Measure）</strong><br />
用于评估合并两个簇的合理性。<br />
计算公式为：</p>
<p><span class="math display">$$
Goodness(C_i, C_j) = \frac{links(C_i, C_j)}{(size(C_i) + size(C_j))^{1 +
2f(\theta)} - size(C_i)^{1 + 2f(\theta)} - size(C_j)^{1 + 2f(\theta)}}
$$</span></p>
<p>其中：</p>
<ul>
<li><span
class="math display"><em>l</em><em>i</em><em>n</em><em>k</em><em>s</em>(<em>C</em><sub><em>i</em></sub>,<em>C</em><sub><em>j</em></sub>)</span>
是两个簇间所有点对的链接数之和<br />
</li>
<li><span
class="math display"><em>s</em><em>i</em><em>z</em><em>e</em>(<em>C</em>)</span>
是簇中点的数量<br />
</li>
<li><span class="math display"><em>f</em>(<em>θ</em>)</span>
是阈值相关的函数，用于调节合并力度</li>
</ul></li>
</ul>
<h4 id="算法流程-2">算法流程</h4>
<ol type="1">
<li><strong>计算邻居关系</strong>
<ul>
<li>对数据集中每对数据点计算相似度（如 Jaccard 相似度）。<br />
</li>
<li>如果相似度大于阈值 <span
class="math display"><em>θ</em></span>，则认为它们是邻居。</li>
</ul></li>
<li><strong>计算链接数</strong>
<ul>
<li>对每对数据点，统计它们共同邻居的数量，这个数量称为“链接数”。</li>
</ul></li>
<li><strong>初始化簇</strong>
<ul>
<li>将每个数据点作为一个单独的簇。</li>
</ul></li>
<li><strong>计算簇间好处度</strong>
<ul>
<li>根据簇间所有点对的链接数以及簇大小，计算两个簇合并的好处度（Goodness
Measure）。</li>
</ul></li>
<li><strong>合并簇</strong>
<ul>
<li>选择好处度最高的两个簇进行合并。</li>
</ul></li>
<li><strong>重复合并</strong>
<ul>
<li>不断重复计算好处度和合并操作，直到达到预定的簇数或满足其他终止条件。</li>
</ul></li>
</ol>
<h4 id="特点-1">特点</h4>
<ul>
<li>适合类别数据和非球形簇<br />
</li>
<li>利用邻居链接捕捉复杂的结构关系<br />
</li>
<li>计算量较大，适合中小规模数据</li>
</ul>
<h3 id="chameleon算法">CHAMELEON算法</h3>
<p>CHAMELEON是一种基于图的层次聚类算法，能够自动发现不同形状和密度的簇，适用于复杂数据结构。它结合了动态模型和多阶段聚类，利用簇的内部连接性和相似性来决定合并策略。</p>
<h4 id="核心思想-2">核心思想</h4>
<ul>
<li>利用<strong>k-近邻图（k-NN Graph）</strong>表示数据点间的关系<br />
</li>
<li>通过度量簇间的<strong>相对连通性（Relative
Connectivity）</strong>和<strong>相对相似性（Relative
Closeness）</strong>，决定簇是否合并<br />
</li>
<li>动态调整合并过程，适应簇的形状和密度差异</li>
</ul>
<h4 id="主要概念">主要概念</h4>
<ul>
<li><p><strong>k-近邻图</strong><br />
构建一个图，点为数据样本，边连接每个点的k个最近邻，边权反映点之间的相似度</p></li>
<li><p><strong>相对连通性（Relative Connectivity, RC）</strong><br />
衡量两个簇间连接强度相对于它们内部连接的强度</p></li>
<li><p><strong>相对相似性（Relative Closeness, RCl）</strong><br />
衡量两个簇之间的距离相对于它们各自的内部距离</p></li>
</ul>
<h4 id="算法流程-3">算法流程</h4>
<ol type="1">
<li><strong>构建k-近邻图</strong>
<ul>
<li>计算数据点间的相似度，构造加权k-近邻图</li>
</ul></li>
<li><strong>初始聚类</strong>
<ul>
<li>使用图划分方法（如基于最小割的聚类）将数据划分成多个细粒度的小簇</li>
</ul></li>
<li><strong>计算簇间相似度</strong>
<ul>
<li>对每对簇计算相对连通性（RC）和相对相似性（RCl）</li>
</ul></li>
<li><strong>动态合并簇</strong>
<ul>
<li>根据RC和RCl的综合评价，选择合适的簇对进行合并</li>
</ul></li>
<li><strong>重复合并</strong>
<ul>
<li>直到满足预设的簇数或没有合适的簇对可以合并</li>
</ul></li>
</ol>
<h4 id="特点-2">特点</h4>
<ul>
<li>适应不同簇形状和密度<br />
</li>
<li>结合了局部结构和全局信息<br />
</li>
<li>适合处理复杂数据集，但计算复杂度较高</li>
</ul>
<h2 id="基于密度的聚类方法">基于密度的聚类方法</h2>
<h3 id="密度的概念">密度的概念</h3>
<h4 id="ε-邻域epsilon-neighborhood">1. ε-邻域（Epsilon
Neighborhood）</h4>
<ul>
<li>给定一个点 <span class="math display"><em>p</em></span>，其
<strong>ε-邻域</strong> 是所有与点 <span
class="math display"><em>p</em></span> 距离不超过 <span
class="math display"><em>ε</em></span> 的点的集合。</li>
<li>通俗地说，就是以点 <span class="math display"><em>p</em></span>
为圆心，半径为 <span class="math display"><em>ε</em></span>
的“圆”里有谁。</li>
</ul>
<h4 id="核心对象core-object">2. 核心对象（Core Object）</h4>
<ul>
<li>如果一个点的 ε-邻域内至少包含
<strong>MinPts</strong>（最小点数，包括它自己）个点，则称它为<strong>核心对象</strong>。</li>
<li>它说明该区域“密度足够大”。</li>
</ul>
<h4 id="直接密度可达directly-density-reachable-ddr">3.
直接密度可达（Directly Density-Reachable, DDR）</h4>
<ul>
<li>如果点 <span class="math display"><em>p</em></span> 在点 <span
class="math display"><em>q</em></span> 的 ε-邻域内，<strong>且 <span
class="math display"><em>q</em></span>
是核心对象</strong>，那么我们说：<br />
<strong>点 <span class="math display"><em>p</em></span> 是从点 <span
class="math display"><em>q</em></span> 直接密度可达的</strong>。</li>
<li>注意：只有从核心对象出发，才能有“直接密度可达”。</li>
</ul>
<h4 id="密度可达density-reachable">4. 密度可达（Density-Reachable）</h4>
<ul>
<li>如果存在一条由多个点组成的链 <span
class="math display"><em>p</em><sub>1</sub>, <em>p</em><sub>2</sub>, ..., <em>p</em><sub><em>n</em></sub></span>，使得：
<ul>
<li><span
class="math display"><em>p</em><sub>1</sub> = <em>q</em></span>，<span
class="math display"><em>p</em><sub><em>n</em></sub> = <em>p</em></span>；</li>
<li>对于每对相邻点 <span
class="math display"><em>p</em><sub><em>i</em></sub></span> 和 <span
class="math display"><em>p</em><sub><em>i</em> + 1</sub></span>，都有
<span class="math display"><em>p</em><sub><em>i</em> + 1</sub></span>
是从 <span class="math display"><em>p</em><sub><em>i</em></sub></span>
<strong>直接密度可达</strong>的；</li>
</ul></li>
<li>那么我们说：<strong>点 <span class="math display"><em>p</em></span>
是从点 <span class="math display"><em>q</em></span>
密度可达的</strong>。</li>
</ul>
<blockquote>
<p>这个关系是<strong>单向的</strong>：即 <span
class="math display"><em>p</em></span> 从 <span
class="math display"><em>q</em></span> 可达，不代表 <span
class="math display"><em>q</em></span> 从 <span
class="math display"><em>p</em></span> 可达。</p>
</blockquote>
<h4 id="密度相连density-connected">5. 密度相连（Density-Connected）</h4>
<ul>
<li>如果存在某个点 <span class="math display"><em>o</em></span>，使得：
<ul>
<li>点 <span class="math display"><em>p</em></span> 和点 <span
class="math display"><em>q</em></span> 都是从 <span
class="math display"><em>o</em></span> 密度可达的；</li>
</ul></li>
<li>那么我们说：<strong>点 <span class="math display"><em>p</em></span>
和点 <span class="math display"><em>q</em></span>
是密度相连的</strong>。</li>
</ul>
<blockquote>
<p>这个关系是<strong>对称的</strong>，用于判断是否属于同一个簇。</p>
</blockquote>
<h4 id="示例">示例</h4>
<figure>
<img src="p.png" srcset="/BLOG/img/loading.gif" lazyload alt="密度相关概念的例子" />
<figcaption aria-hidden="true">密度相关概念的例子</figcaption>
</figure>
<h3 id="dbscan算法">DBSCAN算法</h3>
<h4 id="核心参数">核心参数</h4>
<ul>
<li><strong><span
class="math display"><em>ε</em></span>（eps）</strong>：定义点的“邻域”范围<br />
</li>
<li><strong>MinPts</strong>：最小邻居点数，决定一个点是否为“核心点”</li>
</ul>
<h4 id="算法流程-4">算法流程</h4>
<ol type="1">
<li><p><strong>初始化状态</strong><br />
所有数据点标记为“未访问”。</p></li>
<li><p><strong>从一个未访问点 <span
class="math inline"><em>p</em></span> 开始：</strong></p>
<ul>
<li>计算 <span class="math inline"><em>p</em></span> 的 <span
class="math inline"><em>ε</em></span>-邻域（记为 <span
class="math inline"><em>N</em><sub><em>ε</em></sub>(<em>p</em>)</span>）；</li>
<li>如果邻域内的点数 &lt; MinPts：将 <span
class="math inline"><em>p</em></span> 标记为噪声，跳过；</li>
<li>否则，将 <span class="math inline"><em>p</em></span>
作为<strong>核心点</strong>，创建一个新簇，将 <span
class="math inline"><em>N</em><sub><em>ε</em></sub>(<em>p</em>)</span>
中的所有点加入该簇。</li>
</ul></li>
<li><p><strong>扩展当前簇：</strong><br />
对刚加入簇的每个点 <span
class="math inline"><em>q</em></span>（若未访问）执行：</p>
<ul>
<li>标记 <span class="math inline"><em>q</em></span> 为已访问；</li>
<li>若 <span class="math inline"><em>q</em></span>
是核心点（其邻域内点数 ≥ MinPts）：
<ul>
<li>将 <span
class="math inline"><em>N</em><sub><em>ε</em></sub>(<em>q</em>)</span>
中的所有点加入当前簇（即密度可达）；</li>
</ul></li>
<li>否则（<span class="math inline"><em>q</em></span>
是边界点），保留在当前簇中，不继续扩展。</li>
</ul></li>
<li><p><strong>重复步骤 2~3</strong></p>
<ul>
<li>直到所有点都被访问。</li>
</ul></li>
</ol>
<h4 id="dbscan-的输出">DBSCAN 的输出</h4>
<ul>
<li>若干个簇（每个由核心点及其密度可达的点组成）<br />
</li>
<li>一些噪声点（不属于任何簇）</li>
</ul>
<h4 id="优点">优点</h4>
<ul>
<li>不需要预先指定簇数<br />
</li>
<li>可发现任意形状的簇<br />
</li>
<li>可识别噪声点<br />
</li>
<li>对离群点不敏感</li>
</ul>
<h4 id="缺点">缺点</h4>
<ul>
<li>需要合适的 <span class="math display"><em>ε</em></span> 和
MinPts<br />
</li>
<li>当数据密度差异较大时效果较差<br />
</li>
<li>高维数据中，距离不再可靠（维数灾难）</li>
</ul>
<h3 id="optics算法">OPTICS算法</h3>
<p>OPTICS（Ordering Points To Identify the Clustering
Structure）是一种基于密度的聚类算法，解决了 DBSCAN
对参数敏感的问题。它不仅能发现不同密度的簇，还能输出数据的聚类结构排序，方便后续分析。</p>
<h4 id="相关概念">相关概念</h4>
<ul>
<li><p><strong>核心距离（Core Distance）</strong><br />
对点 <span class="math display"><em>p</em></span>，如果它的 <span
class="math display"><em>ε</em></span>-邻域内至少有 MinPts
个点，则核心距离是到第 MinPts 个最近邻的距离；否则未定义。</p></li>
<li><p><strong>可达距离（Reachability Distance）</strong><br />
对点 <span class="math display"><em>p</em></span> 和它的邻居 <span
class="math display"><em>o</em></span>，可达距离定义为：</p>
<p><span
class="math display"><em>r</em><em>e</em><em>a</em><em>c</em><em>h</em><em>a</em><em>b</em><em>i</em><em>l</em><em>i</em><em>t</em><em>y</em>_<em>d</em><em>i</em><em>s</em><em>t</em>(<em>o</em>,<em>p</em>) = max (<em>c</em><em>o</em><em>r</em><em>e</em>_<em>d</em><em>i</em><em>s</em><em>t</em>(<em>p</em>),<em>d</em><em>i</em><em>s</em><em>t</em>(<em>p</em>,<em>o</em>))</span></p>
<p>如果 <span class="math display"><em>p</em></span> 是核心点，且 <span
class="math display"><em>o</em></span> 在 <span
class="math display"><em>p</em></span> 的邻域内，否则未定义。</p></li>
</ul>
<h4 id="算法流程-5">算法流程</h4>
<ol type="1">
<li><strong>初始化</strong>
<ul>
<li>给数据集中每个点设置状态为“未访问”。<br />
</li>
<li>给每个点设置可达距离 <span
class="math display"><em>r</em><em>e</em><em>a</em><em>c</em><em>h</em><em>a</em><em>b</em><em>i</em><em>l</em><em>i</em><em>t</em><em>y</em>_<em>d</em><em>i</em><em>s</em><em>t</em> = ∞</span>（表示未知）。<br />
</li>
<li>准备一个空的“输出顺序列表”（ordering），用于记录访问点的顺序。</li>
</ul></li>
<li><strong>遍历所有点</strong><br />
对数据集中的每个点 <span class="math display"><em>p</em></span>：
<ul>
<li>如果 <span class="math display"><em>p</em></span>
已经被访问过，跳过；<br />
</li>
<li>否则，开始处理 <span class="math display"><em>p</em></span>：</li>
</ul></li>
<li><strong>访问点 <span class="math display"><em>p</em></span></strong>
<ul>
<li>将 <span class="math display"><em>p</em></span>
标记为“已访问”。<br />
</li>
<li>计算 <span class="math display"><em>p</em></span> 的核心距离 <span
class="math display"><em>c</em><em>o</em><em>r</em><em>e</em>_<em>d</em><em>i</em><em>s</em><em>t</em>(<em>p</em>)</span>：
<ul>
<li>找出 <span class="math display"><em>p</em></span> 的 <span
class="math display"><em>ε</em></span>-邻域中的所有点，排序后取第 MinPts
个最近邻点的距离作为核心距离。<br />
</li>
<li>如果邻域内点数不足 MinPts，则 <span
class="math display"><em>p</em></span>
不是核心点，核心距离未定义。<br />
</li>
</ul></li>
<li>把 <span class="math display"><em>p</em></span>
加入输出顺序列表（ordering），此时 <span
class="math display"><em>p</em></span>
的可达距离保持为当前值（初始可能为无穷大）。</li>
</ul></li>
<li><strong>如果 <span class="math display"><em>p</em></span>
是核心点</strong>
<ul>
<li>取 <span class="math display"><em>p</em></span> 的 <span
class="math display"><em>ε</em></span>-邻域内所有<strong>未访问的点</strong>，放入一个优先队列（或类似结构），根据可达距离排序（距离越小优先）。<br />
</li>
<li>对这些邻居点 <span class="math display"><em>o</em></span>：
<ul>
<li>计算从 <span class="math display"><em>p</em></span> 到 <span
class="math display"><em>o</em></span> 的可达距离：<br />
<span
class="math display"><em>r</em><em>e</em><em>a</em><em>c</em><em>h</em><em>a</em><em>b</em><em>i</em><em>l</em><em>i</em><em>t</em><em>y</em>_<em>d</em><em>i</em><em>s</em><em>t</em>(<em>o</em>) = max (<em>c</em><em>o</em><em>r</em><em>e</em>_<em>d</em><em>i</em><em>s</em><em>t</em>(<em>p</em>),<em>d</em><em>i</em><em>s</em><em>t</em>(<em>p</em>,<em>o</em>))</span><br />
</li>
<li>如果 <span class="math display"><em>o</em></span>
当前的可达距离大于这个新值，则更新 <span
class="math display"><em>o</em></span> 的可达距离为新值，并将 <span
class="math display"><em>o</em></span> 加入优先队列。<br />
</li>
</ul></li>
<li>依次从优先队列中取出可达距离最小的点 <span
class="math display"><em>q</em></span>，重复步骤 3 和
4，直到优先队列空。</li>
</ul></li>
<li><strong>继续处理剩余点</strong>
<ul>
<li>返回步骤 2，处理下一个未访问的点，直到所有点都被访问。</li>
</ul></li>
</ol>
<h4 id="结果输出">结果输出</h4>
<ul>
<li>一条有序的点列表，反映数据的密度聚类结构。<br />
</li>
<li>通过绘制 <strong>可达距离图（Reachability
Plot）</strong>，可以直观观察不同密度簇的分布和边界。</li>
</ul>
<figure>
<img src="可达距离.jpg" srcset="/BLOG/img/loading.gif" lazyload alt="可达距离图示意" />
<figcaption aria-hidden="true">可达距离图示意</figcaption>
</figure>
<h4 id="优点-1">优点</h4>
<ul>
<li>不需要像 DBSCAN 那样对 <span class="math display"><em>ε</em></span>
选取非常敏感。<br />
</li>
<li>能发现不同密度的簇。<br />
</li>
<li>输出的可达距离序列方便后续聚类分析。</li>
</ul>
<h2 id="基于网格的聚类方法">基于网格的聚类方法</h2>
<h2 id="基于模型的聚类方法">基于模型的聚类方法</h2>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/BLOG/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" class="category-chain-item">计算机</a>
  
  
    <span>></span>
    
  <a href="/BLOG/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" class="category-chain-item">数据挖掘</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/BLOG/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/" class="print-no-link">#计算机</a>
      
        <a href="/BLOG/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" class="print-no-link">#数据挖掘</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>数据挖掘课程复习</div>
      <div>https://gongzihang.github.io/2025/06/11/数据挖掘课程复习/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Zihang Gong</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年6月11日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/BLOG/2025/05/25/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%EF%BC%9A%E7%9F%A9%E9%98%B5%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/" title="线性代数：矩阵与线性方程组">
                        <span class="hidden-mobile">线性代数：矩阵与线性方程组</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"KkofgzAxn6fOOSXjKQhAz5KQ-gzGzoHsz","appKey":"gZuiRbaJsIMdptuIXKhe1KdK","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://gongzihang.github.io/" target="_self"><span>Zihang_Gong</span></a> | <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/BLOG/js/events.js" ></script>
<script  src="/BLOG/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/BLOG/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/BLOG/js/leancloud.js" ></script>

  <script  src="/BLOG/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/BLOG/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
